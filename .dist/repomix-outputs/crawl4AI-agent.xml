This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
crawl4AI-examples/
  1-crawl_single_page.py
  2-crawl_docs_sequential.py
  3-crawl_docs_FAST.py
examples/
  autogen_integration.py
n8n-version/
  Crawl4AI_Agent.json
studio-integration-version/
  .dockerignore
  .env.example
  Dockerfile
  pydantic_ai_expert_endpoint.py
  pydantic_ai_expert.py
tests/
  test_crawl4ai.py
.env.example
crawl_pydantic_ai_docs.py
crawl4ai.py
pydantic_ai_expert.py
README.md
requirements.txt
site_pages.sql
streamlit_ui.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="crawl4AI-examples/1-crawl_single_page.py">
import asyncio
from crawl4ai import *

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://ai.pydantic.dev/",
        )
        print(result.markdown)

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="crawl4AI-examples/2-crawl_docs_sequential.py">
import asyncio
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
import requests
from xml.etree import ElementTree

async def crawl_sequential(urls: List[str]):
    print("\n=== Sequential Crawling with Session Reuse ===")

    browser_config = BrowserConfig(
        headless=True,
        # For better performance in Docker or low-memory environments:
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )

    crawl_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
    )

    # Create the crawler (opens the browser)
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        session_id = "session1"  # Reuse the same session across all URLs
        for url in urls:
            result = await crawler.arun(
                url=url,
                config=crawl_config,
                session_id=session_id
            )
            if result.success:
                print(f"Successfully crawled: {url}")
                # E.g. check markdown length
                print(f"Markdown length: {len(result.markdown_v2.raw_markdown)}")
            else:
                print(f"Failed: {url} - Error: {result.error_message}")
    finally:
        # After all URLs are done, close the crawler (and the browser)
        await crawler.close()

def get_pydantic_ai_docs_urls():
    """
    Fetches all URLs from the Pydantic AI documentation.
    Uses the sitemap (https://ai.pydantic.dev/sitemap.xml) to get these URLs.
    
    Returns:
        List[str]: List of URLs
    """            
    sitemap_url = "https://ai.pydantic.dev/sitemap.xml"
    try:
        response = requests.get(sitemap_url)
        response.raise_for_status()
        
        # Parse the XML
        root = ElementTree.fromstring(response.content)
        
        # Extract all URLs from the sitemap
        # The namespace is usually defined in the root element
        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]
        
        return urls
    except Exception as e:
        print(f"Error fetching sitemap: {e}")
        return []

async def main():
    urls = get_pydantic_ai_docs_urls()
    if urls:
        print(f"Found {len(urls)} URLs to crawl")
        await crawl_sequential(urls)
    else:
        print("No URLs found to crawl")

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="crawl4AI-examples/3-crawl_docs_FAST.py">
import os
import sys
import psutil
import asyncio
import requests
from xml.etree import ElementTree

__location__ = os.path.dirname(os.path.abspath(__file__))
__output__ = os.path.join(__location__, "output")

# Append parent directory to system path
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)

from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def crawl_parallel(urls: List[str], max_concurrent: int = 3):
    print("\n=== Parallel Crawling with Browser Reuse + Memory Check ===")

    # We'll keep track of peak memory usage across all tasks
    peak_memory = 0
    process = psutil.Process(os.getpid())

    def log_memory(prefix: str = ""):
        nonlocal peak_memory
        current_mem = process.memory_info().rss  # in bytes
        if current_mem > peak_memory:
            peak_memory = current_mem
        print(f"{prefix} Current Memory: {current_mem // (1024 * 1024)} MB, Peak: {peak_memory // (1024 * 1024)} MB")

    # Minimal browser config
    browser_config = BrowserConfig(
        headless=True,
        verbose=False,   # corrected from 'verbos=False'
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )
    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

    # Create the crawler instance
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        # We'll chunk the URLs in batches of 'max_concurrent'
        success_count = 0
        fail_count = 0
        for i in range(0, len(urls), max_concurrent):
            batch = urls[i : i + max_concurrent]
            tasks = []

            for j, url in enumerate(batch):
                # Unique session_id per concurrent sub-task
                session_id = f"parallel_session_{i + j}"
                task = crawler.arun(url=url, config=crawl_config, session_id=session_id)
                tasks.append(task)

            # Check memory usage prior to launching tasks
            log_memory(prefix=f"Before batch {i//max_concurrent + 1}: ")

            # Gather results
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Check memory usage after tasks complete
            log_memory(prefix=f"After batch {i//max_concurrent + 1}: ")

            # Evaluate results
            for url, result in zip(batch, results):
                if isinstance(result, Exception):
                    print(f"Error crawling {url}: {result}")
                    fail_count += 1
                elif result.success:
                    success_count += 1
                else:
                    fail_count += 1

        print(f"\nSummary:")
        print(f"  - Successfully crawled: {success_count}")
        print(f"  - Failed: {fail_count}")

    finally:
        print("\nClosing crawler...")
        await crawler.close()
        # Final memory log
        log_memory(prefix="Final: ")
        print(f"\nPeak memory usage (MB): {peak_memory // (1024 * 1024)}")

def get_pydantic_ai_docs_urls():
    """
    Fetches all URLs from the Pydantic AI documentation.
    Uses the sitemap (https://ai.pydantic.dev/sitemap.xml) to get these URLs.
    
    Returns:
        List[str]: List of URLs
    """            
    sitemap_url = "https://ai.pydantic.dev/sitemap.xml"
    try:
        response = requests.get(sitemap_url)
        response.raise_for_status()
        
        # Parse the XML
        root = ElementTree.fromstring(response.content)
        
        # Extract all URLs from the sitemap
        # The namespace is usually defined in the root element
        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]
        
        return urls
    except Exception as e:
        print(f"Error fetching sitemap: {e}")
        return []        

async def main():
    urls = get_pydantic_ai_docs_urls()
    if urls:
        print(f"Found {len(urls)} URLs to crawl")
        await crawl_parallel(urls, max_concurrent=10)
    else:
        print("No URLs found to crawl")    

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="examples/autogen_integration.py">
"""
Example of integrating Crawl4AI with AutoGen v0.4.7.

This example demonstrates how to use the Crawl4AI agent in a multi-agent system
with AutoGen v0.4.7, including integration with the Claude wrapper.
"""

import os
import sys
import logging
from pathlib import Path

# Add parent directory to path to import crawl4ai
sys.path.append(str(Path(__file__).parent.parent))

from crawl4ai import Crawl4AIAgent
from autogen import UserProxyAgent, ConversableAgent, GroupChat, GroupChatManager
from soln_ai.llm.claude_wrapper import ClaudeWrapper

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """Run the example."""
    # Initialize Claude wrapper
    claude = ClaudeWrapper()
    
    # Test Claude connection
    test_result = claude.test_connection()
    if test_result["success"]:
        logger.info(f"Claude connection successful! Latency: {test_result['latency_seconds']}s")
    else:
        logger.error(f"Claude connection failed: {test_result['error']}")
        return
    
    # Get LLM config for AutoGen
    llm_config = claude.get_llm_config()
    
    # Create the web crawler agent
    crawler = Crawl4AIAgent(
        name="WebCrawler",
        system_message="""You are a specialized web crawler agent. You can navigate websites, 
        extract information, and summarize content. Use your tools to help the team with research tasks.
        Always respect website terms of service and robots.txt.""",
        llm_config=llm_config
    )
    
    # Create a research agent
    researcher = ConversableAgent(
        name="Researcher",
        system_message="""You are a research specialist. You analyze information provided by the WebCrawler
        and identify key insights, patterns, and facts. Ask the WebCrawler to find specific information when needed.""",
        llm_config=llm_config
    )
    
    # Create a writer agent
    writer = ConversableAgent(
        name="Writer",
        system_message="""You are a content writer. You take research findings and create well-structured,
        engaging content. Ask for specific information from the Researcher or WebCrawler if needed.""",
        llm_config=llm_config
    )
    
    # Create a user proxy agent
    user_proxy = UserProxyAgent(
        name="User",
        human_input_mode="TERMINATE",
        code_execution_config={"work_dir": "workspace"}
    )
    
    # Create a group chat
    groupchat = GroupChat(
        agents=[user_proxy, crawler, researcher, writer],
        messages=[],
        max_round=12
    )
    
    # Create a group chat manager
    manager = GroupChatManager(
        groupchat=groupchat,
        llm_config=llm_config
    )
    
    # Start the conversation
    user_proxy.initiate_chat(
        manager,
        message="""I need a comprehensive report on the latest developments in AI safety. 
        Please research recent publications, extract key findings, and create a structured report.
        Focus on developments from the last year."""
    )

if __name__ == "__main__":
    main()
</file>

<file path="n8n-version/Crawl4AI_Agent.json">
{
  "name": "Crawl4AI Agent",
  "nodes": [
    {
      "parameters": {},
      "id": "b90d81a3-0920-419e-bdee-e227ac106383",
      "name": "When clicking ‘Test workflow’",
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        760,
        300
      ]
    },
    {
      "parameters": {
        "url": "https://ai.pydantic.dev/sitemap.xml",
        "options": {}
      },
      "id": "e6033590-2727-4b81-a27b-3dd45dcfbb79",
      "name": "HTTP Request",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        960,
        160
      ]
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "1db8ab3d-4594-45cf-8256-d05d7f7697f7",
      "name": "XML",
      "type": "n8n-nodes-base.xml",
      "typeVersion": 1,
      "position": [
        1160,
        360
      ]
    },
    {
      "parameters": {
        "fieldToSplitOut": "urlset.url",
        "options": {}
      },
      "id": "8eeca9bb-0c16-4ebf-b9f4-db481e963c41",
      "name": "Split Out",
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [
        1320,
        160
      ]
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "460dcd6c-d26c-407c-ad06-016cc800609d",
      "name": "Loop Over Items",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        1560,
        260
      ]
    },
    {
      "parameters": {},
      "id": "8d35dd97-4926-4c22-989a-008d974f9799",
      "name": "Wait",
      "type": "n8n-nodes-base.wait",
      "typeVersion": 1.1,
      "position": [
        1980,
        260
      ],
      "webhookId": "9af87c5e-b07f-48dc-9ca8-61b471a24cad"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://seashell-app-gvc6l.ondigitalocean.app/crawl",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "urls",
              "value": "={{ $json.loc }}"
            },
            {
              "name": "priority",
              "value": "10"
            }
          ]
        },
        "options": {}
      },
      "id": "b644ca6c-33fd-40bb-902f-79025d65b1a3",
      "name": "HTTP Request1",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        1780,
        260
      ],
      "credentials": {
        "httpHeaderAuth": {
          "id": "6wzSkRM1jflKXEHm",
          "name": "testauth"
        }
      }
    },
    {
      "parameters": {
        "url": "=https://seashell-app-gvc6l.ondigitalocean.app/task/{{ $json.task_id }}",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "options": {
          "timeout": 5000
        }
      },
      "id": "f85a925e-348a-4616-ae61-9e5fa60ac8c2",
      "name": "HTTP Request2",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        2200,
        260
      ],
      "retryOnFail": true,
      "waitBetweenTries": 5000,
      "credentials": {
        "httpHeaderAuth": {
          "id": "6wzSkRM1jflKXEHm",
          "name": "testauth"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "9d90c1ce-590e-40a5-ae8c-d92326032975",
              "leftValue": "={{ $json.status }}",
              "rightValue": "completed",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "83e7b017-4196-4e59-b255-921f88b2c5ef",
      "name": "If",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        2420,
        260
      ]
    },
    {
      "parameters": {
        "jsonMode": "expressionData",
        "jsonData": "={{ $json.result.markdown }}",
        "options": {
          "metadata": {
            "metadataValues": [
              {
                "name": "page",
                "value": "={{ $json.result.url }}"
              }
            ]
          }
        }
      },
      "id": "57cd5ed4-0be0-47b7-b278-84192b28c1f0",
      "name": "Default Data Loader",
      "type": "@n8n/n8n-nodes-langchain.documentDefaultDataLoader",
      "typeVersion": 1,
      "position": [
        2800,
        260
      ]
    },
    {
      "parameters": {
        "chunkSize": 5000
      },
      "id": "36b4f50d-2eef-419e-b424-e3203afc9980",
      "name": "Character Text Splitter",
      "type": "@n8n/n8n-nodes-langchain.textSplitterCharacterTextSplitter",
      "typeVersion": 1,
      "position": [
        2940,
        400
      ]
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "bd5af242-1d0c-445f-af84-07e337bfa768",
      "name": "Embeddings OpenAI",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "typeVersion": 1.1,
      "position": [
        2640,
        260
      ],
      "credentials": {
        "openAiApi": {
          "id": "xAeHxzxTT16sMdwS",
          "name": "Backup OpenAI Account"
        }
      }
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "f2bcdb54-e1fe-4670-99aa-6eec973bf5f1",
              "name": "task_id",
              "value": "={{ $('HTTP Request1').item.json.task_id }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "id": "556103b7-217d-4247-8aa0-cb45ede76b3b",
      "name": "Edit Fields",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        2660,
        460
      ]
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "b4054e7f-b429-4f4c-9d55-3004bfd9e1ce",
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "typeVersion": 1.1,
      "position": [
        1420,
        -500
      ],
      "webhookId": "0949763f-f3f7-46bf-8676-c050d92e6966"
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "105b2092-36d3-45d6-9686-673f886933f8",
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1,
      "position": [
        1580,
        -280
      ],
      "credentials": {
        "openAiApi": {
          "id": "xAeHxzxTT16sMdwS",
          "name": "Backup OpenAI Account"
        }
      }
    },
    {
      "parameters": {},
      "id": "1c33efa9-4492-4579-987d-ed0c9d56ecce",
      "name": "Postgres Chat Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryPostgresChat",
      "typeVersion": 1.3,
      "position": [
        1720,
        -280
      ],
      "credentials": {
        "postgres": {
          "id": "PPVUOCGy8dTN3kdl",
          "name": "autogen_studio Test"
        }
      }
    },
    {
      "parameters": {
        "name": "pydantic_ai_docs",
        "description": "Retrieves data related to Pydantic AI using their documentation."
      },
      "id": "95207afc-1362-401f-9ea0-4d9c61bf9b13",
      "name": "Vector Store Tool",
      "type": "@n8n/n8n-nodes-langchain.toolVectorStore",
      "typeVersion": 1,
      "position": [
        2060,
        -380
      ]
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "5c1bfc8e-b208-4c22-93b5-4e66f45a495a",
      "name": "Embeddings OpenAI1",
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "typeVersion": 1.1,
      "position": [
        1820,
        -80
      ],
      "credentials": {
        "openAiApi": {
          "id": "xAeHxzxTT16sMdwS",
          "name": "Backup OpenAI Account"
        }
      }
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "eb940dd5-56e6-4e04-9534-2f910b5d6c8e",
      "name": "OpenAI Chat Model1",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1,
      "position": [
        2200,
        -200
      ],
      "credentials": {
        "openAiApi": {
          "id": "xAeHxzxTT16sMdwS",
          "name": "Backup OpenAI Account"
        }
      }
    },
    {
      "parameters": {
        "mode": "insert",
        "tableName": {
          "__rl": true,
          "value": "documents",
          "mode": "list",
          "cachedResultName": "documents"
        },
        "options": {
          "queryName": "match_documents"
        }
      },
      "id": "45ca72cc-063c-4dfa-9957-fd9d56647374",
      "name": "Supabase Vector Store",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "typeVersion": 1,
      "position": [
        2660,
        40
      ],
      "credentials": {
        "supabaseApi": {
          "id": "Eu6anmgplJHxqZu2",
          "name": "Studio Test"
        }
      }
    },
    {
      "parameters": {
        "options": {}
      },
      "id": "c697f5fe-6d78-4b93-b4f9-a86b7b0aea03",
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 1.7,
      "position": [
        1680,
        -500
      ]
    },
    {
      "parameters": {
        "tableName": {
          "__rl": true,
          "value": "documents",
          "mode": "list",
          "cachedResultName": "documents"
        },
        "options": {
          "queryName": "match_documents"
        }
      },
      "id": "2d9d0203-c24b-4e5e-9c5f-d362bd85b966",
      "name": "Supabase Vector Store1",
      "type": "@n8n/n8n-nodes-langchain.vectorStoreSupabase",
      "typeVersion": 1,
      "position": [
        1860,
        -220
      ],
      "credentials": {
        "supabaseApi": {
          "id": "Eu6anmgplJHxqZu2",
          "name": "Studio Test"
        }
      }
    },
    {
      "parameters": {
        "content": "# n8n + Crawl4AI Agent\n\n## Author: [Cole Medin](https://www.youtube.com/@ColeMedin)\n\nThis AI agent demonstrates how to use a Docker deployment of Crawl4AI to leverage this incredible open source web scraping tool directly in n8n.\n\nThe prerequisite for this workflow is that you have Crawl4AI hosted in a Docker container following these [instructions in the their docs](https://docs.crawl4ai.com/core/docker-deploymeny/).\n\n## How to use this workflow\n\n1. Execute the bottom workflow by clicking on \"Test workflow\". This will ingest all the Pydantic AI documentation into the Supabase DB for RAG.\n\n2. Chat with the agent with the \"Chat\" button - it'll be able to answer questions about Pydantic AI using the documentation as its source!\n\n## Extend this workflow!\n\nThis is just a starting point showing you how to use Crawl4AI in n8n! Feel free to take this along with the Crawl4AI documentation to run wild with building RAG AI agents. The possibilities with this setup are endless!",
        "height": 613.6610941618816,
        "width": 589.875,
        "color": 6
      },
      "id": "71f6f900-60a0-4d15-85f1-88b30f56c492",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        660,
        -540
      ]
    }
  ],
  "pinData": {},
  "connections": {
    "When clicking ‘Test workflow’": {
      "main": [
        [
          {
            "node": "HTTP Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "HTTP Request": {
      "main": [
        [
          {
            "node": "XML",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "XML": {
      "main": [
        [
          {
            "node": "Split Out",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Out": {
      "main": [
        [
          {
            "node": "Loop Over Items",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Loop Over Items": {
      "main": [
        [],
        [
          {
            "node": "HTTP Request1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Wait": {
      "main": [
        [
          {
            "node": "HTTP Request2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "HTTP Request1": {
      "main": [
        [
          {
            "node": "Wait",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "HTTP Request2": {
      "main": [
        [
          {
            "node": "If",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If": {
      "main": [
        [
          {
            "node": "Supabase Vector Store",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Edit Fields",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Default Data Loader": {
      "ai_document": [
        [
          {
            "node": "Supabase Vector Store",
            "type": "ai_document",
            "index": 0
          }
        ]
      ]
    },
    "Character Text Splitter": {
      "ai_textSplitter": [
        [
          {
            "node": "Default Data Loader",
            "type": "ai_textSplitter",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings OpenAI": {
      "ai_embedding": [
        [
          {
            "node": "Supabase Vector Store",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Edit Fields": {
      "main": [
        [
          {
            "node": "Wait",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "When chat message received": {
      "main": [
        [
          {
            "node": "AI Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "AI Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Postgres Chat Memory": {
      "ai_memory": [
        [
          {
            "node": "AI Agent",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "Vector Store Tool": {
      "ai_tool": [
        [
          {
            "node": "AI Agent",
            "type": "ai_tool",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings OpenAI1": {
      "ai_embedding": [
        [
          {
            "node": "Supabase Vector Store1",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model1": {
      "ai_languageModel": [
        [
          {
            "node": "Vector Store Tool",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Supabase Vector Store": {
      "main": [
        [
          {
            "node": "Loop Over Items",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Supabase Vector Store1": {
      "ai_vectorStore": [
        [
          {
            "node": "Vector Store Tool",
            "type": "ai_vectorStore",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "6c3e7c6e-591f-43ad-88d0-05888cc30c4d",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "f65a08c0adc90a3cde2c633d24c6daecde3817033b75588ee10a781b0b7aa3f5"
  },
  "id": "9NoV8Zpk9uLd9pzG",
  "tags": []
}
</file>

<file path="studio-integration-version/.dockerignore">
__pycache__
venv
</file>

<file path="studio-integration-version/.env.example">
# Get your SUPABASE URL from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY=

# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# You only need this environment variable set if you are using GPT (and not Ollama)
OPENAI_API_KEY=

# The LLM you want to use from OpenAI. See the list of models here:
# https://platform.openai.com/docs/models
# Example: gpt-4o-mini
LLM_MODEL=

# Set this bearer token to whatever you want. This will be changed once the agent is hosted for you on the Studio!
API_BEARER_TOKEN=
</file>

<file path="studio-integration-version/Dockerfile">
FROM ottomator/base-python:latest

# Build argument for port with default value
ARG PORT=8001
ENV PORT=${PORT}

WORKDIR /app

# Copy the application code
COPY . .

# Install the Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Expose the port from build argument
EXPOSE ${PORT}

# Command to run the application
CMD ["sh", "-c", "uvicorn pydantic_ai_expert_endpoint:app --host 0.0.0.0 --port ${PORT}"]
</file>

<file path="studio-integration-version/pydantic_ai_expert_endpoint.py">
from typing import List, Optional, Dict, Any
from fastapi import FastAPI, HTTPException, Security, Depends
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from fastapi.middleware.cors import CORSMiddleware
from supabase import create_client, Client
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import AsyncOpenAI
from pathlib import Path
import httpx
import sys
import os

from pydantic_ai.messages import (
    ModelRequest,
    ModelResponse,
    UserPromptPart,
    TextPart
)

from pydantic_ai_expert import pydantic_ai_expert, PydanticAIDeps

# Load environment variables
load_dotenv()

# Initialize FastAPI app
app = FastAPI()
security = HTTPBearer()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Supabase setup
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

# OpenAI setup
openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Request/Response Models
class AgentRequest(BaseModel):
    query: str
    user_id: str
    request_id: str
    session_id: str

class AgentResponse(BaseModel):
    success: bool

def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)) -> bool:
    """Verify the bearer token against environment variable."""
    expected_token = os.getenv("API_BEARER_TOKEN")
    if not expected_token:
        raise HTTPException(
            status_code=500,
            detail="API_BEARER_TOKEN environment variable not set"
        )
    if credentials.credentials != expected_token:
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication token"
        )
    return True    

async def fetch_conversation_history(session_id: str, limit: int = 10) -> List[Dict[str, Any]]:
    """Fetch the most recent conversation history for a session."""
    try:
        response = supabase.table("messages") \
            .select("*") \
            .eq("session_id", session_id) \
            .order("created_at", desc=True) \
            .limit(limit) \
            .execute()
        
        # Convert to list and reverse to get chronological order
        messages = response.data[::-1]
        return messages
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch conversation history: {str(e)}")

async def store_message(session_id: str, message_type: str, content: str, data: Optional[Dict] = None):
    """Store a message in the Supabase messages table."""
    message_obj = {
        "type": message_type,
        "content": content
    }
    if data:
        message_obj["data"] = data

    try:
        supabase.table("messages").insert({
            "session_id": session_id,
            "message": message_obj
        }).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to store message: {str(e)}")

@app.post("/api/pydantic-ai-expert", response_model=AgentResponse)
async def pydantic_ai_expert_endpoint(
    request: AgentRequest,
    authenticated: bool = Depends(verify_token)
):
    try:
        # Fetch conversation history
        conversation_history = await fetch_conversation_history(request.session_id)
        
        # Convert conversation history to format expected by agent
        messages = []
        for msg in conversation_history:
            msg_data = msg["message"]
            msg_type = msg_data["type"]
            msg_content = msg_data["content"]
            msg = ModelRequest(parts=[UserPromptPart(content=msg_content)]) if msg_type == "human" else ModelResponse(parts=[TextPart(content=msg_content)])
            messages.append(msg)

        # Store user's query
        await store_message(
            session_id=request.session_id,
            message_type="human",
            content=request.query
        )            

        # Initialize agent dependencies
        async with httpx.AsyncClient() as client:
            deps = PydanticAIDeps(
                supabase=supabase,
                openai_client=openai_client
            )

            # Run the agent with conversation history
            result = await pydantic_ai_expert.run(
                request.query,
                message_history=messages,
                deps=deps
            )

        # Store agent's response
        await store_message(
            session_id=request.session_id,
            message_type="ai",
            content=result.data,
            data={"request_id": request.request_id}
        )

        return AgentResponse(success=True)

    except Exception as e:
        print(f"Error processing agent request: {str(e)}")
        # Store error message in conversation
        await store_message(
            session_id=request.session_id,
            message_type="ai",
            content="I apologize, but I encountered an error processing your request.",
            data={"error": str(e), "request_id": request.request_id}
        )
        return AgentResponse(success=False)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
</file>

<file path="studio-integration-version/pydantic_ai_expert.py">
from __future__ import annotations as _annotations

from dataclasses import dataclass
from dotenv import load_dotenv
import logfire
import asyncio
import httpx
import os

from pydantic_ai import Agent, ModelRetry, RunContext
from pydantic_ai.models.openai import OpenAIModel
from openai import AsyncOpenAI
from supabase import Client
from typing import List

load_dotenv()

llm = os.getenv('LLM_MODEL', 'gpt-4o-mini')
model = OpenAIModel(llm)

logfire.configure(send_to_logfire='if-token-present')

@dataclass
class PydanticAIDeps:
    supabase: Client
    openai_client: AsyncOpenAI

system_prompt = """
You are an expert at Pydantic AI - a Python AI agent framework that you have access to all the documentation to,
including examples, an API reference, and other resources to help you build Pydantic AI agents.

Your only job is to assist with this and you don't answer other questions besides describing what you are able to do.

Don't ask the user before taking an action, just do it. Always make sure you look at the documentation with the provided tools before answering the user's question unless you have already.

When you first look at the documentation, always start with RAG.
Then also always check the list of available documentation pages and retrieve the content of page(s) if it'll help.

Always let the user know when you didn't find the answer in the documentation or the right URL - be honest.
"""

pydantic_ai_expert = Agent(
    model,
    system_prompt=system_prompt,
    deps_type=PydanticAIDeps,
    retries=2
)

async def get_embedding(text: str, openai_client: AsyncOpenAI) -> List[float]:
    """Get embedding vector from OpenAI."""
    try:
        response = await openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"Error getting embedding: {e}")
        return [0] * 1536  # Return zero vector on error

@pydantic_ai_expert.tool
async def retrieve_relevant_documentation(ctx: RunContext[PydanticAIDeps], user_query: str) -> str:
    """
    Retrieve relevant documentation chunks based on the query with RAG.
    
    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The user's question or query
        
    Returns:
        A formatted string containing the top 5 most relevant documentation chunks
    """
    try:
        # Get the embedding for the query
        query_embedding = await get_embedding(user_query, ctx.deps.openai_client)
        
        # Query Supabase for relevant documents
        result = ctx.deps.supabase.rpc(
            'match_site_pages',
            {
                'query_embedding': query_embedding,
                'match_count': 5,
                'filter': {'source': 'pydantic_ai_docs'}
            }
        ).execute()
        
        if not result.data:
            return "No relevant documentation found."
            
        # Format the results
        formatted_chunks = []
        for doc in result.data:
            chunk_text = f"""
# {doc['title']}

{doc['content']}
"""
            formatted_chunks.append(chunk_text)
            
        # Join all chunks with a separator
        return "\n\n---\n\n".join(formatted_chunks)
        
    except Exception as e:
        print(f"Error retrieving documentation: {e}")
        return f"Error retrieving documentation: {str(e)}"

@pydantic_ai_expert.tool
async def list_documentation_pages(ctx: RunContext[PydanticAIDeps]) -> List[str]:
    """
    Retrieve a list of all available Pydantic AI documentation pages.
    
    Returns:
        List[str]: List of unique URLs for all documentation pages
    """
    try:
        # Query Supabase for unique URLs where source is pydantic_ai_docs
        result = ctx.deps.supabase.from_('site_pages') \
            .select('url') \
            .eq('metadata->>source', 'pydantic_ai_docs') \
            .execute()
        
        if not result.data:
            return []
            
        # Extract unique URLs
        urls = sorted(set(doc['url'] for doc in result.data))
        return urls
        
    except Exception as e:
        print(f"Error retrieving documentation pages: {e}")
        return []

@pydantic_ai_expert.tool
async def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:
    """
    Retrieve the full content of a specific documentation page by combining all its chunks.
    
    Args:
        ctx: The context including the Supabase client
        url: The URL of the page to retrieve
        
    Returns:
        str: The complete page content with all chunks combined in order
    """
    try:
        # Query Supabase for all chunks of this URL, ordered by chunk_number
        result = ctx.deps.supabase.from_('site_pages') \
            .select('title, content, chunk_number') \
            .eq('url', url) \
            .eq('metadata->>source', 'pydantic_ai_docs') \
            .order('chunk_number') \
            .execute()
        
        if not result.data:
            return f"No content found for URL: {url}"
            
        # Format the page with its title and all chunks
        page_title = result.data[0]['title'].split(' - ')[0]  # Get the main title
        formatted_content = [f"# {page_title}\n"]
        
        # Add each chunk's content
        for chunk in result.data:
            formatted_content.append(chunk['content'])
            
        # Join everything together
        return "\n\n".join(formatted_content)
        
    except Exception as e:
        print(f"Error retrieving page content: {e}")
        return f"Error retrieving page content: {str(e)}"
</file>

<file path="tests/test_crawl4ai.py">
"""
Tests for the Crawl4AI agent.
"""

import os
import sys
import unittest
from unittest.mock import patch, MagicMock
from pathlib import Path

# Add parent directory to path to import crawl4ai
sys.path.append(str(Path(__file__).parent.parent))

from crawl4ai import Crawl4AIAgent

class TestCrawl4AIAgent(unittest.TestCase):
    """Test cases for the Crawl4AI agent."""
    
    def setUp(self):
        """Set up test environment."""
        # Create a mock LLM config
        self.llm_config = {
            "model": "claude-3-7-sonnet",
            "temperature": 0.7
        }
        
        # Create the agent
        self.agent = Crawl4AIAgent(
            name="test_crawler",
            system_message="Test crawler agent",
            llm_config=self.llm_config
        )
    
    def test_initialization(self):
        """Test agent initialization."""
        self.assertEqual(self.agent.name, "test_crawler")
        self.assertEqual(self.agent.llm_config, self.llm_config)
        
        # Check default settings
        self.assertGreater(self.agent.rate_limit, 0)
        self.assertGreater(self.agent.max_depth, 0)
        self.assertIsNotNone(self.agent.user_agent)
    
    @patch('requests.get')
    def test_is_valid_url(self, mock_get):
        """Test URL validation."""
        # Valid URLs
        self.assertTrue(self.agent._is_valid_url("https://example.com"))
        self.assertTrue(self.agent._is_valid_url("http://test.org/page"))
        
        # Invalid URLs
        self.assertFalse(self.agent._is_valid_url("not-a-url"))
        self.assertFalse(self.agent._is_valid_url(""))
    
    @patch('requests.get')
    def test_check_robots_txt(self, mock_get):
        """Test robots.txt checking."""
        # Mock response for robots.txt
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.text = """
        User-agent: *
        Disallow: /private/
        Allow: /
        """
        mock_get.return_value = mock_response
        
        # Test allowed URL
        self.assertTrue(self.agent._check_robots_txt("https://example.com/public"))
        
        # Test disallowed URL
        self.assertFalse(self.agent._check_robots_txt("https://example.com/private/data"))
    
    @patch('requests.get')
    def test_crawl_url(self, mock_get):
        """Test URL crawling."""
        # Mock response for HTML page
        mock_html_response = MagicMock()
        mock_html_response.status_code = 200
        mock_html_response.headers = {"Content-Type": "text/html"}
        mock_html_response.text = """
        <html>
            <head>
                <title>Test Page</title>
            </head>
            <body>
                <main>
                    <h1>Test Content</h1>
                    <p>This is a test page.</p>
                    <a href="https://example.com/page1">Link 1</a>
                    <a href="/page2">Link 2</a>
                </main>
            </body>
        </html>
        """
        
        # Mock robots.txt response
        mock_robots_response = MagicMock()
        mock_robots_response.status_code = 200
        mock_robots_response.text = "User-agent: *\nAllow: /"
        
        # Set up mock to return different responses
        def side_effect(url, **kwargs):
            if url.endswith("robots.txt"):
                return mock_robots_response
            return mock_html_response
        
        mock_get.side_effect = side_effect
        
        # Test crawling
        result = self.agent.crawl_url("https://example.com", max_depth=1)
        
        # Check results
        self.assertEqual(result["url"], "https://example.com")
        self.assertGreaterEqual(result["pages_crawled"], 1)
        self.assertIn("https://example.com", result["content"])
        self.assertEqual(result["content"]["https://example.com"]["title"], "Test Page")
        self.assertEqual(result["content"]["https://example.com"]["type"], "html")
        self.assertIn("Test Content", result["content"]["https://example.com"]["text"])
    
    def test_extract_data(self):
        """Test data extraction."""
        # HTML content
        html_content = """
        <html>
            <body>
                <div class="product-item">
                    <h2 class="product-name">Product 1</h2>
                    <span class="product-price">$10.99</span>
                    <p class="product-description">Description 1</p>
                </div>
                <div class="product-item">
                    <h2 class="product-name">Product 2</h2>
                    <span class="product-price">$20.99</span>
                    <p class="product-description">Description 2</p>
                </div>
            </body>
        </html>
        """
        
        # Extraction pattern
        extraction_pattern = {
            "products": {
                "selector": ".product-item",
                "fields": {
                    "name": ".product-name",
                    "price": ".product-price",
                    "description": ".product-description"
                }
            }
        }
        
        # Test extraction
        result = self.agent.extract_data(html_content, extraction_pattern)
        
        # Check results
        self.assertTrue(result["success"])
        self.assertIn("products", result["data"])
        self.assertEqual(len(result["data"]["products"]), 2)
        self.assertEqual(result["data"]["products"][0]["name"], "Product 1")
        self.assertEqual(result["data"]["products"][0]["price"], "$10.99")
        self.assertEqual(result["data"]["products"][1]["name"], "Product 2")
    
    @patch.object(Crawl4AIAgent, '_get_llm_response')
    def test_summarize_content(self, mock_get_llm_response):
        """Test content summarization."""
        # Mock LLM response
        mock_get_llm_response.return_value = {
            "role": "assistant",
            "content": "This is a summary of the test content."
        }
        
        # Test content
        content = "This is a long piece of test content that needs to be summarized."
        
        # Test summarization
        result = self.agent.summarize_content(content, max_length=100)
        
        # Check results
        self.assertTrue(result["success"])
        self.assertIn("summary", result)
        self.assertLessEqual(len(result["summary"]), 100)
    
    def test_set_content_filter(self):
        """Test setting content filter."""
        # Initial values
        initial_threshold = self.agent.content_filter["relevance_threshold"]
        
        # Set new values
        self.agent.set_content_filter(
            relevance_threshold=0.8,
            required_keywords=["test", "example"],
            excluded_sections=[".sidebar", ".comments"]
        )
        
        # Check updated values
        self.assertEqual(self.agent.content_filter["relevance_threshold"], 0.8)
        self.assertEqual(self.agent.content_filter["required_keywords"], ["test", "example"])
        self.assertEqual(self.agent.content_filter["excluded_sections"], [".sidebar", ".comments"])
        
        # Test bounds checking
        self.agent.set_content_filter(relevance_threshold=1.5)
        self.assertEqual(self.agent.content_filter["relevance_threshold"], 1.0)
        
        self.agent.set_content_filter(relevance_threshold=-0.5)
        self.assertEqual(self.agent.content_filter["relevance_threshold"], 0.0)

if __name__ == '__main__':
    unittest.main()
</file>

<file path=".env.example">
# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# You only need this environment variable set if you are using GPT (and not Ollama)
OPENAI_API_KEY=

# For the Supabase version (sample_supabase_agent.py), set your Supabase URL and Service Key.
# Get your SUPABASE_URL from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY=

# The LLM you want to use from OpenAI. See the list of models here:
# https://platform.openai.com/docs/models
# Example: gpt-4o-mini
LLM_MODEL=
</file>

<file path="crawl_pydantic_ai_docs.py">
import os
import sys
import json
import asyncio
import requests
from xml.etree import ElementTree
from typing import List, Dict, Any
from dataclasses import dataclass
from datetime import datetime, timezone
from urllib.parse import urlparse
from dotenv import load_dotenv

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from openai import AsyncOpenAI
from supabase import create_client, Client

load_dotenv()

# Initialize OpenAI and Supabase clients
openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

@dataclass
class ProcessedChunk:
    url: str
    chunk_number: int
    title: str
    summary: str
    content: str
    metadata: Dict[str, Any]
    embedding: List[float]

def chunk_text(text: str, chunk_size: int = 5000) -> List[str]:
    """Split text into chunks, respecting code blocks and paragraphs."""
    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        # Calculate end position
        end = start + chunk_size

        # If we're at the end of the text, just take what's left
        if end >= text_length:
            chunks.append(text[start:].strip())
            break

        # Try to find a code block boundary first (```)
        chunk = text[start:end]
        code_block = chunk.rfind('```')
        if code_block != -1 and code_block > chunk_size * 0.3:
            end = start + code_block

        # If no code block, try to break at a paragraph
        elif '\n\n' in chunk:
            # Find the last paragraph break
            last_break = chunk.rfind('\n\n')
            if last_break > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size
                end = start + last_break

        # If no paragraph break, try to break at a sentence
        elif '. ' in chunk:
            # Find the last sentence break
            last_period = chunk.rfind('. ')
            if last_period > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size
                end = start + last_period + 1

        # Extract chunk and clean it up
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        # Move start position for next chunk
        start = max(start + 1, end)

    return chunks

async def get_title_and_summary(chunk: str, url: str) -> Dict[str, str]:
    """Extract title and summary using GPT-4."""
    system_prompt = """You are an AI that extracts titles and summaries from documentation chunks.
    Return a JSON object with 'title' and 'summary' keys.
    For the title: If this seems like the start of a document, extract its title. If it's a middle chunk, derive a descriptive title.
    For the summary: Create a concise summary of the main points in this chunk.
    Keep both title and summary concise but informative."""
    
    try:
        response = await openai_client.chat.completions.create(
            model=os.getenv("LLM_MODEL", "gpt-4o-mini"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"URL: {url}\n\nContent:\n{chunk[:1000]}..."}  # Send first 1000 chars for context
            ],
            response_format={ "type": "json_object" }
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"Error getting title and summary: {e}")
        return {"title": "Error processing title", "summary": "Error processing summary"}

async def get_embedding(text: str) -> List[float]:
    """Get embedding vector from OpenAI."""
    try:
        response = await openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"Error getting embedding: {e}")
        return [0] * 1536  # Return zero vector on error

async def process_chunk(chunk: str, chunk_number: int, url: str) -> ProcessedChunk:
    """Process a single chunk of text."""
    # Get title and summary
    extracted = await get_title_and_summary(chunk, url)
    
    # Get embedding
    embedding = await get_embedding(chunk)
    
    # Create metadata
    metadata = {
        "source": "pydantic_ai_docs",
        "chunk_size": len(chunk),
        "crawled_at": datetime.now(timezone.utc).isoformat(),
        "url_path": urlparse(url).path
    }
    
    return ProcessedChunk(
        url=url,
        chunk_number=chunk_number,
        title=extracted['title'],
        summary=extracted['summary'],
        content=chunk,  # Store the original chunk content
        metadata=metadata,
        embedding=embedding
    )

async def insert_chunk(chunk: ProcessedChunk):
    """Insert a processed chunk into Supabase."""
    try:
        data = {
            "url": chunk.url,
            "chunk_number": chunk.chunk_number,
            "title": chunk.title,
            "summary": chunk.summary,
            "content": chunk.content,
            "metadata": chunk.metadata,
            "embedding": chunk.embedding
        }
        
        result = supabase.table("site_pages").insert(data).execute()
        print(f"Inserted chunk {chunk.chunk_number} for {chunk.url}")
        return result
    except Exception as e:
        print(f"Error inserting chunk: {e}")
        return None

async def process_and_store_document(url: str, markdown: str):
    """Process a document and store its chunks in parallel."""
    # Split into chunks
    chunks = chunk_text(markdown)
    
    # Process chunks in parallel
    tasks = [
        process_chunk(chunk, i, url) 
        for i, chunk in enumerate(chunks)
    ]
    processed_chunks = await asyncio.gather(*tasks)
    
    # Store chunks in parallel
    insert_tasks = [
        insert_chunk(chunk) 
        for chunk in processed_chunks
    ]
    await asyncio.gather(*insert_tasks)

async def crawl_parallel(urls: List[str], max_concurrent: int = 5):
    """Crawl multiple URLs in parallel with a concurrency limit."""
    browser_config = BrowserConfig(
        headless=True,
        verbose=False,
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )
    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

    # Create the crawler instance
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        # Create a semaphore to limit concurrency
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_url(url: str):
            async with semaphore:
                result = await crawler.arun(
                    url=url,
                    config=crawl_config,
                    session_id="session1"
                )
                if result.success:
                    print(f"Successfully crawled: {url}")
                    await process_and_store_document(url, result.markdown_v2.raw_markdown)
                else:
                    print(f"Failed: {url} - Error: {result.error_message}")
        
        # Process all URLs in parallel with limited concurrency
        await asyncio.gather(*[process_url(url) for url in urls])
    finally:
        await crawler.close()

def get_pydantic_ai_docs_urls() -> List[str]:
    """Get URLs from Pydantic AI docs sitemap."""
    sitemap_url = "https://ai.pydantic.dev/sitemap.xml"
    try:
        response = requests.get(sitemap_url)
        response.raise_for_status()
        
        # Parse the XML
        root = ElementTree.fromstring(response.content)
        
        # Extract all URLs from the sitemap
        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]
        
        return urls
    except Exception as e:
        print(f"Error fetching sitemap: {e}")
        return []

async def main():
    # Get URLs from Pydantic AI docs
    urls = get_pydantic_ai_docs_urls()
    if not urls:
        print("No URLs found to crawl")
        return
    
    print(f"Found {len(urls)} URLs to crawl")
    await crawl_parallel(urls)

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="crawl4ai.py">
"""
Crawl4AI Agent - Intelligent web crawling and data extraction agent for SolnAI.
Built on AutoGen v0.4.7 with Claude 3.7 Sonnet integration.
"""

import os
import time
import logging
import random
import json
import re
from typing import Dict, List, Any, Optional, Union
from urllib.parse import urlparse, urljoin
from pathlib import Path

import requests
from bs4 import BeautifulSoup
import PyPDF2
from PIL import Image
import pytesseract
from dotenv import load_dotenv
from autogen import ConversableAgent

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Crawl4AIAgent(ConversableAgent):
    """
    Agent for web crawling and data extraction.
    
    This agent can autonomously navigate websites, extract structured data,
    and integrate the information with other SolnAI agents.
    """
    
    def __init__(self, name: str, system_message: str, llm_config: Dict[str, Any], **kwargs):
        """
        Initialize the Crawl4AI agent.
        
        Args:
            name: Name of the agent
            system_message: System message for the agent
            llm_config: LLM configuration
            **kwargs: Additional arguments to pass to ConversableAgent
        """
        super().__init__(name=name, system_message=system_message, llm_config=llm_config, **kwargs)
        
        # Register tools
        self.register_tool(self.crawl_url)
        self.register_tool(self.extract_data)
        self.register_tool(self.summarize_content)
        
        # Configure crawler settings
        self.rate_limit = float(os.environ.get("CRAWL4AI_RATE_LIMIT", 5))  # requests per second
        self.max_depth = int(os.environ.get("CRAWL4AI_MAX_DEPTH", 3))
        self.user_agent = os.environ.get(
            "CRAWL4AI_USER_AGENT", 
            "Crawl4AI Bot (https://solnai.com/bot)"
        )
        
        # Initialize memory store
        self.memory = {}
        
        # Content filtering settings
        self.content_filter = {
            "relevance_threshold": 0.5,
            "required_keywords": [],
            "excluded_sections": []
        }
        
        logger.info(f"Crawl4AI agent '{name}' initialized")
    
    def crawl_url(self, url: str, max_depth: int = 1, follow_links: bool = True) -> Dict[str, Any]:
        """
        Crawl a URL and its linked pages up to max_depth.
        
        Args:
            url: The URL to crawl
            max_depth: Maximum depth to crawl (default: 1)
            follow_links: Whether to follow links (default: True)
            
        Returns:
            Dict with crawling results
        """
        logger.info(f"Crawling URL: {url} (max_depth={max_depth}, follow_links={follow_links})")
        
        # Validate URL
        if not self._is_valid_url(url):
            return {"error": f"Invalid URL: {url}"}
        
        # Check robots.txt
        if not self._check_robots_txt(url):
            return {"error": f"URL not allowed by robots.txt: {url}"}
        
        # Initialize results
        results = {
            "url": url,
            "pages_crawled": 0,
            "content": {},
            "links": [],
            "errors": []
        }
        
        # Set maximum depth
        max_depth = min(max_depth, self.max_depth)
        
        # Crawl the URL and its linked pages
        visited = set()
        to_visit = [(url, 0)]  # (url, depth)
        
        while to_visit:
            current_url, depth = to_visit.pop(0)
            
            # Skip if already visited or exceeds max depth
            if current_url in visited or depth > max_depth:
                continue
            
            # Mark as visited
            visited.add(current_url)
            
            try:
                # Respect rate limit
                time.sleep(1 / self.rate_limit)
                
                # Fetch page content
                headers = {"User-Agent": self.user_agent}
                response = requests.get(current_url, headers=headers, timeout=10)
                response.raise_for_status()
                
                # Process content based on content type
                content_type = response.headers.get("Content-Type", "").lower()
                
                if "text/html" in content_type:
                    # Process HTML content
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    # Extract page title
                    title = soup.title.string if soup.title else current_url
                    
                    # Extract main content
                    main_content = self._extract_main_content(soup)
                    
                    # Store content
                    results["content"][current_url] = {
                        "title": title,
                        "type": "html",
                        "text": main_content,
                        "metadata": {
                            "url": current_url,
                            "depth": depth,
                            "content_type": content_type
                        }
                    }
                    
                    # Extract links if follow_links is True and depth < max_depth
                    if follow_links and depth < max_depth:
                        links = self._extract_links(soup, current_url)
                        results["links"].extend(links)
                        
                        # Add links to visit queue
                        for link in links:
                            if link not in visited:
                                to_visit.append((link, depth + 1))
                
                elif "application/pdf" in content_type:
                    # Process PDF content
                    pdf_text = self._extract_pdf_content(response.content)
                    
                    # Store content
                    results["content"][current_url] = {
                        "title": os.path.basename(urlparse(current_url).path),
                        "type": "pdf",
                        "text": pdf_text,
                        "metadata": {
                            "url": current_url,
                            "depth": depth,
                            "content_type": content_type
                        }
                    }
                
                elif "image/" in content_type:
                    # Process image content with OCR
                    image_text = self._extract_image_text(response.content)
                    
                    # Store content
                    results["content"][current_url] = {
                        "title": os.path.basename(urlparse(current_url).path),
                        "type": "image",
                        "text": image_text,
                        "metadata": {
                            "url": current_url,
                            "depth": depth,
                            "content_type": content_type
                        }
                    }
                
                else:
                    # Skip unsupported content types
                    logger.warning(f"Unsupported content type: {content_type} for URL: {current_url}")
                    results["errors"].append(f"Unsupported content type: {content_type} for URL: {current_url}")
                    continue
                
                # Increment pages crawled counter
                results["pages_crawled"] += 1
                
            except Exception as e:
                logger.error(f"Error crawling URL {current_url}: {str(e)}")
                results["errors"].append(f"Error crawling URL {current_url}: {str(e)}")
        
        logger.info(f"Crawling completed: {results['pages_crawled']} pages crawled, {len(results['errors'])} errors")
        return results
    
    def extract_data(self, html_content: str, extraction_pattern: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract structured data from HTML content.
        
        Args:
            html_content: HTML content to extract data from
            extraction_pattern: Pattern defining what to extract
            
        Returns:
            Dict with extracted data
        """
        logger.info("Extracting data using pattern")
        
        try:
            # Parse HTML
            soup = BeautifulSoup(html_content, "html.parser")
            
            # Initialize results
            results = {
                "success": True,
                "data": {}
            }
            
            # Extract data based on pattern
            for section_name, section_config in extraction_pattern.items():
                section_selector = section_config.get("selector", "")
                section_elements = soup.select(section_selector)
                
                section_data = []
                
                for element in section_elements:
                    item_data = {}
                    
                    # Extract fields
                    for field_name, field_selector in section_config.get("fields", {}).items():
                        field_element = element.select_one(field_selector)
                        item_data[field_name] = field_element.get_text(strip=True) if field_element else None
                    
                    section_data.append(item_data)
                
                results["data"][section_name] = section_data
            
            return results
            
        except Exception as e:
            logger.error(f"Error extracting data: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def summarize_content(self, content: str, max_length: int = 500) -> Dict[str, Any]:
        """
        Generate a summary of the content using Claude.
        
        Args:
            content: Content to summarize
            max_length: Maximum length of summary in characters
            
        Returns:
            Dict with summary
        """
        logger.info(f"Summarizing content (max_length={max_length})")
        
        try:
            # Truncate content if too long
            if len(content) > 100000:  # Limit to 100k characters to avoid token limits
                content = content[:100000] + "..."
            
            # Create prompt for summarization
            prompt = f"""
            Please summarize the following content in a concise way, highlighting the key points.
            Keep the summary under {max_length} characters.
            
            CONTENT:
            {content}
            
            SUMMARY:
            """
            
            # Use the LLM to generate summary
            messages = [{"role": "user", "content": prompt}]
            
            # Get the LLM response
            response = self._get_llm_response(messages)
            
            # Extract summary from response
            summary = response.get("content", "").strip()
            
            # Truncate if still too long
            if len(summary) > max_length:
                summary = summary[:max_length] + "..."
            
            return {
                "success": True,
                "summary": summary,
                "length": len(summary)
            }
            
        except Exception as e:
            logger.error(f"Error summarizing content: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def set_content_filter(self, relevance_threshold: float = None, 
                          required_keywords: List[str] = None,
                          excluded_sections: List[str] = None) -> None:
        """
        Set content filtering parameters.
        
        Args:
            relevance_threshold: Minimum relevance score (0-1)
            required_keywords: List of keywords that must be present
            excluded_sections: List of CSS selectors to exclude
        """
        if relevance_threshold is not None:
            self.content_filter["relevance_threshold"] = max(0.0, min(1.0, relevance_threshold))
        
        if required_keywords is not None:
            self.content_filter["required_keywords"] = required_keywords
        
        if excluded_sections is not None:
            self.content_filter["excluded_sections"] = excluded_sections
        
        logger.info(f"Content filter updated: {self.content_filter}")
    
    def _is_valid_url(self, url: str) -> bool:
        """Check if a URL is valid."""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
    
    def _check_robots_txt(self, url: str) -> bool:
        """Check if a URL is allowed by robots.txt."""
        try:
            parsed_url = urlparse(url)
            robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
            
            response = requests.get(robots_url, timeout=5)
            if response.status_code != 200:
                # If robots.txt doesn't exist or can't be fetched, assume allowed
                return True
            
            # Very basic robots.txt parsing
            # A more robust implementation would use a proper parser
            lines = response.text.split('\n')
            user_agent_applies = False
            
            for line in lines:
                line = line.strip().lower()
                
                if line.startswith('user-agent:'):
                    agent = line[11:].strip()
                    if agent == '*' or self.user_agent.lower().find(agent) != -1:
                        user_agent_applies = True
                    else:
                        user_agent_applies = False
                
                if user_agent_applies and line.startswith('disallow:'):
                    disallow_path = line[9:].strip()
                    if disallow_path and parsed_url.path.startswith(disallow_path):
                        return False
            
            return True
            
        except Exception as e:
            logger.warning(f"Error checking robots.txt for {url}: {str(e)}")
            # If there's an error checking robots.txt, assume allowed
            return True
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """Extract the main content from an HTML page."""
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.extract()
        
        # Remove excluded sections
        for selector in self.content_filter["excluded_sections"]:
            for element in soup.select(selector):
                element.extract()
        
        # Try to find main content
        main_content = None
        
        # Look for common content containers
        for selector in ["main", "article", "#content", ".content", "#main", ".main"]:
            content = soup.select_one(selector)
            if content:
                main_content = content
                break
        
        # If no main content found, use body
        if not main_content:
            main_content = soup.body
        
        # If still no content, use the whole soup
        if not main_content:
            main_content = soup
        
        # Get text
        text = main_content.get_text(separator='\n', strip=True)
        
        # Check for required keywords
        if self.content_filter["required_keywords"]:
            text_lower = text.lower()
            if not all(keyword.lower() in text_lower for keyword in self.content_filter["required_keywords"]):
                return ""
        
        return text
    
    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Extract links from an HTML page."""
        links = []
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            
            # Skip empty links, anchors, and javascript
            if not href or href.startswith('#') or href.startswith('javascript:'):
                continue
            
            # Convert relative URLs to absolute
            absolute_url = urljoin(base_url, href)
            
            # Skip mailto and tel links
            if absolute_url.startswith(('mailto:', 'tel:')):
                continue
            
            # Ensure it's from the same domain (optional)
            # base_domain = urlparse(base_url).netloc
            # link_domain = urlparse(absolute_url).netloc
            # if link_domain != base_domain:
            #     continue
            
            links.append(absolute_url)
        
        return links
    
    def _extract_pdf_content(self, pdf_bytes: bytes) -> str:
        """Extract text from PDF content."""
        try:
            from io import BytesIO
            
            # Create a file-like object from bytes
            pdf_file = BytesIO(pdf_bytes)
            
            # Create PDF reader
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            # Extract text from all pages
            text = ""
            for page_num in range(len(pdf_reader.pages)):
                text += pdf_reader.pages[page_num].extract_text() + "\n\n"
            
            return text
            
        except Exception as e:
            logger.error(f"Error extracting PDF content: {str(e)}")
            return f"[PDF extraction error: {str(e)}]"
    
    def _extract_image_text(self, image_bytes: bytes) -> str:
        """Extract text from image using OCR."""
        try:
            from io import BytesIO
            
            # Create a file-like object from bytes
            image_file = BytesIO(image_bytes)
            
            # Open the image
            image = Image.open(image_file)
            
            # Use pytesseract for OCR
            text = pytesseract.image_to_string(image)
            
            return text
            
        except Exception as e:
            logger.error(f"Error extracting image text: {str(e)}")
            return f"[Image OCR error: {str(e)}]"
    
    def _get_llm_response(self, messages: List[Dict[str, str]]) -> Dict[str, str]:
        """
        Get a response from the LLM.
        
        This is a helper method that uses the agent's LLM to generate a response.
        """
        # This is a simplified implementation
        # In a real implementation, this would use the agent's LLM configuration
        # to generate a response
        
        # For now, we'll just return a mock response
        return {
            "role": "assistant",
            "content": "This is a mock summary of the content."
        }


# Example usage
if __name__ == "__main__":
    # Initialize the agent
    crawler = Crawl4AIAgent(
        name="web_crawler",
        system_message="You are a helpful web crawler that finds and extracts information.",
        llm_config={
            "model": "claude-3-7-sonnet",
            "temperature": 0.7
        }
    )
    
    # Example crawl
    results = crawler.crawl_url("https://example.com", max_depth=1)
    print(json.dumps(results, indent=2))
</file>

<file path="pydantic_ai_expert.py">
from __future__ import annotations as _annotations

from dataclasses import dataclass
from dotenv import load_dotenv
import logfire
import asyncio
import httpx
import os

from pydantic_ai import Agent, ModelRetry, RunContext
from pydantic_ai.models.openai import OpenAIModel
from openai import AsyncOpenAI
from supabase import Client
from typing import List

load_dotenv()

llm = os.getenv('LLM_MODEL', 'gpt-4o-mini')
model = OpenAIModel(llm)

logfire.configure(send_to_logfire='if-token-present')

@dataclass
class PydanticAIDeps:
    supabase: Client
    openai_client: AsyncOpenAI

system_prompt = """
You are an expert at Pydantic AI - a Python AI agent framework that you have access to all the documentation to,
including examples, an API reference, and other resources to help you build Pydantic AI agents.

Your only job is to assist with this and you don't answer other questions besides describing what you are able to do.

Don't ask the user before taking an action, just do it. Always make sure you look at the documentation with the provided tools before answering the user's question unless you have already.

When you first look at the documentation, always start with RAG.
Then also always check the list of available documentation pages and retrieve the content of page(s) if it'll help.

Always let the user know when you didn't find the answer in the documentation or the right URL - be honest.
"""

pydantic_ai_expert = Agent(
    model,
    system_prompt=system_prompt,
    deps_type=PydanticAIDeps,
    retries=2
)

async def get_embedding(text: str, openai_client: AsyncOpenAI) -> List[float]:
    """Get embedding vector from OpenAI."""
    try:
        response = await openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"Error getting embedding: {e}")
        return [0] * 1536  # Return zero vector on error

@pydantic_ai_expert.tool
async def retrieve_relevant_documentation(ctx: RunContext[PydanticAIDeps], user_query: str) -> str:
    """
    Retrieve relevant documentation chunks based on the query with RAG.
    
    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The user's question or query
        
    Returns:
        A formatted string containing the top 5 most relevant documentation chunks
    """
    try:
        # Get the embedding for the query
        query_embedding = await get_embedding(user_query, ctx.deps.openai_client)
        
        # Query Supabase for relevant documents
        result = ctx.deps.supabase.rpc(
            'match_site_pages',
            {
                'query_embedding': query_embedding,
                'match_count': 5,
                'filter': {'source': 'pydantic_ai_docs'}
            }
        ).execute()
        
        if not result.data:
            return "No relevant documentation found."
            
        # Format the results
        formatted_chunks = []
        for doc in result.data:
            chunk_text = f"""
# {doc['title']}

{doc['content']}
"""
            formatted_chunks.append(chunk_text)
            
        # Join all chunks with a separator
        return "\n\n---\n\n".join(formatted_chunks)
        
    except Exception as e:
        print(f"Error retrieving documentation: {e}")
        return f"Error retrieving documentation: {str(e)}"

@pydantic_ai_expert.tool
async def list_documentation_pages(ctx: RunContext[PydanticAIDeps]) -> List[str]:
    """
    Retrieve a list of all available Pydantic AI documentation pages.
    
    Returns:
        List[str]: List of unique URLs for all documentation pages
    """
    try:
        # Query Supabase for unique URLs where source is pydantic_ai_docs
        result = ctx.deps.supabase.from_('site_pages') \
            .select('url') \
            .eq('metadata->>source', 'pydantic_ai_docs') \
            .execute()
        
        if not result.data:
            return []
            
        # Extract unique URLs
        urls = sorted(set(doc['url'] for doc in result.data))
        return urls
        
    except Exception as e:
        print(f"Error retrieving documentation pages: {e}")
        return []

@pydantic_ai_expert.tool
async def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:
    """
    Retrieve the full content of a specific documentation page by combining all its chunks.
    
    Args:
        ctx: The context including the Supabase client
        url: The URL of the page to retrieve
        
    Returns:
        str: The complete page content with all chunks combined in order
    """
    try:
        # Query Supabase for all chunks of this URL, ordered by chunk_number
        result = ctx.deps.supabase.from_('site_pages') \
            .select('title, content, chunk_number') \
            .eq('url', url) \
            .eq('metadata->>source', 'pydantic_ai_docs') \
            .order('chunk_number') \
            .execute()
        
        if not result.data:
            return f"No content found for URL: {url}"
            
        # Format the page with its title and all chunks
        page_title = result.data[0]['title'].split(' - ')[0]  # Get the main title
        formatted_content = [f"# {page_title}\n"]
        
        # Add each chunk's content
        for chunk in result.data:
            formatted_content.append(chunk['content'])
            
        # Join everything together
        return "\n\n".join(formatted_content)
        
    except Exception as e:
        print(f"Error retrieving page content: {e}")
        return f"Error retrieving page content: {str(e)}"
</file>

<file path="README.md">
# Crawl4AI Agent for SolnAI

## Overview

Crawl4AI is a specialized agent for SolnAI that performs intelligent web crawling and data extraction. Built on AutoGen v0.4.7, this agent can autonomously navigate websites, extract structured data, and integrate the information with other SolnAI agents.

## Features

- **Intelligent Web Crawling**: Autonomously navigates websites following semantic relevance rather than just link structure
- **Selective Data Extraction**: Identifies and extracts relevant information based on context and user requirements
- **Content Summarization**: Generates concise summaries of crawled content using Claude 3.7 Sonnet
- **Multi-format Support**: Handles various content types including HTML, PDF, images (via OCR), and JavaScript-rendered content
- **Rate Limiting & Politeness**: Built-in mechanisms to respect robots.txt, rate limits, and ethical crawling practices
- **Integration with SolnAI**: Seamlessly works with other agents in the SolnAI ecosystem

## Architecture

The Crawl4AI agent consists of several components:

1. **Crawler Engine**: Core component for web navigation and content retrieval
2. **Content Processor**: Extracts and processes different content types
3. **LLM Integration**: Uses Claude 3.7 Sonnet for content understanding and summarization
4. **Memory Store**: Maintains context and history of crawled content
5. **API Interface**: Exposes functionality to other SolnAI agents

## Installation

```bash
# From the SolnAI-agents directory
cd crawl4AI-agent
pip install -r requirements.txt
```

## Configuration

Create a `.env` file with the following variables:

```
ANTHROPIC_API_KEY=your_api_key_here
CRAWL4AI_RATE_LIMIT=5  # requests per second
CRAWL4AI_MAX_DEPTH=3   # maximum crawl depth
CRAWL4AI_USER_AGENT="Crawl4AI Bot (https://solnai.com/bot)"
```

## Usage

### Basic Usage

```python
from crawl4ai import Crawl4AIAgent
from autogen import UserProxyAgent

# Initialize the agent
crawler = Crawl4AIAgent(
    name="web_crawler",
    system_message="You are a helpful web crawler that finds and extracts information.",
    llm_config={
        "model": "claude-3-7-sonnet",
        "temperature": 0.7
    }
)

# Create a user proxy
user = UserProxyAgent(
    name="user",
    human_input_mode="TERMINATE",
    code_execution_config={"work_dir": "crawl_workspace"}
)

# Start a conversation
user.initiate_chat(
    crawler,
    message="Crawl https://example.com and extract all product information."
)
```

### Integration with SolnAI

```python
from soln_ai.agents import TeamManager
from crawl4ai import Crawl4AIAgent
from soln_ai.llm.claude_wrapper import ClaudeWrapper

# Initialize Claude wrapper
claude = ClaudeWrapper()

# Create the crawler agent
crawler = Crawl4AIAgent(
    name="web_crawler",
    system_message="You are a helpful web crawler that finds and extracts information.",
    llm_config=claude.get_llm_config()
)

# Add to a team
team = TeamManager()
team.add_agent(crawler)
team.add_agent(research_agent)
team.add_agent(writer_agent)

# Start the team task
team.initiate_task("Research the latest AI trends and write a report.")
```

## API Reference

### Crawl4AIAgent Class

```python
class Crawl4AIAgent(ConversableAgent):
    """
    Agent for web crawling and data extraction.
    """
    
    def __init__(self, name, system_message, llm_config, **kwargs):
        """Initialize the Crawl4AI agent."""
        super().__init__(name=name, system_message=system_message, llm_config=llm_config, **kwargs)
        
        # Register tools
        self.register_tool(self.crawl_url)
        self.register_tool(self.extract_data)
        self.register_tool(self.summarize_content)
        
    def crawl_url(self, url, max_depth=1, follow_links=True):
        """Crawl a URL and its linked pages up to max_depth."""
        # Implementation details...
        
    def extract_data(self, html_content, extraction_pattern):
        """Extract structured data from HTML content."""
        # Implementation details...
        
    def summarize_content(self, content, max_length=500):
        """Generate a summary of the content."""
        # Implementation details...
```

## Advanced Features

### Custom Extraction Patterns

You can define custom extraction patterns to target specific information:

```python
extraction_pattern = {
    "products": {
        "selector": ".product-item",
        "fields": {
            "name": ".product-name",
            "price": ".product-price",
            "description": ".product-description"
        }
    }
}

results = crawler.extract_data(html_content, extraction_pattern)
```

### Content Filtering

Filter crawled content based on relevance:

```python
crawler.set_content_filter(
    relevance_threshold=0.7,
    required_keywords=["AI", "machine learning"],
    excluded_sections=[".sidebar", ".comments"]
)
```

## Limitations

- JavaScript-heavy sites may require additional configuration
- Some websites may block automated crawling
- Processing large volumes of content may require significant resources
- OCR functionality for images has limited accuracy

## Contributing

Contributions to Crawl4AI are welcome! Please see our [contributing guidelines](../../CONTRIBUTING.md) for more information.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="requirements.txt">
aiofiles==24.1.0
aiohappyeyeballs==2.4.4
aiohttp==3.11.11
aiosignal==1.3.2
aiosqlite==0.20.0
altair==5.5.0
annotated-types==0.7.0
anthropic==0.42.0
anyio==4.8.0
attrs==24.3.0
beautifulsoup4==4.12.3
blinker==1.9.0
cachetools==5.5.0
certifi==2024.12.14
cffi==1.17.1
charset-normalizer==3.4.1
click==8.1.8
colorama==0.4.6
Crawl4AI==0.4.247
cryptography==44.0.0
Deprecated==1.2.15
deprecation==2.1.0
distro==1.9.0
eval_type_backport==0.2.2
executing==2.1.0
fake-http-header==0.3.5
filelock==3.16.1
frozenlist==1.5.0
fsspec==2024.12.0
gitdb==4.0.12
GitPython==3.1.44
google-auth==2.37.0
googleapis-common-protos==1.66.0
gotrue==2.11.1
greenlet==3.1.1
griffe==1.5.4
groq==0.15.0
h11==0.14.0
h2==4.1.0
hpack==4.0.0
httpcore==1.0.7
httpx==0.27.2
huggingface-hub==0.27.1
hyperframe==6.0.1
idna==3.10
importlib_metadata==8.5.0
iniconfig==2.0.0
Jinja2==3.1.5
jiter==0.8.2
joblib==1.4.2
jsonpath-python==1.0.6
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
litellm==1.57.8
logfire==3.1.0
logfire-api==3.1.0
lxml==5.3.0
markdown-it-py==3.0.0
MarkupSafe==3.0.2
mdurl==0.1.2
mistralai==1.2.6
mockito==1.5.3
multidict==6.1.0
mypy-extensions==1.0.0
narwhals==1.21.1
nltk==3.9.1
numpy==2.2.1
openai==1.59.6
opentelemetry-api==1.29.0
opentelemetry-exporter-otlp-proto-common==1.29.0
opentelemetry-exporter-otlp-proto-http==1.29.0
opentelemetry-instrumentation==0.50b0
opentelemetry-proto==1.29.0
opentelemetry-sdk==1.29.0
opentelemetry-semantic-conventions==0.50b0
packaging==24.2
pandas==2.2.3
pillow==10.4.0
playwright==1.49.1
pluggy==1.5.0
postgrest==0.19.1
propcache==0.2.1
protobuf==5.29.3
psutil==6.1.1
pyarrow==18.1.0
pyasn1==0.6.1
pyasn1_modules==0.4.1
pycparser==2.22
pydantic==2.10.5
pydantic-ai==0.0.18
pydantic-ai-slim==0.0.18
pydantic_core==2.27.2
pydeck==0.9.1
pyee==12.0.0
Pygments==2.19.1
pyOpenSSL==24.3.0
pytest==8.3.4
pytest-mockito==0.0.4
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytz==2024.2
PyYAML==6.0.2
rank-bm25==0.2.2
realtime==2.1.0
referencing==0.35.1
regex==2024.11.6
requests==2.32.3
rich==13.9.4
rpds-py==0.22.3
rsa==4.9
six==1.17.0
smmap==5.0.2
sniffio==1.3.1
snowballstemmer==2.2.0
soupsieve==2.6
storage3==0.11.0
streamlit==1.41.1
StrEnum==0.4.15
supabase==2.11.0
supafunc==0.9.0
tenacity==9.0.0
tf-playwright-stealth==1.1.0
tiktoken==0.8.0
tokenizers==0.21.0
toml==0.10.2
tornado==6.4.2
tqdm==4.67.1
typing-inspect==0.9.0
typing_extensions==4.12.2
tzdata==2024.2
urllib3==2.3.0
watchdog==6.0.0
websockets==13.1
wrapt==1.17.1
xxhash==3.5.0
yarl==1.18.3
zipp==3.21.0
</file>

<file path="site_pages.sql">
-- Enable the pgvector extension
create extension if not exists vector;

-- Create the documentation chunks table
create table site_pages (
    id bigserial primary key,
    url varchar not null,
    chunk_number integer not null,
    title varchar not null,
    summary varchar not null,
    content text not null,  -- Added content column
    metadata jsonb not null default '{}'::jsonb,  -- Added metadata column
    embedding vector(1536),  -- OpenAI embeddings are 1536 dimensions
    created_at timestamp with time zone default timezone('utc'::text, now()) not null,
    
    -- Add a unique constraint to prevent duplicate chunks for the same URL
    unique(url, chunk_number)
);

-- Create an index for better vector similarity search performance
create index on site_pages using ivfflat (embedding vector_cosine_ops);

-- Create an index on metadata for faster filtering
create index idx_site_pages_metadata on site_pages using gin (metadata);

-- Create a function to search for documentation chunks
create function match_site_pages (
  query_embedding vector(1536),
  match_count int default 10,
  filter jsonb DEFAULT '{}'::jsonb
) returns table (
  id bigint,
  url varchar,
  chunk_number integer,
  title varchar,
  summary varchar,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    url,
    chunk_number,
    title,
    summary,
    content,
    metadata,
    1 - (site_pages.embedding <=> query_embedding) as similarity
  from site_pages
  where metadata @> filter
  order by site_pages.embedding <=> query_embedding
  limit match_count;
end;
$$;

-- Everything above will work for any PostgreSQL database. The below commands are for Supabase security

-- Enable RLS on the table
alter table site_pages enable row level security;

-- Create a policy that allows anyone to read
create policy "Allow public read access"
  on site_pages
  for select
  to public
  using (true);
</file>

<file path="streamlit_ui.py">
from __future__ import annotations
from typing import Literal, TypedDict
import asyncio
import os

import streamlit as st
import json
import logfire
from supabase import Client
from openai import AsyncOpenAI

# Import all the message part classes
from pydantic_ai.messages import (
    ModelMessage,
    ModelRequest,
    ModelResponse,
    SystemPromptPart,
    UserPromptPart,
    TextPart,
    ToolCallPart,
    ToolReturnPart,
    RetryPromptPart,
    ModelMessagesTypeAdapter
)
from pydantic_ai_expert import pydantic_ai_expert, PydanticAIDeps

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
supabase: Client = Client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

# Configure logfire to suppress warnings (optional)
logfire.configure(send_to_logfire='never')

class ChatMessage(TypedDict):
    """Format of messages sent to the browser/API."""

    role: Literal['user', 'model']
    timestamp: str
    content: str


def display_message_part(part):
    """
    Display a single part of a message in the Streamlit UI.
    Customize how you display system prompts, user prompts,
    tool calls, tool returns, etc.
    """
    # system-prompt
    if part.part_kind == 'system-prompt':
        with st.chat_message("system"):
            st.markdown(f"**System**: {part.content}")
    # user-prompt
    elif part.part_kind == 'user-prompt':
        with st.chat_message("user"):
            st.markdown(part.content)
    # text
    elif part.part_kind == 'text':
        with st.chat_message("assistant"):
            st.markdown(part.content)          


async def run_agent_with_streaming(user_input: str):
    """
    Run the agent with streaming text for the user_input prompt,
    while maintaining the entire conversation in `st.session_state.messages`.
    """
    # Prepare dependencies
    deps = PydanticAIDeps(
        supabase=supabase,
        openai_client=openai_client
    )

    # Run the agent in a stream
    async with pydantic_ai_expert.run_stream(
        user_input,
        deps=deps,
        message_history= st.session_state.messages[:-1],  # pass entire conversation so far
    ) as result:
        # We'll gather partial text to show incrementally
        partial_text = ""
        message_placeholder = st.empty()

        # Render partial text as it arrives
        async for chunk in result.stream_text(delta=True):
            partial_text += chunk
            message_placeholder.markdown(partial_text)

        # Now that the stream is finished, we have a final result.
        # Add new messages from this run, excluding user-prompt messages
        filtered_messages = [msg for msg in result.new_messages() 
                            if not (hasattr(msg, 'parts') and 
                                    any(part.part_kind == 'user-prompt' for part in msg.parts))]
        st.session_state.messages.extend(filtered_messages)

        # Add the final response to the messages
        st.session_state.messages.append(
            ModelResponse(parts=[TextPart(content=partial_text)])
        )


async def main():
    st.title("Pydantic AI Agentic RAG")
    st.write("Ask any question about Pydantic AI, the hidden truths of the beauty of this framework lie within.")

    # Initialize chat history in session state if not present
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display all messages from the conversation so far
    # Each message is either a ModelRequest or ModelResponse.
    # We iterate over their parts to decide how to display them.
    for msg in st.session_state.messages:
        if isinstance(msg, ModelRequest) or isinstance(msg, ModelResponse):
            for part in msg.parts:
                display_message_part(part)

    # Chat input for the user
    user_input = st.chat_input("What questions do you have about Pydantic AI?")

    if user_input:
        # We append a new request to the conversation explicitly
        st.session_state.messages.append(
            ModelRequest(parts=[UserPromptPart(content=user_input)])
        )
        
        # Display user prompt in the UI
        with st.chat_message("user"):
            st.markdown(user_input)

        # Display the assistant's partial response while streaming
        with st.chat_message("assistant"):
            # Actually run the agent now, streaming the text
            await run_agent_with_streaming(user_input)


if __name__ == "__main__":
    asyncio.run(main())
</file>

</files>
