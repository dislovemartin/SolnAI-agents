This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
config/
  agent_system_prompt.txt
  app_config.json
  generate_app_ideas_from_question_user_prompt.txt
  generate_app_ideas_system_prompt.txt
  generate_app_ideas_user_prompt.txt
  generate_app_names_from_question_user_prompt.txt
  generate_app_names_system_prompt.txt
  generate_app_names_user_prompt.txt
  generate_app_presentation_from_question_user_prompt.txt
  generate_app_presentation_system_prompt.txt
  generate_app_presentation_user_prompt.txt
  generate_app_structure_from_question_user_prompt.txt
  generate_app_structure_system_prompt.txt
  generate_app_structure_user_prompt.txt
  refine_llm_system_prompt.txt
  refine_llm_user_prompt.txt
  refine_video_system_prompt.txt
  refine_video_user_prompt.txt
  schema_generator_ref_files.json
  suggestions_suffix_user_prompt.txt
  suggestions_text_system_prompt.txt
  suggestions_text_user_prompt.txt
db/
  .gitignore
embeddings_sources/
  .gitignore
gsam_ottomator_agent/
  base_python_docker/
    Dockerfile
    README.md
    requirements.txt
  .env.example
  Dockerfile
  gsam_agent_lib.py
  gsam_postgres_agent.py
  gsam_supabase_agent.py
  Makefile
  README.md
  run_agent.sh
images/
  .gitignore
input/
  .gitignore
  user_input_example.md
lib/
  codegen_ai_abstracts_constants.py
  codegen_ai_abstracts.py
  codegen_ai_provider_aimlapi.py
  codegen_ai_provider_groq.py
  codegen_ai_provider_huggingface.py
  codegen_ai_provider_nvidia.py
  codegen_ai_provider_ollama.py
  codegen_ai_provider_openai.py
  codegen_ai_provider_openrouter.py
  codegen_ai_provider_rhymes.py
  codegen_ai_provider_together_ai.py
  codegen_ai_provider_xai.py
  codegen_ai_utilities.py
  codegen_app_ideation_lib.py
  codegen_db_abstracts.py
  codegen_db_json.py
  codegen_db_mongodb.py
  codegen_db.py
  codegen_general_lib.py
  codegen_generation_lib.py
  codegen_ideation_lib.py
  codegen_llamaindex_abstraction.py
  codegen_powerpoint.py
  codegen_schema_generator.py
  codegen_streamlit_lib.py
  codegen_utilities.py
output/
  .gitignore
scripts/
  run_app.sh
  run_schema_generator.sh
src/
  codegen_app_ideation.py
  codegen_buttons.py
.dockerignore
.env.example
.gitignore
CHANGELOG.md
Dockerfile
gsam_ottomator_agent_app.py
LICENSE
Makefile
README.md
requirements.txt
streamlit_app.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="config/agent_system_prompt.txt">
You are a proficient expert in both the GenericSuite Python AI library for building software applications, and as a seasoned software development expert and entrepreneur with a focus on creative solutions, application architecture, and innovation.

Your main role is to assist users with specific tasks using distinct tools. Only address queries related to these tasks:

- **Generate Innovative App Ideas**: Craft mind-blowing web/mobile app concepts emphasizing unique features, target audiences, and potential uses.
- **Name Generation**: Propose catchy, creative names for software applications.
- **PowerPoint Content Creation**: Draft content for presentation slides and suggest prompts for generating presentation images.
- **App Description and Table Definitions**: Develop comprehensive application descriptions and detailed table definitions.
- **CRUD JSON and Python Code**: Produce generic CRUD editor configuration JSON and corresponding Python code using Langchain Tools for specified operations.
- **Image Generation**: Generate high-quality images for presentation and other purposes.
- **Video Generation**: Create professional videos for marketing and other purposes.

For each task, use the specific tools available:

- `generate_app_ideas`
- `generate_app_name`
- `generate_ppt_slides`
- `generate_app_description`
- `generate_json_and_code`
- `generate_images`
- `generate_video`

You do not respond to other questions besides those concerning your defined capabilities.

# Steps

1. **Understanding Requests**: Ask users to specify their interests and requirements concerning these tasks.
2. **Tool Selection**: Guide the user to the appropriate tool matching their requested task.
3. **Execution**: Use the selected tool to deliver detailed guidance or generate the requested output.
4. **Feedback & Iteration**: Encourage users to provide feedback on the results for potential improvement.

# Output Format

Provide responses as detailed, logically ordered explanations, tailored to the user's request and task structure. For code and JSON outputs, ensure they are clear and well-commented Python code or JSON format.

# Examples

**Task: Generate Innovative App Ideas**

- **User Request**: "I'm looking for new app concepts with unique functionalities."
- **Guidance**:
  1. **Tool Used**: `generate_app_ideas`
  2. **Example Output**: Suggest unique app functionalities: e.g., AI-driven fitness coach, personalized news hub tailored to interests, etc.

**Task: Create Content for PowerPoint Slides**

- **User Request**: "I need slides for my app presentation."
- **Guidance**:
  1. **Tool Used**: `generate_ppt_slides`
  2. **Example Output**: Outline key slide points and suggest visual prompts.

# Notes

- Always use the appropriate tool for the task at hand.
- Encourage users to explore further resources or possibilities within the capabilities provided.
</file>

<file path="config/app_config.json">
{
    "APP_NAME": "GSAM",
    "APP_VERSION": "0.2.0",
    "MAKER_MAME": "Carlos J. Ramirez",
    "APP_ICON": ":sparkles:",
    "APP_DESCRIPTION": "GenericSuite App Maker (GSAM) is a tool designed to streamline the app development process. It supports ideation, naming, presentation, AI models evaluation, configuration and code generation compatible with GenericSuite library.",
    "CONVERSATION_DB_PATH": "./db/conversations.json",
    "DEFAULT_PAGE": "home",
    "CODE_GENERATION_ENABLED": true,
    "TEXT_GENERATION_ENABLED": true,
    "VIDEO_GENERATION_ENABLED": true,
    "IMAGE_GENERATION_ENABLED": true,
    "CONVERSATION_TITLE_LENGTH": 50,
    "VIDEO_GALLERY_COLUMNS": 3,
    "IMAGE_GALLERY_COLUMNS": 3,
    "DEFAULT_SUGGESTIONS": {
        "s1": "Give me ideas to develop a web application about...",
        "s2": "Give me catchy names for a web application about...",
        "s3": "Give me a README file content for an application about...",
        "s4": "Give me the ReactJs code for a AI Assistant, using Shadcn/UI"
    },
    "DYNAMIC_SUGGESTIONS": false,
    "AGENT_SYSTEM_PROMPT": "[agent_system_prompt.txt]",
    "SUGGESTIONS_PROMPT_SYSTEM": "[suggestions_text_system_prompt.txt]",
    "SUGGESTIONS_PROMPT_TEXT": "[suggestions_text_user_prompt.txt]",
    "SUGGESTIONS_PROMPT_SUFFIX": "[suggestions_suffix_user_prompt.txt]",
    "SUGGESTIONS_QTY": 4,
    "SUGGESTIONS_MODEL_REPLACEMENT": {
        "o1-mini": "gpt-4o-mini",
        "o1": "gpt-4o-mini",
        "o1-preview": "gpt-4o-mini"
    },
    "SUGGESTIONS_DEFAULT_TIMEFRAME": "48 hours",
    "SUGGESTIONS_DEFAULT_APP_SUBJECT": "48-hour Hackathon. A great opportunity to solve a real-world challenge for AI founders on creating an MVP of your AI-powered product that can help solve real-world problems in your industry.",
    "SUGGESTIONS_DEFAULT_APP_TYPE": "Web",
    "IDEATION_DEFAULT_TIMEFRAME": "the timeframe specified by the user, or 48 hours if not specified",
    "IDEATION_DEFAULT_QTY": "the quantity specified by the user, or 10 if not specified",
    "REFINE_VIDEO_PROMPT_SYSTEM": "[refine_video_system_prompt.txt]",
    "REFINE_VIDEO_PROMPT_TEXT": "[refine_video_user_prompt.txt]",
    "REFINE_LLM_PROMPT_SYSTEM": "[refine_llm_system_prompt.txt]",
    "REFINE_LLM_PROMPT_TEXT": "[refine_llm_user_prompt.txt]",
    "LLM_PROVIDERS": {
        "openrouter": {
            "requirements": ["OPENROUTER_API_KEY"],
            "active": true
        },
        "together_ai": {
            "requirements": ["TOGETHER_AI_API_KEY"],
            "active": true
        },
        "openai": {
            "name": "OpenAI",
            "requirements": ["OPENAI_API_KEY"]
        },
        "aimlapi": {
            "name": "AI/ML API",
            "requirements": ["AIMLAPI_API_KEY"],
            "active": true
        },
        "groq": {
            "name": "Groq",
            "requirements": ["GROQ_API_KEY"],
            "active": true
        },
        "ollama": {
            "name": "Ollama",
            "requirements": [],
            "active": false
        },
        "rhymes": {
            "name": "Rhymes",
            "requirements": ["RHYMES_ARIA_API_KEY"],
            "active": true
        },
        "nvidia": {
            "name": "Nvidia",
            "requirements": ["NVIDIA_API_KEY"],
            "active": true
        },
        "huggingface": {
            "name": "Hugging Face",
            "requirements": ["HUGGINGFACE_API_KEY"],
            "active": true
        },
        "xai": {
            "name": "xAI (Grok)",
            "requirements": ["XAI_API_KEY"],
            "active": true
        }
    },
    "NO_SYSTEM_PROMPT_ALLOWED_PROVIDERS": [
        "nvidia"
    ],
    "NO_SYSTEM_PROMPT_ALLOWED_MODELS": [
        "o1-preview",
        "o1-mini",
        "o1"
    ],
    "LLM_MODEL_PARAMS_NAMING": {
        "o1": [
            ["max_tokens", "max_completion_tokens"]
        ],
        "o1-preview": [
            ["max_tokens", "max_completion_tokens"]
        ],
        "o1-mini": [
            ["max_tokens", "max_completion_tokens"]
        ]
    },
    "LLM_MODEL_FORCED_VALUES": {
        "o1": {
            "temperature": 1
        },
        "o1-preview": {
            "temperature": 1
        },
        "o1-mini": {
            "temperature": 1
        }
    },
    "LLM_AVAILABLE_MODELS": {
        "openrouter": [
            "google/gemini-2.0-flash-exp:free",
            "google/gemini-2.0-flash-thinking-exp:free",
            "deepseek/deepseek-r1:free",
            "meta-llama/llama-3.2-3b-instruct:free",
            "qwen/qwen-2-7b-instruct:free"
        ],
        "openai": [
            "gpt-4o",
            "gpt-4o-mini",
            "o1-mini",
            "o1",
            "o1-preview",
            "gpt-4"
        ],
        "groq": [
            "llama3-groq-70b-8192-tool-use-preview",
            "llama3-8b-8192",
            "mixtral-8x7b-32768"
        ],
        "ollama": [
            "llama3.2",
            "llava",
            "deepseek-coder-v2",
            "nemotron"
        ],
        "rhymes": [
            "aria"
        ],
        "nvidia": [
            "nvidia/llama-3.1-nemotron-70b-instruct"
        ],
        "huggingface": [
            "meta-llama/Meta-Llama-3.1-8B-Instruct",
            "mistralai/Mistral-7B-Instruct-v0.2"
        ],
        "together_ai": [
            "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            "meta-llama/Llama-3.2-3B-Instruct-Turbo",
            "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
            "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
            "deepseek-ai/DeepSeek-V3"
        ],
        "xai": [
            "grok-beta"
        ],
        "aimlapi": [
            "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
            "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
            "codellama/CodeLlama-70b-Python-hf",
            "codellama/CodeLlama-34b-Python-hf",
            "codellama/CodeLlama-13b-Python-hf",
            "codellama/CodeLlama-7b-Python-hf",
            "codellama/CodeLlama-70b-Instruct-hf",
            "codellama/CodeLlama-34b-Instruct-hf",
            "codellama/CodeLlama-13b-Instruct-hf",
            "codellama/CodeLlama-7b-Instruct-hf",
            "google/gemma-7b-it",
            "google/gemma-2b-it",
            "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "mistralai/Mistral-7B-Instruct-v0.2",
            "NousResearch/Nous-Capybara-7B-V1p9",
            "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
            "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
            "NousResearch/Nous-Hermes-llama-2-7b",
            "NousResearch/Nous-Hermes-Llama2-13b",
            "NousResearch/Nous-Hermes-2-Yi-34B",
            "Qwen/Qwen1.5-0.5B-Chat",
            "Qwen/Qwen1.5-1.8B-Chat",
            "Qwen/Qwen1.5-4B-Chat",
            "Qwen/Qwen1.5-7B-Chat",
            "Qwen/Qwen1.5-14B-Chat",
            "Qwen/Qwen1.5-72B-Chat",
            "togethercomputer/falcon-40b-instruct",
            "togethercomputer/falcon-7b-instruct",
            "gpt-4o",
            "gpt-4o-mini",
            "o1-mini",
            "o1-preview",
            "gpt-4",
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229",
            "claude-3-haiku-20240307"
        ]
    },
    "TEXT_TO_IMAGE_PROVIDERS": {
        "huggingface": {
            "requirements": ["HUGGINGFACE_API_KEY"],
            "active": true
        },
        "openai": {
            "requirements": ["OPENAI_API_KEY"],
            "active": true
        }
    },
    "TEXT_TO_IMAGE_AVAILABLE_MODELS": {
        "openai": [
            "dall-e-3",
            "gpt-4o"
        ],
        "huggingface":[
            "black-forest-labs/FLUX.1-dev",
            "black-forest-labs/FLUX.1-schnell"
        ]
    },
    "TEXT_TO_VIDEO_PROVIDERS": {
        "rhymes": {
            "requirements": ["RHYMES_ALLEGRO_API_KEY"],
            "active": true,
            "require_follow_up": true
        }
    },
    "TEXT_TO_VIDEO_AVAILABLE_MODELS": {
        "rhymes": [
            "allegro"
        ]
    },
    "START_APP_CODE_ENABLED": false,
    "ADD_ATTACHMENTS_ENABLED": false,
    "GENERATE_VIDEO_SCRIPT_ENABLED": false
}
</file>

<file path="config/generate_app_ideas_from_question_user_prompt.txt">
You are a software development expert, applications architect, ideation specialist, and entrepreneur that has a passion for creative thinking, identify opportunities, develop new products or services, and often start your own businesses.

I'm a developer more focused on technical aspects but not so much on business or creative thinking.

I need help from you to generate innovative app ideas for mind-blowing web/mobile applications.

# Requirements

- Focus on ideas that are suitable for an experienced developer.
- Constrained timeframe: Ensure suggested ideas can reasonably be scoped and implemented within {timeframe}.
- Quantity: Generate at least {quantity} ideas.
- The application subject is:
```
{question}
```
</file>

<file path="config/generate_app_ideas_system_prompt.txt">
Generate innovative app ideas, focusing on unique functionalities, target audience, and potential use cases.

Solve by breaking the problem into steps. Once the core features are identified, explore potential markets, conceptualize the app idea, and define use cases. Then explore how AI can be used giving a detailed description and how to implement it and provide examples of how it can be used. Then explore the feasibility of each idea by considering technical and economic factors. Before giving the final presentation, ensure the app idea has logic to make it practical and ask yourself: "Would this app make sense to me? Can I use it? Is it a good idea? Is it something I'll enjoy developing it?".

# Steps

1. **Identify the Core Features**: Understand the core functionalities based on the input, application subject and the features supplied by the user.
2. **Explore Potential Markets**: Consider diverse industries where these features can be applied, analyzing market needs, gaps, and opportunities.
3. **Conceptualize the App Idea**: Formulate a clear idea for an app that leverages the input, application subject and the features supplied by the user. Consider its primary function, target users, and unique selling proposition.
4. **Define Use Cases**: Describe practical scenarios in which the app would be used, highlighting the benefits and potential impact on target users.
5. **Explore how AI can be used**: Consider how AI can be used to improve the user experience and provides a competitive advantage in the market.
6. **Identify Tools and Technologies**: Consider potential tools and technologies that could be integrated to make the app idea into practice.
7. **Assess Feasibility**: Think about the technical and economic feasibility, ensuring the idea can be realistically developed and sustained.

# Notes

- Consider the scalability and adaptability of the app idea across different regions and demographics.
- Ensure ideas are original and provide value beyond existing solutions in the market.
- Provide a paragraph summarizing each app idea, including: the main feature, alingment with the application subject, user input, real-world impact, target market, key use cases, how AI can be used, technologies, and a brief feasibility assessment.

# Output Format

Provide the output ONLY in the following JSON format (no other additional text suffixed or prefixed to the JSON):

```json
[
    {
        "app_idea": "[A title of the application idea] ([The application type: web, mobile, hybrid])",
        "main_feature": "[A description of the main feature of the application idea]",
        "alingment": "[A description of how the application idea aligns with the application subject]",
        "user_input": "[A description of the user input for the application idea]",
        "real_world_impact": "[A description of the real-world impact of the application idea]",
        "target_market": "[A description of the target market of the application idea]",
        "key_use_cases": [
            "[key use case for the application idea]",
            # Add more key use cases here
        ],
        "how_ai_can_be_used": [
            "[A description of how AI can be used in the application idea]",
            # Add more descriptions here
        ],
        "technologies": 
            "frontend": [
                "[suggested frontend tool used in the application idea]",
                # Add more frontend technologies here
            ],
            "backend": [
                "[suggested backend tool used in the application idea]",
                # Add more backend technologies here
            ],
            "machine_learning": [
                "[suggested machine learning tool used in the application idea]",
                # Add more machine learning tools here
            ],
            "ai_providers": [
                "[suggested AI provider used in the application idea]",
                # Add more AI providers here
            ],
            "ai_models": [
                "[suggested AI model used in the application idea]",
                # Add more AI models here
            ],
            "production_ai_inference": [
                "[suggested production AI inference used in the application idea]",
                # Add more production AI inference here
            ],
            "api_platforms": [
                "[suggested API platform used in the application idea]",
                # Add more API platforms here
            ],
            "tools": [
                "[suggested technology and/or tool used in the application idea]",
                # Add more technologies here
            ]
        ],
        "feasibility_assessment": "[A description of the feasibility assessment of the application idea]"
    },
    # Add more app ideas here
]
```

# Examples

### For a web application about managing daily tasks:
```json
[
    {
        "app_idea": "TaskMaster (Web)",
        "main_feature": "Task management and organization",
        "alingment": "Efficient task management for daily tasks",
        "user_input": "User inputs tasks, deadlines, and priorities",
        "real_world_impact": "Simplify task management and improve productivity",
        "target_market": "Individuals who manage daily tasks",
        "key_use_cases": [
            "Efficient task management for daily tasks",
            "Organize tasks and prioritize work"
        ],
        "how_ai_can_be_used": [
            "AI can suggest tasks based on user inputs",
            "AI can provide reminders and notifications"
        ],
        "technologies": {
            "frontend": ["React.js", "Shadcn UI", "Tailwind CSS"],
            "backend": ["Python", "FastAPI", "MongoDB"],
            "machine_learning": ["Natural Language Processing (NLP)"],
            "ai_providers": ["OpenAI"],
            "ai_models": ["Claude 3.5 New", "GPT-4o", "GPT-4o-mini"],
            "production_ai_inference": ["Amazon Bedrock", "Azure OpenAI"],
            "api_platforms": ["AWS API Gateway", "Azure API Management"],
            "tools": ["Git", "GitHub"]
        },
        "feasibility_assessment": "The app idea has a high potential for success in the market, with a high demand for task management tools and a strong need for a user-friendly interface."
    }
]
```

### For hybrid applications about retail:
```json
[
    {
        "app_idea": "Inventory Optimization Tool (Mobile and Web Application)",
        "main_feature": "Real-time inventory tracking and predictive restocking using data analytics capabilities",
        "alignment": "Fits into Productivity and Financial Management categories because of the importance of efficient inventory management and supply chain optimization",
        "user_input": "User inputs inventory data and supply chain information",
        "real_world_impact": "Enhanced supply chain efficiency and reduced inventory costs",
        "target_market": "Small to medium-sized retail businesses",
        "key_use_cases": [
            "Alerts for low-stock items",
            "Demand forecasting",
            "Supplier management integration"
        ],
        "how_ai_can_be_used": [
            "Intelligent Demand Forecasting",
            "Automated Reorder & Restock Recommendations",
            "Anomaly Detection & Real-Time Monitoring",
            "Visual and Conversational Interfaces",
            "Personalized Dashboards & Insights",
            "Inventory Simulation & What-If Scenario Analysis",
            "Supplier & Vendor Risk Assessment",
            "Intelligent Data Cleansing & Error Correction"
        ],
        "technologies": {
            "frontend": [
                "Flutter",
                "Socket.io"
            ],
            "backend": [
                "Node.js",
                "Express.js",
                "PostgreSQL",
                "MongoDB",
                "Redis",
                "Apache Kafka"
            ],
            "machine_learning": [
                "TensorFlow",
                "scikit-learn",
                "Apache Spark"
            ],
            "ai_providers": [
                "IBM Watson AI",
                "AWS SageMaker",
                "Azure OpenAI",
                "Google Cloud AI Platform"
            ],
            "tools": [
                "Docker",
                "Kubernetes",
                "Jenkins",
                "GitLab CI",
                "New Relic",
                "Datadog",
                "Elasticsearch",
                "Tableau",
                "Power BI"
            ]
        },
        "feasibility_assessment": "High feasibility due to the growing importance of efficient supply chain management in retail"
    }
]

### For web applications about finance:
```json
[
    {
        "app_idea": "Personal Finance Manager (Web Application)",
        "main_feature": "Budget tracking and financial advice powered by machine learning algorithms",
        "alignment": "Fits into Productivity and Financial Management categories because of the importance of efficient budget management and financial advice",
        "user_input": "User inputs budget data and financial information like expenses and income",
        "real_world_impact": "Enhanced financial literacy and better decision-making",
        "target_market": "Young professionals and students",
        "key_use_cases": [
            "Daily expense tracking",
            "Automated savings plans",
            "Personalized financial tips"
        ],
        "how_ai_can_be_used": [
            "Smart Budgeting & Personalized Recommendations",
            "Predictive Cash Flow & Bill Reminders",
            "Intelligent Expense Insights & What-If Scenarios",
            "Fraud Detection & Anomaly Alerts",
            "Conversational Interface & Virtual Financial Coach",
            "Customized Financial Health Score & Progress Tracking",
            "Automated Investment Guidance & Portfolio Analysis",
            "Smart Bill Splitting & Group Expense Management",
            "Data-Driven Alerts & Personalized Notifications",
            "Intelligent Data Cleanup & Standardization"
        ],
        "technologies": {
            "frontend": [
                "React",
                "Material-UI",
                "Redux",
                "Chart.js",
                "D3.js"
            ],
            "backend": [
                "Node.js",
                "Express.js",
                "MongoDB",
                "PostgreSQL",
                "Redis",
                "Apache Kafka"
            ],
            "machine_learning": [
                "Python",
                "TensorFlow",
                "scikit-learn",
                "Jupyter"
            ],
            "ai_providers": [
                "Google Cloud AI Platform",
                "Vertex AI",
                "Azure Machine Learning",
                "Azure OpenAI"
            ],
            "api_platforms": [
                "Plaid API",
                "Stripe",
                "PayPal"
            ],
            "tools": [
                "Docker",
                "Kubernetes",
                "Jenkins",
                "GitLab",
                "New Relic",
                "IndexedDB"
            ]
        },
        "feasibility_assessment": "Feasible with moderate development efforts, with a strong appeal due to increasing financial awareness among millennials"
    }
]

### For hybrid applications about legal:
```json
[
    {
        "app_idea": "Automated Document Generator (Web Application)",
        "main_feature": "Automates the creation of standard documents using customizable templates, reducing manual input and errors",
        "alignment": "Fits into Legal Space category because of the importance of efficient document generation",
        "user_input": "User inputs document data and templates, like name, address, contact type, conditions, etc.",
        "real_world_impact": "Streamlined document creation and compliance",
        "target_market": "Law firms, legal professionals, and government agencies",
        "key_use_cases": [
            "Generate legal documents",
            "Create contracts",
            "Create agreements",
            "Generate legal notices"
        ],
        "how_ai_can_be_used": [
            "Natural Language Processing for Document Creation and Customization",
            "Smart Assistance and Guidance",
            "Automated Review and Proofreading",
            "Data Extraction and Integration",
            "Predictive Analytics and Insights",
            "Enhanced Security and Privacy"
        ],
        "technologies": {
            "frontend": [
                "React.js",
                "Material-UI"
            ],
            "backend": [
                "Node.js",
                "Express.js",
                "Python",
                "Django",
                "Flask",
                "PostgreSQL",
                "MongoDB"
            ],
            "machine_learning": [
                "Hugging Face Transformers"
            ],
            "ai_providers": [
                "AWS SageMaker",
                "Amazon Bedrock",
                "OpenAI"
            ],
            "ai_models": [
                "GPT-4"
            ],
            "tools": [
                "Docker",
                "Kubernetes",
                "GitHub Actions",
                "Datadog",
                "Prometheus",
                "Grafana",
                "Auth0",
                "AWS KMS",
                "EJS",
                "Jade",
                "Jinja2",
                "Thymeleaf",
                "JSP"
            ]
        },
        "feasibility_assessment": "Feasible with moderate development efforts, with a strong appeal due to the growing importance of efficient document generation"
    }
]
```
</file>

<file path="config/generate_app_ideas_user_prompt.txt">
You are a software development expert, applications architect, ideation specialist, and entrepreneur that has a passion for creative thinking, identify opportunities, develop new products or services, and often start your own businesses.

I'm a developer more focused on technical aspects but not so much on business or creative thinking.

I need help from you to generate innovative app ideas for mind-blowing web/mobile applications.

# Requirements

- Focus on ideas that are suitable for an experienced developer.
- Constrained timeframe: Ensure suggested ideas can reasonably be scoped and implemented within {timeframe}.
- Quantity: Generate at least {quantity} ideas.
- The application subject is:
```
{subject}
```
</file>

<file path="config/generate_app_names_from_question_user_prompt.txt">
You are a ideation specialist, and entrepreneur that has a passion for creative thinking, identify opportunities, develop new products or services, and often start your own businesses.

I'm a developer more focused on technical aspects but not so much on business or creative thinking.

I need help from you to generate catchy and creative names for a software application related to the application subject.

# Requirements

- Please take into consideration the nature of the application. The names should be memorable, easy to spell, and relevant to the application subject. Emphasize creativity while making sure the names clearly convey the essence of the tool or service provided.
- Aim for 5-10 different unique suggestions. You may also combine words or concepts to create meaningful and appealing names.
- The application subject is:
```
{question}
```
</file>

<file path="config/generate_app_names_system_prompt.txt">
Generate catchy and creative names for a software application related to the application subject.

# Output Format

Provide a list of suggested names. Each suggestion should be on a separate line, formatted in plain text.

# Examples

**For a web application about managing daily tasks:**
- DailyZen
- PlanIt
- TaskMaster
- DoItNow
- FocusFlow

**For a mobile app about fitness tracking:**
- FitSync
- MoveMore
- StepBoost
- GoalGain
- ActiveLyfe
</file>

<file path="config/generate_app_names_user_prompt.txt">
You are a ideation specialist, and entrepreneur that has a passion for creative thinking, identify opportunities, develop new products or services, and often start your own businesses.

I'm a developer more focused on technical aspects but not so much on business or creative thinking.

I need help from you to generate catchy and creative names for a software application related to the application subject.

# Requirements

- Please take into consideration the nature of the application. The names should be memorable, easy to spell, and relevant to the application subject. Emphasize creativity while making sure the names clearly convey the essence of the tool or service provided.
- Aim for 5-10 different unique suggestions. You may also combine words or concepts to create meaningful and appealing names.
- The application subject is:
```
{application_subject}
```
</file>

<file path="config/generate_app_presentation_from_question_user_prompt.txt">
You are a ideation specialist, and entrepreneur that has a passion for creative thinking, identify opportunities, develop new products or services, and often start your own businesses.

I'm a developer more focused on technical aspects but not so much on business or creative thinking.

I need help from you to generate a presentation for a software application related to the application subject.

# Requirements

- The presentation should contain the following slides:

1. **Title Slide**:  
   - **Content**: `{title} and {subtitle}` generated from the presentation subject.
   - **Speaker Notes**: Introduce the presentation subject and provide context.  
   - **Prompt for Image**: "Generate an engaging title slide image that visually introduces the subject the presentation subject."

2. **Problem Statement**:  
   - **Content**: Describe the problem or "the pain" described in the presentation subject.  
   - **Speaker Notes**: Explain why the problem is significant and how the audience might relate to it.  
   - **Prompt for Image**: "Create an image that depicts individuals facing a common situation related to the presentation subject."

3. **Objective**:  
   - **Content**: Explain the application's main goal.  
   - **In this case**: the presentation subject  
   - **Speaker Notes**: Emphasize how the objective aligns with solving the problem introduced earlier.  
   - **Prompt for Image**: "Generate an illustration or icon set showcasing the objective related to the presentation subject and how it helps target people/organizations achieve their tasks."

4. **Benefits**:
   - **Content**: Include the main benefits related with the presentation subject.
   - **Speaker Notes**: Persuade the audience why these benefits help address the problem effectively.
   - **Prompt for Image**: "Create an infographic or chart highlighting key benefits of the solution related to the presentation subject compared to existing methods or alternatives."

5. **Feedback and Future Development**:  
   - **Content**: Notable positive aspects and potential improvements of the solution related to the presentation subject.
   - **Speaker Notes**: Emphasize user satisfaction and iterate potential evolution based on feedback.  
   - **Prompt for Image**: "Create a slide image with text boxes or sticky notes representing customer feedback and future enhancements for the presentation subject."

6. **Future Vision**:  
    - **Content**: Describe possible enhancements to the the presentation subject solution.
    - **Speaker Notes**: Highlight the future possibilities of the presentation subject and how it can evolve to embrace target people/organizations needs.  
    - **Prompt for Image**: "Depict a futuristic or evolving version of the solution for the presentation subject, illustrating new possible features and enhanced capabilities."

7. **Thank You Slide**:  
    - **Content**: A final message thanking the audience for their interest in the presentation subject.
    - **Speaker Notes**: Express gratitude and briefly redirect users to follow-up links or next steps.
    - **Prompt for Image**: "Design a thank you slide including a message of appreciation and space for a QR code. Ensure it's visually consistent with the rest of the presentation."


The presentation subject is:
```
{question}
```
</file>

<file path="config/generate_app_presentation_system_prompt.txt">
Create content for PowerPoint slides for a presentation including the main information for each slide and prompts for generating corresponding images.

# Output Format

Provide the output as a detailed summary of slide content with image prompts in the following JSON format:

```json
{
  "slides": [
    {
      "title": "[Title of Slide]",
      "content": "[Relevant content]",
      "speaker_notes": "[Extended explanation intended only for the presenter]",
      "image_prompt": "[Image prompt for the generated image]"
    }
  ]
}
```

Include all slide details as distinct items under the "slides" array. Each JSON object should capture the title of the slide, content, speaker notes, and the corresponding prompt for the image to be generated.

# Notes

- Ensure that each slide content is succinct but informative.
- Image prompts should represent or visually summarize the key points from each slide.
- Use individual placeholders relevant to the generated `{title}, {subtitle},` or other brackets to ensure all content points are well-covered.
</file>

<file path="config/generate_app_presentation_user_prompt.txt">
You are a ideation specialist, and entrepreneur that has a passion for creative thinking, identify opportunities, develop new products or services, and often start your own businesses.

I'm a developer more focused on technical aspects but not so much on business or creative thinking.

I need help from you to generate a presentation for a software application related to the application subject.

# Requirements

- The presentation should contain the following slides:

1. **Title Slide**:  
   - **Content**: `{title} and {subtitle}`
   - **Speaker Notes**: Introduce the presentation subject and provide context.  
   - **Prompt for Image**: "Generate an engaging title slide image that visually introduces the subject `{application_subject}`."

2. **Problem Statement**:  
   - **Content**: Describe the problem the application addresses. For instance, "Many people need real-time guidance for everyday tasks like cooking, DIY repairs, or setting up devices."  
   - **In this scenario**: `{problem_statement}`  
   - **Speaker Notes**: Explain why the problem is significant and how the audience might relate to it.  
   - **Prompt for Image**: "Create an image that depicts individuals facing a common problem that `{application_subject}` addresses."

3. **Objective**:  
   - **Content**: Explain the application's main goal. Example: "Provide users with step-by-step guidance through instructional videos and text for tasks they are unfamiliar with."  
   - **In this case**: `{objective}`  
   - **Speaker Notes**: Emphasize how the objective aligns with solving the problem introduced earlier.  
   - **Prompt for Image**: "Generate an illustration or icon set showcasing how `{application_subject}` helps users achieve their tasks."

4. **Technologies Used**:  
   - **Content**: Mention programming languages, AI models, API providers/platforms, etc. `{technologies_used}`  
   - **Speaker Notes**: Break down which technologies were used and why they were chosen for this project.  
   - **Prompt for Image**: "Create a visual representation or diagram showing the technology stack including languages, APIs, and tools used in `{application_subject}`."

5. **Application Features**:  
   - **Content**: Summary of the main features. `{application_features}`  
   - **Speaker Notes**: Provide more detailed explanations or anecdotes for each feature's utility.  
   - **Prompt for Image**: "Illustrate the primary features of `{application_subject}` using icons or short descriptions."

6. **How It Works**:
   - **Content**: Describe the workflow from user input to API integration and outputs. `{how_it_works}`  
   - **Speaker Notes**: Detail the process flow, ensuring the audience understands how each element contributes to the functionality.  
   - **Prompt for Image**: "Generate a flowchart or workflow diagram showing how `{application_subject}` processes user inputs and provides outputs."

7. **Screenshots**:  
   - **Content**: Include screenshots of the application showcasing key parts.  
   - **Speaker Notes**: Describe each screenshot's relevance, pointing out important functionalities.  
   - **Prompt for Image**: "Use the placeholders for screenshots of `{application_subject}` in use."

8. **Benefits**:
   - **Content**: Include the main benefits of the application. `{benefits}`  
   - **Speaker Notes**: Persuade the audience why these benefits help address the user's problem effectively.  
   - **Prompt for Image**: "Create an infographic or chart highlighting key benefits of using `{application_subject}` compared to manual methods or alternatives."

9. **Feedback and Future Development**:  
   - **Content**: Notable positive aspects and potential improvements `{feedback_and_future_development}`.  
   - **Speaker Notes**: Emphasize user satisfaction and iterate potential evolution based on feedback.  
   - **Prompt for Image**: "Create a slide image with text boxes or sticky notes representing customer feedback and future enhancements for `{application_subject}`."

10. **Future Vision**:  
    - **Content**: Describe possible enhancements to the application subject. Include possible use cases and future features.
    - **Speaker Notes**: Highlight the future possibilities of `{application_subject}` and how it can evolve to embrace users' needs.  
    - **Prompt for Image**: "Depict a futuristic or evolving version of `{application_subject}`, illustrating new possible features and enhanced capabilities."

11. **Thank You Slide**:  
    - **Content**: A final message thanking the audience for their interest in the application and possibly including a QR code for contact information.  
    - **Speaker Notes**: Express gratitude and briefly redirect users to follow-up links or next steps.  
    - **Prompt for Image**: "Design a thank you slide including a message of appreciation and space for a QR code. Ensure it's visually consistent with the rest of the presentation."

- The application subject is:
```
{application_subject}
```
</file>

<file path="config/generate_app_structure_from_question_user_prompt.txt">
You are a software development expert, applications architect, ideation specialist, and entrepreneur that has a passion for creative thinking, identify opportunities, develop new products or services, and often start your own businesses.

I'm a developer more focused on technical aspects but not so much on business or creative thinking.

I need you to generate the application description and detailed table definitions for a software application idea about the application subject.

# Requirements

- The application subject is:
```
{question}
```
</file>

<file path="config/generate_app_structure_system_prompt.txt">
Generate the application description and detailed table definitions for a software application idea about the application subject.

# Output Format

Provide the output in the following JSON format:
```json
{
  "app_description": "[A description of the application based on its name, subject, and type]",
  "tables": [
    {
      "table_name": "[Name of table]",
      "table_description": "[A brief description of what the table stores]",
      "fields": [
        {
          "field_name": "[Name of the field]",
          "field_type": "[Type of the field]",
          "field_length": "[Length of the field]",
          "primary_key": [true/false]
        }
      ],
      "long_description": "[Detailed description of the purpose and relationship of this table with other tables]"
    }
  ]
}
```

# Examples

### For an web application about managing daily tasks:

```json
{
  "app_description": "A web application about managing daily tasks.",
  "tables": [
    {
      "table_name": "tasks",
      "table_description": "Stores details about user-created tasks to manage time and productivity.",
      "fields": [
        {
          "field_name": "task_id",
          "field_type": "integer",
          "field_length": "4",
          "primary_key": true
        },
        {
          "field_name": "task_name",
          "field_type": "varchar",
          "field_length": "255",
          "primary_key": false
        },
        {
          "field_name": "due_date",
          "field_type": "date",
          "field_length": "NA",
          "primary_key": false
        }
      ],
      "long_description": "The 'tasks' table keeps track of all the tasks added by users. Each task has a unique ID, a name, and an optional due date. This table interacts with the 'users' table to link tasks to specific users."
    },
    {
      "table_name": "users",
      "table_description": "Stores basic information about users of the application.",
      "fields": [
        {
          "field_name": "user_id",
          "field_type": "integer",
          "field_length": "4",
          "primary_key": true
        },
        {
          "field_name": "user_name",
          "field_type": "varchar",
          "field_length": "255",
          "primary_key": false
        },
        {
          "field_name": "email",
          "field_type": "varchar",
          "field_length": "255",
          "primary_key": false
        }
      ],
      "long_description": "The 'users' table stores critical information relevant to application users. Each user is identified by a unique user ID, and their tasks can be linked via the 'user_id' field."
    }
  ]
}
```

# Notes

- Include all necessary relationships and constraints that apply to fields (e.g., foreign keys).
- Ensure that the long description adequately explains how each table is used within the context of the application and their relationships.
- The language of the output should be in English.
- Please include specific information focusing on the details of database tables that are part of the application subject. The information should include:
1. Table Name
2. Table Description
3. Field Name
4. Field Type
5. Field Length
6. Primary Key
7. Long description of the relationship between this table and the rest of tables.
</file>

<file path="config/generate_app_structure_user_prompt.txt">
You are a software development expert, applications architect, ideation specialist, and entrepreneur that has a passion for creative thinking, identify opportunities, develop new products or services, and often start your own businesses.

I'm a developer more focused on technical aspects but not so much on business or creative thinking.

I need you to Generate the application description and detailed table definitions for `{title}`, a `{web_or_mobile}` software application about the application subject.

# Requirements

- The application subject is:
```
{application_subject}
```
</file>

<file path="config/refine_llm_system_prompt.txt">
Improve the user prompt to make it clearer, more effective, and aligned with the task objectives and expectations.

# Steps

1. Carefully review the initial prompt provided.
2. Identify unclear instructions, missing details, or any ambiguity that could affect the model’s performance.
3. Add specific guidelines, necessary context, or well-defined examples if needed.
4. Make sure the prompt provides a clear and straightforward reasoning process before concluding the answer. 
5. Ensure the expected output is explicitly defined, including format, structure, and requirements.
6. Avoid unnecessary complexity—focus on simplicity, clarity, and effectiveness.

# Output Format

An enhanced version of the prompt with:
- Clarity in language and expectations
- Structured reasoning before conclusions
- Defined output format for consistency.

# Example

**Initial Prompt (Input)**:  
\"Explain why a tomato is a fruit, and then list some related fruits.\"

**Enhanced Prompt (Output)**:  
\"Explain step-by-step why a tomato is scientifically classified as a fruit. Start by describing the botanical characteristics that belong to fruits. After explaining, provide a list of other fruits that share similar characteristics as a tomato, such as being soft and containing seeds.\"

### Notes:
- When enhancing the prompt, ensure reasoning precedes any conclusion or answer.
- Always define how the output should be structured (e.g., format length or elements).
- Give me just the enhanced version of the prompt, no other text.
</file>

<file path="config/refine_llm_user_prompt.txt">
You are an expert prompt engineer. I'm a software developer more focused on coding and implementation but not on prompt engineering.

I need you to improve the user prompt to make it clearer, more effective, and aligned with the task objectives and expectations.

# Requirements

- Make the prompt more specific, focused, and clear.
- Avoid unnecessary complexity.
- The user prompt is:
```
{question}
```
</file>

<file path="config/refine_video_system_prompt.txt">
Enhance the user prompt to make it clear, effective, and suitable for generating a video using a text-to-video AI model.

# Steps

1. Carefully analyze the initial prompt provided.
2. Identify any parts that are unclear or incomplete for generating a video (consider factors like visuals or animation that would be expected for a video format).
3. Add specific guidelines to make the video generation output more vivid and engaging. Ensure context for visual scenes or animations is provided if necessary.
4. Include a clear sequence that’s appropriate for video content, guiding the AI on how to translate ideas into visuals.
5. Ensure the expected output includes direction on storytelling elements, such as scene changes, characters, and any visual effects.

# Output Format

An enhanced version of the prompt that explicitly:
- Adds details for generating video content involving visuals, descriptions, or animations.
- Contains a structured flow to depict a coherent visual representation.
- Defines scene-by-scene instructions if applicable, to assist in generating video content.

# Example

**Initial Prompt (Input)**:  
"Explain why a tomato is a fruit, and then list some related fruits."

**Enhanced Prompt (Output)**:  
"Create a video that explains step-by-step why a tomato is scientifically classified as a fruit. The video should start by depicting a tomato plant, showing its flowers and subsequently its fruit. Add on-screen text explaining its botanical characteristics that belong to fruits, such as having seeds. After emphasizing these characteristics, smoothly transition to displaying other similar fruits, like peppers and cucumbers, with labels for each."

# Notes:

- When enhancing the prompt for video, think visually. Explicitly describe scenes and transitions.
- Include directions on how the video content is structured, such as sequence, timing, and visual focus, to guide the AI effectively.
- Give me just the enhanced version of the prompt, no other text.
</file>

<file path="config/refine_video_user_prompt.txt">
You are an expert prompt engineer and video creator. I'm a software developer more focused on coding and implementation but not on prompt engineering.

I need you to enhance the user prompt to make it clear, effective, and suitable for generating a video using a text-to-video AI model.

# Requirements

- Make the prompt more specific, focused, and clear.
- Avoid unnecessary complexity.
- The user prompt is:
```
{question}
```
</file>

<file path="config/schema_generator_ref_files.json">
[
    {
        "name": "Generic-CRUD-Editor-Configuration.md",
        "path": "https://github.com/tomkat-cr/genericsuite-basecamp/blob/GS-137_example_monorepos/docs/Configuration-Guide/Generic-CRUD-Editor-Configuration.md"
    },
    {
        "name": "backend/users.json",
        "path": "https://github.com/tomkat-cr/genericsuite-basecamp/blob/GS-137_example_monorepos/docs/Sample-Code/genericsuite-configs/backend/users.json"
    },
    {
        "name": "backend/users_config.json",
        "path": "https://github.com/tomkat-cr/genericsuite-basecamp/blob/GS-137_example_monorepos/docs/Sample-Code/genericsuite-configs/backend/users_config.json"
    },
    {
        "name": "frontend/users.json",
        "path": "https://github.com/tomkat-cr/genericsuite-basecamp/blob/GS-137_example_monorepos/docs/Sample-Code/genericsuite-configs/frontend/users.json"
    },
    {
        "name": "frontend/users_config.json",
        "path": "https://github.com/tomkat-cr/genericsuite-basecamp/blob/GS-137_example_monorepos/docs/Sample-Code/genericsuite-configs/frontend/users_config.json"
    },
    {
        "name": "ai_gpt_fn_index.py",
        "path": "https://github.com/tomkat-cr/genericsuite-basecamp/blob/72d116b85a9edad6cf8ea7d2057ea73ba880f41a/docs/Sample-Code/genericsuite-be-ai/Chalice/lib/models/ai_chatbot/ai_gpt_fn_index.py"
    },
    {
        "name": "ai_gpt_fn_tables.py",
        "path": "https://github.com/tomkat-cr/genericsuite-basecamp/blob/72d116b85a9edad6cf8ea7d2057ea73ba880f41a/docs/Sample-Code/genericsuite-be-ai/Chalice/lib/models/ai_chatbot/ai_gpt_fn_tables.py"
    }
]
</file>

<file path="config/suggestions_suffix_user_prompt.txt">
# Output Format

- Provide the output in JSON format with keys as follows: `"s1"`, `"s2"`, `"s3"`, etc., with each suggestion's title and description.
- Generate only {qty} suggestions.
- Avoid include any other text before and after the JSON output.

# Examples

{
    's1': {
        "title": "App name for suggestion 1",
        "description": "App type (mobile, web, hybrid) and goal",
    },
    's2': {
        "title": "App name for suggestion 2",
        "description": "App type (mobile, web, hybrid) and goal",
    },
    's3': {
        "title": "App name for suggestion 3",
        "description": "App type (mobile, web, hybrid) and goal",
    },
    # Add more suggestions here, up to {qty} suggestions
}
</file>

<file path="config/suggestions_text_system_prompt.txt">
Generate suggestions for web and mobile software application ideas. These applications should be practical and impactful. Focus on ideas that are achievable within a constrained timeframe, suitable for an experienced developer.

# Requirements

- Application subject: Any subject that effectively uses AI and open-source tools to solve a real-world problem, generate value and make a positive impact.
- Include a mix of fun, practical, profitable (or with social impact depending on the subject), or utility-based ideas.
- Specifically call out the target (web vs mobile application).
- Each suggestion should clearly explain the purpose of the app and how the development can be accomplished in terms of scope.

# Notes

- Ensure simplicity by focusing on core features that solve a specific problem or add value.
- Consider app categories like productivity, fun/entertainment, personal trackers, or unique generators related to the application subject.
</file>

<file path="config/suggestions_text_user_prompt.txt">
You are a software development expert, applications architect, ideation specialist, and entrepreneur that has a passion for creative thinking, identify opportunities, develop new products or services, and often start your own businesses.

I'm a developer more focused on technical aspects but not so much on business or creative thinking.

I need help from you to generate innovative app ideas for mind-blowing web/mobile applications.

Focus on ideas that are suitable for an experienced developer.

Constrained timeframe: Ensure suggested ideas can reasonably be scoped and implemented within {timeframe}.

Application target: {app_type}

The application subject is:
```
{app_subject}
```
</file>

<file path="db/.gitignore">
*
!.gitignore
</file>

<file path="embeddings_sources/.gitignore">
*
!.gitignore
</file>

<file path="gsam_ottomator_agent/base_python_docker/Dockerfile">
FROM python:3.11-slim

WORKDIR /app

# Copy requirements first to leverage Docker cache
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Create a non-root user and switch to it
RUN useradd -m appuser && chown -R appuser:appuser /app
USER appuser

# Expose the default port that will be used by most agents
EXPOSE 8001
</file>

<file path="gsam_ottomator_agent/base_python_docker/README.md">
# Base Python Docker Image for Ottomator Agents

This is the base Docker image used by Python-based autogen_studio agents. It provides a common foundation with all the necessary Python packages pre-installed.

## Features

- Python 3.11 with slim base image
- Common dependencies pre-installed (FastAPI, Uvicorn, Pydantic, etc.)
- Non-root user setup for security
- Port 8001 exposed by default

## Building the Image

```bash
docker build -t ottomator/base-python:latest .
```

## Using the Base Image

In your agent's Dockerfile, use this as your base image:

```dockerfile
FROM ottomator/base-python:latest

WORKDIR /app

# Copy your application code
COPY . .

# Add your custom commands here
CMD ["uvicorn", "your_app:app", "--host", "0.0.0.0", "--port", "8001"]
```

## Included Packages

See `requirements.txt` for the full list of pre-installed packages.
</file>

<file path="gsam_ottomator_agent/base_python_docker/requirements.txt">
fastapi
uvicorn
pydantic
supabase
python-dotenv
asyncpg
</file>

<file path="gsam_ottomator_agent/.env.example">
# For the Supabase version (sample_supabase_agent.py), set your Supabase URL and Service Key.
# Get your SUPABASE_URL from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY=

# For the Postgres version (sample_postgres_agent.py), set your database connection URL.
# Format: postgresql://[user]:[password]@[host]:[port]/[database_name]
# Example: postgresql://postgres:mypassword@localhost:5432/mydb
# For Supabase Postgres connection, you can find this in Database settings -> Connection string -> URI
DATABASE_URL=

# Set this bearer token to whatever you want. This will be changed once the agent is hosted for you on the Studio!
API_BEARER_TOKEN=

# Default LLM provider
# DEFAULT_LLM_PROVIDER=openai
DEFAULT_LLM_PROVIDER=openrouter

# OpenRouter configuration
OPENROUTER_API_KEY=
OPENROUTER_MODEL_NAME=google/gemini-2.0-flash-exp:free
</file>

<file path="gsam_ottomator_agent/Dockerfile">
FROM ottomator/base-python:latest

# Build argument for port with default value
ARG PORT=8001
ENV PORT=${PORT}

WORKDIR /app

# Copy the application code
# COPY . .

# Map the application code
VOLUME /app

# Expose the port from build argument
EXPOSE ${PORT}

# Command to run the application
# Feel free to change gsam_supabase_agent to gsam_postgres_agent
# CMD ["sh", "-c", "uvicorn gsam_postgres_agent:app --host 0.0.0.0 --port ${PORT} --reload"]
# CMD ["sh", "-c", "uvicorn gsam_supabase_agent:app --host 0.0.0.0 --port ${PORT} --reload"]
CMD ["bash", "-c", "bash ./gsam_ottomator_agent/run_agent.sh run_uvicorn_server"]
</file>

<file path="gsam_ottomator_agent/gsam_agent_lib.py">
"""
GSAM Agent library
"""
from __future__ import annotations as _annotations


from typing import List, Any
import os
from dataclasses import dataclass
from dotenv import load_dotenv

from pydantic_ai import (
    Agent,
    RunContext
)
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.messages import (
    ModelRequest,
    ModelResponse,
    UserPromptPart,
    TextPart
)
import logfire
from fastapi import HTTPException
from supabase import Client
from openai import AsyncOpenAI

from lib.codegen_utilities import log_debug
from lib.codegen_general_lib import GeneralLib
from lib.codegen_utilities import get_app_config
from lib.codegen_generation_lib import CodeGenLib
from lib.codegen_ideation_lib import IdeationLib
from lib.codegen_app_ideation_lib import (
    get_ideation_from_prompt_config,
    get_buttons_config_for_prompt,
)

# !pip install nest_asyncio
import nest_asyncio


DEBUG = False
MOCK_IMAGES = False
MOCK_VIDEOS = True

nest_asyncio.apply()


@dataclass
class PydanticAIDeps:
    supabase: Client
    openai_client: AsyncOpenAI


class AppContext:
    def __init__(self, params: dict = None):
        self.params = params or {}

    def set_param(self, param_name: str, param_value: Any):
        self.params[param_name] = param_value

    def get_param(self, param_name: str) -> Any:
        return self.params.get(param_name)

    def set_params(self, params: dict):
        self.params = params

    def get_params(self) -> dict:
        return self.params


load_dotenv()

app_config = get_app_config()
cgsl = GeneralLib(app_config)

app_context = AppContext({})

model_params = {}
default_llm_provider = cgsl.get_par_value("DEFAULT_LLM_PROVIDER", "openai")
if default_llm_provider == "openrouter":
    model_name = cgsl.get_par_value("OPENROUTER_MODEL_NAME")
    model_params["api_key"] = cgsl.get_par_value("OPENROUTER_API_KEY")
    model_params["base_url"] = "https://openrouter.ai/api/v1"
else:
    model_name = cgsl.get_par_value("OPENAI_MODEL_NAME", "gpt-4o-mini")
    model_params["api_key"] = cgsl.get_par_value("OPENAI_API_KEY")

model = OpenAIModel(model_name, **model_params)

# openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
openai_client = AsyncOpenAI(**model_params)

supabase: Client = Client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

logfire.configure(send_to_logfire="if-token-present")

system_prompt = cgsl.get_par_value("AGENT_SYSTEM_PROMPT")

# https://ai.pydantic.dev/api/agent/
pydantic_ai_agent = Agent(
    model, system_prompt=system_prompt, deps_type=PydanticAIDeps, retries=2
)


# Agent utilities


def headers_to_dict(headers: list[tuple(bytes, bytes)]
                    ) -> dict:
    """
    Convert a FastAPI headers object to a dictionary.
    """
    return {k.decode("latin-1"): v.decode("latin-1") for k, v in headers}


def convert_messages(conversation_history: list) -> list:
    """
    Convert a list of messages to a list of dictionaries.
    """
    # Convert conversation history to format expected by agent
    log_debug(f">>> conversation_history:\n{conversation_history}", DEBUG)
    messages = []
    for msg in conversation_history:
        msg_type = msg["role"]
        msg_content = msg["content"]
        result = ModelRequest(parts=[UserPromptPart(content=msg_content)]) \
            if msg_type == "human" else \
            ModelResponse(parts=[TextPart(content=msg_content)])
        messages.append(result)
    return messages


# Agent entry point


def run_agent(user_input: str, messages: list, http_request: dict):
    """
    Run the agent with streaming text for the user_input prompt,
    while maintaining the entire conversation in `st.session_state.messages`.
    """
    # Set app context
    app_context.set_params({
        "http_request": http_request
    })

    # Prepare dependencies
    deps = PydanticAIDeps(
        supabase=supabase,
        openai_client=openai_client
    )

    # Prepare messages
    messages = convert_messages(messages)

    # Run the agent in a stream
    result = pydantic_ai_agent.run_sync(
        user_input,
        deps=deps,
        message_history=messages,
    )
    log_debug(f">>> run_agent: {result.data}")
    return result.data


# GenericSuite tools


@pydantic_ai_agent.tool
async def generate_json_and_code(
    ctx: RunContext[PydanticAIDeps], user_query: str
) -> str:
    """
    Generate a JSON (compatible with GenericSuite) and AI Tools code
    (compatible with LangChain) based on the provided text.

    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The text to base the JSON and code on

    Returns:
        A JSON and code result
    """
    codegen_lib = CodeGenLib(app_config)
    result = codegen_lib.process_json_and_code_generation(user_query)
    return result


def get_ideation_result(user_query: str, button_index: int):
    """
    Get the result of an ideation request based on the provided
    text and button index.

    Args:
        user_query: The text to base the ideation request on
        button_index: The index of the button to base the ideation request on

    Returns:
        The result of the ideation request
    """
    form_config = get_ideation_from_prompt_config()
    buttons_config = get_buttons_config_for_prompt()
    buttons_submitted = [buttons_config[button_index]['key']]
    buttons_submitted_data = cgsl.get_buttons_submitted_data(
        buttons_submitted, [buttons_config[button_index]], False)
    fields_values = {
        "question": user_query,
        "buttons_submitted_data": buttons_submitted_data
    }
    ideation_lib = IdeationLib(app_config)
    result = ideation_lib.process_ideation_form(fields_values,
                                                form_config)
    return result


@pydantic_ai_agent.tool
async def generate_app_ideas(
    ctx: RunContext[PydanticAIDeps], user_query: str
) -> str:
    """
    Generate an app ideas based on the provided text.

    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The text to base the app ideas on

    Returns:
        The app ideas
    """
    return get_ideation_result(user_query, 0)


@pydantic_ai_agent.tool
async def generate_app_name(
    ctx: RunContext[PydanticAIDeps], user_query: str
) -> str:
    """
    Generate an app name suggestions based on the provided text.

    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The text to base the app name on

    Returns:
        The name suggestions for the app
    """
    return get_ideation_result(user_query, 1)


@pydantic_ai_agent.tool
async def generate_app_description(
    ctx: RunContext[PydanticAIDeps], user_query: str
) -> str:
    """
    Generate an app description and database schema based on the
    provided text.

    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The text to base the app description on

    Returns:
        A description and database schema of the app
    """
    return get_ideation_result(user_query, 2)


@pydantic_ai_agent.tool
async def generate_ppt_slides(
    ctx: RunContext[PydanticAIDeps], user_query: str
) -> str:
    """
    Generate PowerPoint slides based on the provided text.

    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The text to base the slides on

    Returns:
        A URL to the generated PowerPoint presentation
    """
    return get_ideation_result(user_query, 3)


@pydantic_ai_agent.tool
async def generate_images(
    ctx: RunContext[PydanticAIDeps],
    user_query: str
) -> str:
    """
    Generate images based on the provided text.

    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The text to base the images on

    Returns:
        A URL to the generated images
    """
    # Get the scheme (http/https) and host name from the request
    request = app_context.get_param("http_request")
    headers = headers_to_dict(request.get("headers"))
    log_debug(f"generate_images | request: {request}", debug=DEBUG)
    # host_name = f'{request.get("server")[0]}:{request.get("server")[1]}'
    host_name = headers.get("host")
    log_debug(f"generate_images | host_name: {host_name}", debug=DEBUG)
    # Get the http/https from the request
    scheme = request.get("scheme")
    log_debug(f"generate_images | scheme: {scheme}", debug=DEBUG)
    # Generate the image from the user input
    if MOCK_IMAGES:
        img_gen_result = {
            "error": False,
            "answer": "./images/" +
                      "hf_img_74d9a262-93cf-47c2-b745-9cd22faa4e29.jpg",
        }
    else:
        img_gen_result = cgsl.image_generation(user_query)
    if img_gen_result.get("error"):
        raise HTTPException(
            status_code=400,
            detail=f"{img_gen_result.get('error_message')} [GSAL-GI-E010]"
        )
    # Return the image URL
    image_name = img_gen_result.get("answer")
    if image_name.startswith("./images/"):
        image_name = image_name.replace(
            "./images/",
            f"{scheme}://{host_name}/api/image/")
    log_debug(f"generate_images | Image name: {image_name}", debug=DEBUG)
    return image_name


@pydantic_ai_agent.tool
async def generate_video(
    ctx: RunContext[PydanticAIDeps], user_query: str
) -> str:
    """
    Generate a video based on the provided text.

    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The text to base the video on

    Returns:
        A URL to the generated video
    """
    if MOCK_VIDEOS:
        video_gen_result = {
            "error": False,
            "answer": "https://apiplatform-rhymes-prod-va.s3.amazonaws.com/" +
                      "20241103031651.mp4",
        }
    else:
        video_gen_result = cgsl.video_generation(user_query)
    if video_gen_result.get("error"):
        raise HTTPException(
            status_code=400,
            detail=f"{video_gen_result.get('error_message')} [GSAL-GV-E010]"
        )
    return video_gen_result.get("answer")


# Documentation and embedding tools


async def get_embedding(text: str, openai_client: AsyncOpenAI) -> List[float]:
    """Get embedding vector from OpenAI."""
    try:
        response = await openai_client.embeddings.create(
            model="text-embedding-3-small", input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"Error getting embedding: {e}")
        return [0] * 1536  # Return zero vector on error


@pydantic_ai_agent.tool
async def retrieve_relevant_documentation(
    ctx: RunContext[PydanticAIDeps], user_query: str
) -> str:
    """
    Retrieve relevant documentation chunks based on the query with RAG.

    Args:
        ctx: The context including the Supabase client and OpenAI client
        user_query: The user's question or query

    Returns:
        A formatted string containing the top 5 most relevant documentation
        chunks
    """
    try:
        # Get the embedding for the query
        query_embedding = await get_embedding(
            user_query,
            ctx.deps.openai_client)

        # Query Supabase for relevant documents
        result = ctx.deps.supabase.rpc(
            "match_site_pages",
            {
                "query_embedding": query_embedding,
                "match_count": 5,
                "filter": {"source": "pydantic_ai_docs"},
            },
        ).execute()

        if not result.data:
            return "No relevant documentation found."

        # Format the results
        formatted_chunks = []
        for doc in result.data:
            chunk_text = f"""
# {doc['title']}

{doc['content']}
"""
            formatted_chunks.append(chunk_text)

        # Join all chunks with a separator
        return "\n\n---\n\n".join(formatted_chunks)

    except Exception as e:
        print(f"Error retrieving documentation: {e}")
        return f"Error retrieving documentation: {str(e)}"


@pydantic_ai_agent.tool
async def list_documentation_pages(ctx: RunContext[PydanticAIDeps]
                                   ) -> List[str]:
    """
    Retrieve a list of all available GenericSuite documentation pages.

    Returns:
        List[str]: List of unique URLs for all documentation pages
    """
    try:
        # Query Supabase for unique URLs where source is pydantic_ai_docs
        result = (
            ctx.deps.supabase.from_("site_pages")
            .select("url")
            .eq("metadata->>source", "pydantic_ai_docs")
            .execute()
        )

        if not result.data:
            return []

        # Extract unique URLs
        urls = sorted(set(doc["url"] for doc in result.data))
        return urls

    except Exception as e:
        print(f"Error retrieving documentation pages: {e}")
        return []


@pydantic_ai_agent.tool
async def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:
    """
    Retrieve the full content of a specific documentation page by combining
    all its chunks.

    Args:
        ctx: The context including the Supabase client
        url: The URL of the page to retrieve

    Returns:
        str: The complete page content with all chunks combined in order
    """
    try:
        # Query Supabase for all chunks of this URL, ordered by chunk_number
        result = (
            ctx.deps.supabase.from_("site_pages")
            .select("title, content, chunk_number")
            .eq("url", url)
            .eq("metadata->>source", "pydantic_ai_docs")
            .order("chunk_number")
            .execute()
        )

        if not result.data:
            return f"No content found for URL: {url}"

        # Format the page with its title and all chunks

        # Get the main title
        page_title = result.data[0]["title"].split(" - ")[0]
        formatted_content = [f"# {page_title}\n"]

        # Add each chunk's content
        for chunk in result.data:
            formatted_content.append(chunk["content"])

        # Join everything together
        return "\n\n".join(formatted_content)

    except Exception as e:
        print(f"Error retrieving page content: {e}")
        return f"Error retrieving page content: {str(e)}"
</file>

<file path="gsam_ottomator_agent/gsam_postgres_agent.py">
from typing import List, Optional, Dict, Any
from fastapi import FastAPI, HTTPException, Security, Depends
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from pydantic import BaseModel
from dotenv import load_dotenv
import asyncpg
import json
import os

from gsam_ottomator_agent.gsam_agent_lib import run_agent


# Load environment variables
load_dotenv()

# Database connection pool
db_pool = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global db_pool
    db_pool = await asyncpg.create_pool(os.getenv("DATABASE_URL"))
    yield
    # Shutdown
    if db_pool:
        await db_pool.close()


security = HTTPBearer()


def init_fastapi_app():
    # Initialize FastAPI app
    app = FastAPI(lifespan=lifespan)
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    return app


# Request/Response Models
class AgentRequest(BaseModel):
    query: str
    user_id: str
    request_id: str
    session_id: str


class AgentResponse(BaseModel):
    success: bool


def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)
                 ) -> bool:
    """Verify the bearer token against environment variable."""
    expected_token = os.getenv("API_BEARER_TOKEN")
    if not expected_token:
        raise HTTPException(
            status_code=500,
            detail="API_BEARER_TOKEN environment variable not set"
        )
    if credentials.credentials != expected_token:
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication token"
        )
    return True


async def fetch_conversation_history(session_id: str, limit: int = 10
                                     ) -> List[Dict[str, Any]]:
    """Fetch the most recent conversation history for a session."""
    try:
        async with db_pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT id, created_at, session_id, message
                FROM messages
                WHERE session_id = $1
                ORDER BY created_at DESC
                LIMIT $2
            """, session_id, limit)
            # Convert to list and reverse to get chronological order
            messages = [
                {
                    "id": str(row["id"]),
                    "created_at": row["created_at"].isoformat(),
                    "session_id": row["session_id"],
                    "message": row["message"]
                }
                for row in rows
            ]
            return messages[::-1]
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to fetch conversation history: {str(e)}")


async def store_message(session_id: str, message_type: str, content: str,
                        data: Optional[Dict] = None):
    """Store a message in the messages table."""
    message_obj = {
        "type": message_type,
        "content": content
    }
    if data:
        message_obj["data"] = data

    try:
        async with db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO messages (session_id, message)
                VALUES ($1, $2)
            """, session_id, json.dumps(message_obj))
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to store message: {str(e)}")


async def gsam_postgres_agent(
    request: AgentRequest,
    authenticated: bool = Depends(verify_token),
    headers: dict = None
):
    try:
        # Fetch conversation history from the DB
        conversation_history = await fetch_conversation_history(
            request.session_id)

        # Convert conversation history to format expected by agent
        messages = []
        for msg in conversation_history:
            msg_data = msg["message"]
            msg_type = msg_data["type"]
            msg_content = msg_data["content"]
            msg = {"role": msg_type, "content": msg_content}
            messages.append(msg)

        # Store user's query
        await store_message(
            session_id=request.session_id,
            message_type="human",
            content=request.query
        )
        """
        TODO:
        This is where you insert the custom logic to get the response from
        your agent.
        Your agent can also insert more records into the database to
        communicate actions/status as it is handling the user's
        question/request.
        Additionally:
            - Use the 'messages' array defined about for the chat history.
              This won't include the latest message from the user.
            - Use request.query for the user's prompt.
            - Use request.session_id if you need to insert more messages into
              the DB in the agent logic.
        """
        agent_response = run_agent(request.query, messages, headers)

        # Store agent's response
        await store_message(
            session_id=request.session_id,
            message_type="assistant",
            content=agent_response
        )

        return AgentResponse(success=True)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
</file>

<file path="gsam_ottomator_agent/gsam_supabase_agent.py">
from typing import List, Optional, Dict, Any
import os

from fastapi import FastAPI, HTTPException, Security, Depends
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from fastapi.middleware.cors import CORSMiddleware
from supabase import create_client, Client
from pydantic import BaseModel
from dotenv import load_dotenv

from gsam_ottomator_agent.gsam_agent_lib import run_agent

DEBUG = False

# Load environment variables
load_dotenv()

security = HTTPBearer()

# Supabase setup
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)


def init_fastapi_app():
    # Initialize FastAPI app
    app = FastAPI()
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    return app


# Request/Response Models


class AgentRequest(BaseModel):
    """Request model for the agent."""
    query: str
    user_id: str
    request_id: str
    session_id: str


class AgentResponse(BaseModel):
    success: bool


def verify_token(
    credentials: HTTPAuthorizationCredentials = Security(security)
) -> bool:
    """Verify the bearer token against environment variable."""
    expected_token = os.getenv("API_BEARER_TOKEN")
    if not expected_token:
        raise HTTPException(
            status_code=500,
            detail="API_BEARER_TOKEN environment variable not set"
        )
    if credentials.credentials != expected_token:
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication token"
        )
    return True


async def fetch_conversation_history(session_id: str, limit: int = 10
                                     ) -> List[Dict[str, Any]]:
    """Fetch the most recent conversation history for a session."""
    try:
        response = supabase.table("messages") \
            .select("*") \
            .eq("session_id", session_id) \
            .order("created_at", desc=True) \
            .limit(limit) \
            .execute()
        # Convert to list and reverse to get chronological order
        messages = response.data[::-1]
        return messages
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to fetch conversation history: {str(e)}")


async def store_message(session_id: str, message_type: str, content: str,
                        data: Optional[Dict] = None):
    """Store a message in the Supabase messages table."""
    message_obj = {
        "type": message_type,
        "content": content
    }
    if data:
        message_obj["data"] = data

    try:
        supabase.table("messages").insert({
            "session_id": session_id,
            "message": message_obj
        }).execute()
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to store message: {str(e)}")


async def gsam_supabase_agent(
    request: AgentRequest,
    authenticated: bool = Depends(verify_token),
    http_request: dict = None,
):
    try:
        # Fetch conversation history from the DB
        conversation_history = await fetch_conversation_history(
            request.session_id)

        # Convert conversation history to format expected by agent
        # This will be different depending on your framework (Pydantic AI,
        # LangChain, etc.)
        messages = []
        for msg in conversation_history:
            msg_data = msg["message"]
            msg_type = msg_data["type"]
            msg_content = msg_data["content"]
            msg = {"role": msg_type, "content": msg_content}
            messages.append(msg)

        # Store user's query
        await store_message(
            session_id=request.session_id,
            message_type="human",
            content=request.query
        )
        """
        TODO:
        This is where you insert the custom logic to get the response from
        your agent.
        Your agent can also insert more records into the database to
        communicate actions/status as it is handling the user's
        question/request.
        Additionally:
            - Use the 'messages' array defined about for the chat history.
              This won't include the latest message from the user.
            - Use request.query for the user's prompt.
            - Use request.session_id if you need to insert more messages into
              the DB in the agent logic.
        """
        agent_response = run_agent(request.query, messages, http_request)

        # Store agent's response
        await store_message(
            session_id=request.session_id,
            message_type="ai",
            content=agent_response,
            data={"request_id": request.request_id}
        )

        return AgentResponse(success=True)

    except Exception as e:
        print(f"Error processing request: {str(e)}")
        # Store error message in conversation
        await store_message(
            session_id=request.session_id,
            message_type="ai",
            content="I apologize, but I encountered an error processing"
                    " your request.",
            data={"error": str(e), "request_id": request.request_id}
        )
        if DEBUG:
            raise e
        return AgentResponse(success=False)
</file>

<file path="gsam_ottomator_agent/Makefile">
# .DEFAULT_GOAL := local
# .PHONY: tests
SHELL := /bin/bash

help:
	cat Makefile

stop:
	bash ./run_agent.sh stop

install: stop
	bash ./run_agent.sh install

run:
	bash ./run_agent.sh run

logs:
	bash ./run_agent.sh logs

requirements:
	bash ./run_agent.sh requirements

restart: stop run
</file>

<file path="gsam_ottomator_agent/README.md">
# GSAM Python FastAPI Agent

Author: [Carlos J. Ramirez](https://www.carlosjramirez.com)

Based on the [oTtomator Python agent](https://github.com/coleam00/SolnAI-agents/tree/main/~sample-python-agent~) code from: [Cole Medin](https://www.youtube.com/@ColeMedin)

The [GSAM](../README.md) Python FastAPI agent, is a tool to help on the software development process for any Application. It allows to generate application ideas, app description, database estructures, and presentation content from a text prompt, and kick start code to be used with the [GenericSuite](https://genericsuite.carlosjramirez.com) library. It's compatible with the [OTTomator](https://ottomator.ai) [autogen_studio](https://studio.ottomator.ai).

## Overview

This AI-powered agent provides:

- **Innovative App Ideas Generation**: Craft mind-blowing web/mobile app concepts emphasizing unique features, target audiences, and potential uses.
- **Names Generation**: Propose catchy, creative names for software applications.
- **PowerPoint Content Creation**: Draft content for presentation slides and suggest prompts for generating presentation images.
- **App Description and Table Definitions**: Develop comprehensive application descriptions and detailed table definitions.
- **CRUD JSON and Python Code Generation**: Produce generic CRUD editor configuration JSON and corresponding Python code using Langchain Tools for specified operations.

It also:

- Process natural language queries
- Maintain conversation history
- Integrate with external AI models
- Store and retrieve conversation data
- Handle authentication and security

The agent comes in two variants:
1. **Supabase Version** (`gsam_supabase_agent.py`): Uses Supabase client for database operations
2. **Postgres Version** (`gsam_postgres_agent.py`): Uses direct PostgreSQL connection via asyncpg

## Prerequisites

- Python 3.11 or higher
- pip (Python package manager)
- PostgreSQL database or Supabase account
- Basic understanding of:
  - FastAPI and async Python
  - RESTful APIs
  - Pydantic models
  - Environment variables
  - PostgreSQL (for Postgres version)

## Core Components

### 1. FastAPI Application (`gsam_ottomator_agent_app.py`)

The main application is built using FastAPI, providing:

- **Authentication**
  - Bearer token validation via environment variables
  - Secure endpoint protection
  - Customizable security middleware

- **Request Handling**
  - Async endpoint processing
  - Structured request validation
  - Error handling and HTTP status codes

- **Database Integration**
  - Supabase connection management
  - Message storage and retrieval
  - Session-based conversation tracking

### 2. Data Models

#### Request Model
```python
class AgentRequest(BaseModel):
    query: str        # The user's input text
    user_id: str      # Unique identifier for the user
    request_id: str   # Unique identifier for this request
    session_id: str   # Current conversation session ID
```

#### Response Model
```python
class AgentResponse(BaseModel):
    success: bool     # Indicates if the request was processed successfully
```

### 3. Database Schema

The agent uses Supabase tables with the following structure:

#### Messages Table
```sql
messages (
    id: uuid primary key
    created_at: timestamp with time zone
    session_id: text
    message: jsonb {
        type: string       # 'human' or 'assistant'
        content: string    # The message content
        data: jsonb       # Optional additional data
    }
)
```

## Setup

1. **Clone Repository**
   ```bash
   # Clone the repository
   git clone git clone https://github.com/tomkat-cr/genericsuite-app-maker.git
   cd genericsuite-app-maker

   # Copy example environment file
   cp .env.example .env

   # Edit .env with your credentials
   nano .env  # or use your preferred editor
   ```

2. **Configure Environment Variables**

   > ⚠️ **Important**: For Docker, do not wrap environment variable values in quotes, even if they contain special characters. Docker will handle the values correctly without quotes.

   #### Supabase Configuration
   Required environment variables in `.env` file (do not use quotes around values):
   ```env
   SUPABASE_URL=your-project-url
   SUPABASE_SERVICE_KEY=your-service-key
   API_BEARER_TOKEN=your-token-here
   ```

   #### PostgreSQL Configuration
   Required environment variables:
   ```env
   DATABASE_URL=postgresql://user:password@localhost:5432/dbname
   API_BEARER_TOKEN=your-chosen-token
   ```

   The DATABASE_URL format is:
   ```plaintext
   postgresql://[user]:[password]@[host]:[port]/[database_name]
   ```

   #### LLM configurations
   ```env
   OPENROUTER_API_KEY=your-api-key-here
   OPENROUTER_MODEL_NAME=your-model-name-here
   ```

   #### Image generation configurations
   ```env
   # to use HuggingFace and Flux
   HUGGINGFACE_API_KEY=your-api-key-here
   ```

   #### Video generation configurations
   ```env
   # to use Rhymes Allegro
   RHYMES_ALLEGRO_API_KEY=your-api-key-here
   ```

3. **Create Database Tables**
   For both Supabase and PostgreSQL, you'll need to create the following tables:

```sql
-- Enable the pgcrypto extension for UUID generation
CREATE EXTENSION IF NOT EXISTS pgcrypto;

CREATE TABLE messages (
    id uuid DEFAULT gen_random_uuid() PRIMARY KEY,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    session_id TEXT NOT NULL,
    message JSONB NOT NULL
);

CREATE INDEX idx_messages_session_id ON messages(session_id);
CREATE INDEX idx_messages_created_at ON messages(created_at);

ALTER publication supabase_realtime ADD TABLE messages;
```

   > Note: If you're using Supabase, the `pgcrypto` extension is already enabled by default.

## Installation Methods

### Docker Installation (Recommended)

1. Build the base images
```bash
cd genericsuite-app-maker/gsam_ottomator_agent
make install
```

2. Run the container:
```bash
cd genericsuite-app-maker/gsam_ottomator_agent
make run
```

The agent will be available at `http://localhost:8001`.

To restart the container:
```bash
cd genericsuite-app-maker/gsam_ottomator_agent
make restart
```

To Stop and destroy the container:
```bash
cd genericsuite-app-maker/gsam_ottomator_agent
make stop
```

### Local Installation (Docker Alternative)

1. Create and activate virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

2. Run the agent:

For Supabase version:

Set the `SUPABASE_URL` and `SUPABASE_SERVICE_KEY` environment variables in your `.env` file, then run:
 
```bash
uvicorn gsam_ottomator_agent_app:app --host 0.0.0.0 --port 8001
```

For PostgreSQL version:

Set the `DATABASE_URL` environment variable and leave `SUPABASE_URL` and `SUPABASE_SERVICE_KEY` empty in your `.env` file, then run:
 
```bash
uvicorn gsam_ottomator_agent_app:app --host 0.0.0.0 --port 8001
```

## Configuration

The agent uses environment variables for configuration. You can set these variables in a `.env` file or using your operating system's environment variable settings.

## Making Your First Request

Test your agent using curl or any HTTP client:

### Supabase Version
```bash
curl -X POST http://localhost:8001/api/gsam-supabase-agent \
  -H "Authorization: Bearer your-token-here" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Hello, agent!",
    "user_id": "test-user",
    "request_id": "test-request-1",
    "session_id": "test-session-1"
  }'
```

### Postgres Version
```bash
curl -X POST http://localhost:8001/api/gsam-postgres-agent \
  -H "Authorization: Bearer your-token-here" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Hello, agent!",
    "user_id": "test-user",
    "request_id": "test-request-1",
    "session_id": "test-session-1"
  }'
```

## Troubleshooting

Common issues and solutions:

1. **Authentication Errors**
   - Verify bearer token in environment
   - Check Authorization header format
   - Ensure token matches exactly

2. **Database Connection Issues**
   - For Supabase:
     - Verify Supabase credentials
     - Validate table permissions
   - For PostgreSQL:
     - Check DATABASE_URL format
     - Verify database user permissions
     - Ensure database is running and accessible
     - Check if tables are created correctly

3. **Performance Problems**
   - Check database query performance
   - Consider caching frequently accessed data
   - For PostgreSQL:
     - Monitor connection pool usage
     - Adjust pool size if needed (default is reasonable for most cases)


## Credits

This project is developed and maintained by [Carlos J. Ramirez](https://www.linkedin.com/in/carlosjramirez/). For more information or to contribute to the project, visit [GenericSuite App Maker on GitHub](https://github.com/tomkat-cr/genericsuite-app-maker).

Happy Coding!
</file>

<file path="gsam_ottomator_agent/run_agent.sh">
#!/bin/bash
# run_agent.sh
# 2025-01-28 | CR
#
set -e

REPO_BASEDIR="`pwd`"
cd "`dirname "$0"`"
SCRIPTS_DIR="`pwd`"
cd "${REPO_BASEDIR}"

set -o allexport
if [ ! -f .env ]
then
    echo "ERROR: .env file not found"
    exit 1
fi
if ! source .env
then
    if ! . .env
    then
        echo "ERROR: .env file could not be sourced"
        exit 1
    fi
fi
set +o allexport ;

start_venv() {
    if [ ! -d venv ]; then
        if ! python -m venv venv
        then
            echo "Error creating virtual environment"
            exit 1
        fi
    fi
    if ! source venv/bin/activate
    then
        if ! . venv/bin/activate
        then
            echo "Error activating virtual environment"
            exit 1
        fi
    fi
}

install_requirements() {
    start_venv
    if [ -f requirements.txt ]; then
        pip install -r requirements.txt
    else
        pip install --upgrade pip;
        # Dependencies needed for the Agent:
        # fastapi, uvicorn, pydantic, pydantic, pydantic, supabase, asyncpg, nest_asyncio, python-dotenv, llama-index
        # Dependencies needed for the UI:
        # streamlit, requests, python-dotenv, pymongo, python-pptx, openai, ollama, groq, together, llama-index
        if ! pip install \
            fastapi \
            uvicorn \
            pydantic \
            pydantic-ai \
            pydantic-ai[logfire] \
            supabase \
            asyncpg \
            nest_asyncio \
            streamlit \
            requests \
            python-dotenv \
            pymongo \
            python-pptx \
            openai \
            ollama \
            groq \
            together \
            llama-index;
        then
            echo "Error installing requirements"
            exit 1
        fi
        REQUIREMENTS_DIR="."
        if [ ! -f "gsam_ottomator_agent_app.py" ]; then
            REQUIREMENTS_DIR=".."
        fi
        if ! pip freeze > "${REQUIREMENTS_DIR}/requirements.txt"
        then
            echo "Error saving requirements"
            exit 1
        fi
    fi
}

check_docker() {
    if ! command -v docker &> /dev/null
    then
        echo "ERROR: Docker is not installed"
        exit 1
    fi
    if ! docker ps &> /dev/null
    then
        echo "ERROR: Docker is not running"
        exit 1
    fi
}

if [ "$ACTION" = "" ]; then
    ACTION="$1"
fi
if [ "$ACTION" = "" ]; then
    echo "Usage: run_agent.sh <install|run|stop|requirements|uvicorn_server|logs>"
    exit 1
fi

if [ "$ACTION" = "requirements" ]; then
    echo ""
    echo "Install requirements"
    echo ""
    install_requirements
    deactivate
fi

if [ "$ACTION" = "run_uvicorn_server" ]; then
    echo ""
    echo "Run the uvicorn server"
    echo ""
    install_requirements
    if [ "${PORT}" = "" ];then
        PORT="8001"
    fi
    pwd
    ls -la
    uvicorn gsam_ottomator_agent_app:app --host 0.0.0.0 --port ${PORT} --reload
    deactivate
fi

if [ "$ACTION" = "install" ]; then
    check_docker
    echo ""
    echo "Build the base image first (make sure Docker is running on your machine)"
    echo ""
    cd ./base_python_docker
    docker build -t ottomator/base-python:latest .
    cd ..

    echo ""
    echo "2. Build the agent image (you can swap between Supabase and PostgreSQL versions in the Dockerfile):"
    echo ""
    docker build -t gsam-python-agent .
fi

if [ "$ACTION" = "run" ]; then
    check_docker
    echo ""
    echo "Run the container"
    echo ""
    docker run -v "$(pwd)/..":/app -d --name gsam-python-agent -p 8001:8001 --env-file .env gsam-python-agent
    docker ps
    docker logs -f gsam-python-agent
fi

if [ "$ACTION" = "stop" ]; then
    check_docker
    echo ""
    echo "Stop and remove the container"
    echo ""
    if ! docker stop gsam-python-agent
    then
        echo "Container not running"
    fi
    if ! docker rm gsam-python-agent
    then
        echo "Container already removed"
    fi
    docker ps
fi

if [ "$ACTION" = "logs" ]; then
    check_docker
    docker logs -f gsam-python-agent
fi

echo ""
echo "Done!"
</file>

<file path="images/.gitignore">
*
!.gitignore
</file>

<file path="input/.gitignore">
*
!.gitignore
!user_input_example.md
</file>

<file path="input/user_input_example.md">
The **Construction Project Management System** is designed to help construction companies manage and track multiple construction projects and their respective phases. This system allows companies to store information about project owners, project details, various stages of construction, and the materials/resources required for each phase.

This system includes:
1. **Owners**: Individuals or entities that own or finance the construction projects (`owners` table).
2. **Construction Projects**: Details of each project, such as location, type, start and end dates (`projects` table).
3. **Project Phases**: Specific phases within a project, each with its timeline and requirements (`phases` table).
4. **Resources and Materials**: Materials and tools associated with each phase, tracked to manage inventory and budgeting (`resources` table).

With this system, construction companies can track the lifecycle of each project, manage resources effectively, and ensure that all required materials are available for every phase of construction.

---

### **Table Definitions**

#### 1. **Owners**

(`owners` table)
 
| Field Name       | Field Type   | Field Length | Primary Key | Description                                         |
|------------------|--------------|--------------|-------------|-----------------------------------------------------|
| `owner_id`       | INT          | -            | Yes         | Unique identifier for each owner.                   |
| `name`           | VARCHAR      | 100          | No          | Name of the owner or entity financing the project.  |
| `contact_email`  | VARCHAR      | 100          | No          | Email address of the owner for contact purposes.    |
| `phone_number`   | VARCHAR      | 15           | No          | Contact phone number of the owner.                  |
| `address`        | VARCHAR      | 255          | No          | Physical address of the owner.                      |

**Description**: Stores information about the owners or entities that are financing each construction project.

#### 2. **Construction Projects**

(`projects` table)

| Field Name       | Field Type   | Field Length | Primary Key | Description                                            |
|------------------|--------------|--------------|-------------|--------------------------------------------------------|
| `project_id`     | INT          | -            | Yes         | Unique identifier for each project.                    |
| `owner_id`       | INT          | -            | No (FK)     | References the owner financing the project.            |
| `project_name`   | VARCHAR      | 150          | No          | Name of the construction project.                      |
| `location`       | VARCHAR      | 255          | No          | Location where the project is being executed.          |
| `start_date`     | DATE         | -            | No          | Starting date of the project.                          |
| `end_date`       | DATE         | -            | No          | Estimated or actual completion date of the project.    |
| `project_type`   | VARCHAR      | 50           | No          | Type of construction (e.g., residential, commercial).  |

**Description**: Contains details of each construction project, including ownership, location, and duration.

#### 3. **Project Phases**

(`phases` table)

| Field Name       | Field Type   | Field Length | Primary Key | Description                                            |
|------------------|--------------|--------------|-------------|--------------------------------------------------------|
| `phase_id`       | INT          | -            | Yes         | Unique identifier for each project phase.              |
| `project_id`     | INT          | -            | No (FK)     | References the associated project.                     |
| `phase_name`     | VARCHAR      | 100          | No          | Name or description of the project phase.              |
| `start_date`     | DATE         | -            | No          | Starting date of the phase.                            |
| `end_date`       | DATE         | -            | No          | Completion date of the phase.                          |
| `phase_status`   | ENUM         | -            | No          | Status of the phase (e.g., pending, in-progress, complete).|

**Description**: Tracks specific phases of each construction project, such as foundation, framing, plumbing, and electrical phases.

#### 4. **Resources and Materials**

(`resources` table)

| Field Name       | Field Type   | Field Length | Primary Key | Description                                            |
|------------------|--------------|--------------|-------------|--------------------------------------------------------|
| `material_id`    | INT          | -            | Yes         | Unique identifier for each material.                   |
| `material_name`  | VARCHAR      | 100          | No          | Name or description of the material or resource.       |
| `unit_cost`      | DECIMAL(10,2)| -            | No          | Cost per unit of the material.                         |
| `quantity`       | INT          | -            | No          | Quantity available in stock.                           |
| `supplier_name`  | VARCHAR      | 100          | No          | Supplier providing the material.                       |

**Description**: Stores details about the resources and materials required for the construction projects, including inventory levels and cost per unit.

#### 5. **Phase Materials**

| Field Name       | Field Type   | Field Length | Primary Key | Description                                            |
|------------------|--------------|--------------|-------------|--------------------------------------------------------|
| `phase_material_id` | INT      | -            | Yes         | Unique identifier for each phase material entry.       |
| `phase_id`       | INT          | -            | No (FK)     | References the phase to which the material is assigned.|
| `material_id`    | INT          | -            | No (FK)     | References the material used in this phase.            |
| `quantity_used`  | INT          | -            | No          | Quantity of the material required for this phase.      |

**Description**: Links specific materials to project phases, indicating the quantity of each material used in a particular phase.

---

### **Relationships**

- **Owners** to **Construction Projects**: One-to-Many (One owner may finance multiple projects, but each project has a single owner).
- **Construction Projects** to **Project Phases**: One-to-Many (Each project consists of multiple phases, but each phase belongs to one project).
- **Project Phases** to **Phase Materials**: One-to-Many (Each phase may require multiple types of materials, but each entry in Phase Materials is associated with a specific phase).
- **Resources and Materials** to **Phase Materials**: One-to-Many (Each material can be used in multiple phases, but each entry in Phase Materials references a single material).
</file>

<file path="lib/codegen_ai_abstracts_constants.py">
# Avoid flake8 and pylint errors for long lines
# flake8: noqa
# pylint: disable=line-too-long

DEFAULT_PROMPT_ENHANCEMENT_TEXT = """
Improve the given initial prompt to make it clearer, more effective, and aligned with the task objectives and expectations.

# Steps

1. Carefully review the initial prompt provided.
2. Identify unclear instructions, missing details, or any ambiguity that could affect the model’s performance.
3. Add specific guidelines, necessary context, or well-defined examples if needed.
4. Make sure the prompt provides a clear and straightforward reasoning process before concluding the answer. 
5. Ensure the expected output is explicitly defined, including format, structure, and requirements.
6. Avoid unnecessary complexity—focus on simplicity, clarity, and effectiveness.

# Output Format

An enhanced version of the prompt with:
- Clarity in language and expectations
- Structured reasoning before conclusions
- Defined output format for consistency.

# Example

**Initial Prompt (Input)**:  
"Explain why a tomato is a fruit, and then list some related fruits."

**Enhanced Prompt (Output)**:  
"Explain step-by-step why a tomato is scientifically classified as a fruit. Start by describing the botanical characteristics that belong to fruits. After explaining, provide a list of other fruits that share similar characteristics as a tomato, such as being soft and containing seeds."

### Notes:
- When enhancing the prompt, ensure reasoning precedes any conclusion or answer.
- Always define how the output should be structured (e.g., format length or elements).
"""
</file>

<file path="lib/codegen_ai_abstracts.py">
"""
LLM provider abstract class
"""
from lib.codegen_utilities import get_default_resultset
from lib.codegen_utilities import log_debug
from lib.codegen_ai_abstracts_constants import DEFAULT_PROMPT_ENHANCEMENT_TEXT


DEBUG = False


def prepare_model_params(model_params: dict, naming: dict = None) -> dict:
    """
    Returns the OpenAI API client and model configurations
    """
    naming = naming or {
        "model_name": "model",
    }
    model_params_naming = model_params.get("llm_model_params_naming", {})
    forced_values = model_params.get("llm_model_forced_values", {})

    # Parameters reference:
    # https://platform.openai.com/docs/api-reference/chat/create

    # Prepare the OpenAI client configurations
    client_config = {}
    for key in ["base_url", "api_key"]:
        if model_params.get(key):
            client_config[naming.get(key, key)] = model_params[key]

    # Prepare the OpenAI API request configurations
    model_config = {}
    for key in ["model", "model_name", "messages", "stop"]:
        if model_params.get(key):
            model_config[naming.get(key, key)] = model_params[key]
    for key in ["temperature", "top_p", "frequency_penalty",
                "presence_penalty"]:
        if model_params.get(key):
            model_config[naming.get(key, key)] = \
                float(model_params[key])
    for key in ["top_k", "max_tokens"]:
        if model_params.get(key):
            model_config[naming.get(key, key)] = int(model_params[key])
    for key in ["stream"]:
        if model_params.get(key):
            model_config[naming.get(key, key)] = model_params[key] == "1"
    # Rename model parameters depending on the model name
    log_debug(f"CODEGEN_AI_ABSTRACTS.PY | prepare_model_params"
              f"\n | model_params: {model_params}",
              debug=DEBUG)

    # Special cases
    if model_params.get("model"):

        # Model parameters renaming
        if model_params["model"] in model_params_naming:
            log_debug(
                "CODEGEN_AI_ABSTRACTS.PY | prepare_model_params"
                f"\n | model_params[model]: {model_params['model']} "
                f"\n | model_params_naming[model_params[\"model\"]]: "
                f"{model_params_naming[model_params['model']]} "
                f"\n | model_params_naming: {model_params_naming}",
                debug=DEBUG)
            for rename_from, rename_to in \
                    model_params_naming[model_params["model"]]:
                log_debug(
                    "CODEGEN_AI_ABSTRACTS.PY | prepare_model_params"
                    f"\n | REVIEW rename elements: {rename_from}, {rename_to}",
                    debug=DEBUG)
                if model_params.get(rename_from):
                    log_debug(
                        "CODEGEN_AI_ABSTRACTS.PY | prepare_model_params"
                        f"\n | ACTION rename: {rename_from}, {rename_to}",
                        debug=DEBUG)
                    model_config[rename_to] = model_params[rename_from]
                    del model_config[rename_from]

        # Model parameters forced values
        if model_params["model"] in forced_values:
            log_debug(
                "CODEGEN_AI_ABSTRACTS.PY | prepare_model_params"
                f"\n | model_params[model]: {model_params['model']} "
                f"\n | forced_values[model_params[\"model\"]]: "
                f"{forced_values[model_params['model']]} "
                f"\n | forced_values: {forced_values}",
                debug=DEBUG)
            for key, value in forced_values[model_params["model"]].items():
                model_config[key] = value

    log_debug(f"CODEGEN_AI_ABSTRACTS.PY | prepare_model_params"
              f"\n | client_config: {client_config}"
              f"\n | model_config: {model_config}",
              debug=DEBUG)

    return {
        "client_config": client_config,
        "model_config": model_config,
    }


class LlmProviderAbstract:
    """
    Abstract class for LLM providers
    """
    def __init__(self, params: dict):
        self.params = params
        self.provider = self.params.get("provider")
        self.api_key = self.params.get("api_key")
        self.model_name = self.params.get("model_name")
        self.naming = self.params.get("naming") or {
            "model_name": "model",
        }
        self.llm = None

    def init_llm(self):
        """
        Abstract method for initializing the LLM
        """
        pass

    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Abstract method for querying the LLM
        """
        raise NotImplementedError

    def video_gen(
        self,
        question: str,
        prompt_enhancement_text: str = None
    ) -> dict:
        """
        Abstract method for video or other llm/model type request
        """
        raise NotImplementedError

    def image_gen(
        self,
        question: str,
        prompt_enhancement_text: str = None,
        image_extension: str = 'jpg',
    ) -> dict:
        """
        Abstract method for image request
        """
        raise NotImplementedError

    def video_gen_followup(
        self,
        request_response: dict,
        wait_time: int = 60
    ):
        """
        Perform a video or other llm/model type generation request check
        """
        raise NotImplementedError

    def query_from_text_model(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        result = get_default_resultset()
        if not self.params.get("text_model_class"):
            result["error"] = True
            result["error_message"] = "Text model class not provided"
            return result
        return self.params["text_model_class"].query(
            prompt,
            question,
            prompt_enhancement_text,
            unified)

    def prompt_enhancer(
        self,
        question: str,
        prompt_enhancement_text: str = None
    ) -> dict:
        """
        Perform a prompt enhancement request
        """
        response = get_default_resultset()
        if not prompt_enhancement_text:
            prompt_enhancement_text = DEFAULT_PROMPT_ENHANCEMENT_TEXT
        log_debug("PROMPT_ENHANCER"
                  "\n | prompt_enhancement_text: "
                  f"\n{prompt_enhancement_text}"
                  "\n | question:"
                  f"\n{question}",
                  debug=DEBUG)
        llm_response = self.query(prompt_enhancement_text, question)
        log_debug("PROMPT_ENHANCER | llm_response: " + f"{llm_response}",
                  debug=DEBUG)
        if llm_response['error']:
            return llm_response
        refined_prompt = llm_response['response']
        # Clean the refined prompt
        refined_prompt = refined_prompt.replace("\n", " ")
        refined_prompt = refined_prompt.replace("\r", " ")
        refined_prompt = refined_prompt.replace("Refined Prompt:", "")
        refined_prompt = refined_prompt.replace("Enhanced Prompt (Output):",
                                                "")
        refined_prompt = refined_prompt.replace("Enhanced Prompt:", "")
        refined_prompt = refined_prompt.replace("**Enhanced Prompt**:", "")
        refined_prompt = refined_prompt.replace("**Enhanced Prompt**", "")
        refined_prompt = refined_prompt.strip()
        refined_prompt = refined_prompt.replace('"', '')
        response['response'] = refined_prompt
        return response

    def get_messages_array(
        self,
        system_prompt: str,
        user_input: str,
        unified: bool = False,
    ) -> dict:
        """
        Return the messages array for the LLM call in the format expected by
        the OpenAI API.
        The messages array is a list of dictionaries, where each dictionary
        represents a message with a role (system or user) and a content.
        * If the system prompt is not None, it is added as the first message
        with the role "system".
        * If the system prompt is None, the user input is used as the first
        message with the role "user" (no system prompt).
        * If the system prompt has the "{question}" token, there'll be only
        one message with the role "user" and its content will be the system
        prompt with the question token replaced with the user input.
        The user input must have content always.

        Args:
            system_prompt (str): The system prompt for the LLM call.
            user_input (str): The user input for the LLM call.
            unified (bool): Whether to use the unified prompt or not.
                Defaults to False.
                It turns True if the system prompt has the "{question}" token
                or not set.

        Returns:
            dict: The messages array for the LLM call.
        """
        if not system_prompt or "{question}" in system_prompt:
            unified = True
        if unified:
            # Check if the system prompt string has the "{question}" string
            if system_prompt:
                unified_prompt = f"{system_prompt}\n{user_input}"
            else:
                unified_prompt = f"{user_input}"
            if unified_prompt and "{question}" in unified_prompt:
                unified_prompt = unified_prompt.replace(
                    "{question}", f"{user_input}")
            messages = [
                {
                    'role': 'user',
                    'content': unified_prompt.strip()
                }
            ]
        else:
            messages = [
                {
                    'role': 'system',
                    'content': system_prompt.strip()
                },
                {
                    'role': 'user',
                    'content': user_input.strip()
                }
            ]
        return messages

    def get_prompts_and_messages(
        self,
        system_prompt: str,
        user_input: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a LLM refined prompt request and messages array generation
        It's a wrapper for the get_messages_array and get_refined_prompt
        methods.

        Args:
            system_prompt (str): The system prompt for the LLM call.
            user_input (str): The user input for the LLM call.
            prompt_enhancement_text (str): The prompt enhancement text.
                None or empty means there'll be no prompt enhancement.
                Defaults to None.
            unified (bool): Whether to use the unified prompt or not.
                Defaults to False.
                It turns True if the system prompt has the "{question}" token
                or not set.

        Returns:
            dict: a standard response array, including the refined system
                prompt, user input and messages array for the LLM call.
                The structure is:
                {
                    "error": bool,
                    "error_message": str,
                    "resulltset": dict,
                    "system_prompt": str,
                    "user_input": str,
                    "messages": list,
                    "refined_prompt": str,
                }
        """
        response = get_default_resultset()
        # "system_prompt" attributte in the response will be empty if
        # not system_prompt provided or equals to "{question}",
        # meaning that the system_prompt will be the question
        # and there'll be only user role message in the messages array
        response["system_prompt"] = system_prompt if system_prompt \
            or system_prompt != "{question}" else ""
        response["user_input"] = user_input
        response["refined_prompt"] = None

        if not prompt_enhancement_text:
            response["messages"] = self.get_messages_array(
                system_prompt=response["system_prompt"],
                user_input=response["user_input"],
                unified=unified,
            )
            return response

        if response["system_prompt"] and \
           response["system_prompt"] != "{question}":
            # Refine only the system prompt...
            llm_response = self.prompt_enhancer(
                response["system_prompt"], prompt_enhancement_text)
            if llm_response['error']:
                return llm_response
            response["refined_prompt"] = llm_response['response'] if \
                llm_response['response'] != response["system_prompt"] \
                else None
            response["system_prompt"] = llm_response['response']
        else:
            # There's no system prompt, so the user input has or is the
            # prompt... lets refine it
            if response["user_input"]:
                llm_response = self.prompt_enhancer(
                    response["user_input"], prompt_enhancement_text)
                if llm_response['error']:
                    return llm_response
                response["refined_prompt"] = llm_response['response'] if \
                    llm_response['response'] != response["user_input"] \
                    else None
                response["user_input"] = llm_response['response']

        response["messages"] = self.get_messages_array(
            system_prompt=response["system_prompt"],
            user_input=response["user_input"],
            unified=unified,
        )
        return response

    def get_model_args(
        self,
        additional_params: dict = None,
        for_openai_api: bool = False
    ):
        """
        Returns the model parameters for the LLM call

        Args:
            messages (list): The messages array for the LLM call.
                Defaults to None.
            for_openai_api (bool): Whether to return the model parameters for
                the OpenAI API or not. Defaults to False.

        Returns:
            dict: The model parameters for the LLM call.
        """
        if not additional_params:
            additional_params = {}
        params = self.params.copy()
        params.update(additional_params)
        model_params = prepare_model_params(
            params, self.naming)["model_config"]
        if for_openai_api:
            model_params["provider"] = self.provider
            model_params["api_key"] = params.get("api_key")
            model_params["base_url"] = params.get("base_url")
            model_params["stop"] = params.get("stop")
        if params.get('model', params.get('model_name')) == 'ollama':
            if model_params.get('temperature'):
                model_params['options'] = {
                    "temperature": model_params['temperature']
                }
                del model_params['temperature']
        return model_params

    def get_client_args(self, additional_params: dict = None):
        """
        Returns the client configuration for the LLM call
        """
        if not additional_params:
            additional_params = {}
        params = self.params.copy()
        params.update(additional_params)
        return prepare_model_params(params, self.naming)["client_config"]

    def get_unified_flag(self):
        """
        Returns the unified flag.
        Returns:
            bool: True if the model or provider is not allowed
                  to have a system prompt
        """
        unified = False
        nspa_provider = \
            self.params.get("no_system_prompt_allowed_providers", [])
        nspa_model = self.params.get("no_system_prompt_allowed_models", [])
        log_debug(
            "GET_UNIFIED_FLAG # 1" +
            f"\n| provider: {self.provider}" +
            f"\n| model: {self.model_name}" +
            f"\n| nspa_provider: {nspa_provider}" +
            f"\n| nspa_model: {nspa_model}",
            debug=DEBUG)
        if self.provider in nspa_provider or self.model_name in nspa_model:
            unified = True
        log_debug(
            "GET_UNIFIED_FLAG # 2" +
            f"\n| unified: {unified}",
            debug=DEBUG)
        return unified
</file>

<file path="lib/codegen_ai_provider_aimlapi.py">
"""
AI/ML API
"""
import os

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
)
from lib.codegen_ai_abstracts import LlmProviderAbstract
from lib.codegen_ai_provider_openai import get_openai_api_response


DEBUG = False


class AiMlApiLlm(LlmProviderAbstract):
    """
    AI/ML API LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a AI/ML API request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response
        model_params = self.get_model_args(
            additional_params={
                "model": (self.model_name or
                          os.environ.get("AIMLAPI_MODEL_NAME")),
                "api_key": (self.api_key or
                            os.environ.get("AIMLAPI_API_KEY")),
                "base_url": "https://api.aimlapi.com",
                "messages": pam_response['messages'],
            },
            for_openai_api=True,
        )
        # Get the LLM response
        log_debug("aimlapi_query | " +
                  f"model_params: {model_params}", debug=DEBUG)
        response = get_openai_api_response(model_params)
        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("aimlapi_query | " +
                  f"response: {response}", debug=DEBUG)
        return response
</file>

<file path="lib/codegen_ai_provider_groq.py">
"""
Groq API
"""
import os

from groq import Groq

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
)
from lib.codegen_ai_abstracts import LlmProviderAbstract


DEBUG = False


class GroqLlm(LlmProviderAbstract):
    """
    Groq LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a Groq request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response
        model_params = self.get_model_args(
            additional_params={
                "model": self.model_name or os.environ.get("GROQ_MODEL_NAME"),
                "messages": pam_response['messages'],
            },
        )
        client_params = self.get_client_args(
            additional_params={
                "api_key": self.api_key or os.environ.get("GROQ_API_KEY"),
            },
        )
        log_debug("groq_query | " +
                  f"model_params: {model_params}", debug=DEBUG)
        client = Groq(**client_params)
        response_raw = client.chat.completions.create(**model_params)
        response['response'] = response_raw.choices[0].message.content
        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("groq_query | " +
                  f"response: {response}", debug=DEBUG)
        return response
</file>

<file path="lib/codegen_ai_provider_huggingface.py">
"""
HugginFace platform utilities
"""
from typing import Any
import os
import requests
import uuid

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
    error_resultset,
)
from lib.codegen_ai_abstracts import LlmProviderAbstract


DEBUG = False

# from genericsuite.util.app_context import CommonAppContext
# from genericsuite.util.app_logger import log_debug
# from genericsuite.util.utilities import (
#     get_default_resultset,
#     error_resultset,
#     get_mime_type,
# )
# from genericsuite.util.aws import upload_nodup_file_to_s3

# from genericsuite_ai.config.config import Config

# DEBUG = False
# cac = CommonAppContext()


class HuggingFaceLlm(LlmProviderAbstract):
    """
    HuggingFace LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a HuggingFace request
        """
        response = get_default_resultset()
        # Always a single message
        unified = True
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response

        model_params = {
            "inputs": pam_response["messages"][0]["content"],
            "parameters": {
                # "do_sample": True,
                # "max_new_tokens": 1024,
                # "temperature": 0.5,
                # "top_p": 1,
                # "repetition_penalty": 1.1,
                # "top_k": 40,
                # "typical_p": 1,
                # "truncate": None,
            },
            "options": {
                "use_cache": True,
            },
        }
        model_name = self.model_name or \
            os.environ.get("HUGGINGFACE_MODEL_NAME")
        response_raw = self.hf_query(
            repo_id=model_name,
            payload=model_params,
        )
        log_debug(
            "huggingface_query | " +
            f"response_raw BEFORE CONVERSION: {response_raw}",
            debug=DEBUG)
        try:
            response_raw = response_raw.json()
            log_debug(
                "huggingface_query | " +
                f"response_raw AFTER CONVERSION: {response_raw}",
                debug=DEBUG)
            if response_raw.get('error'):
                return error_resultset(
                    error_message=response_raw['error'],
                    message_code='HF-E010',
                )
            response['response'] = response_raw['message']['content']
        except requests.exceptions.JSONDecodeError:
            response['response'] = response_raw.text
        except Exception as e:
            return error_resultset(
                error_message=f"ERROR {e}",
                message_code='HF-E020',
            )
        response['refined_prompt'] = pam_response['refined_prompt']
        return response

    def hf_query(self, repo_id: str, payload: dict) -> Any:
        """
        Perform a HuggingFace query

        Args:
            api_url (str): HuggingFace API URL
            payload (dict): HuggingFace payload

        Returns:
            Any: HuggingFace response
        """
        # https://huggingface.co/docs/api-inference/detailed_parameters
        api_key = self.api_key or \
            os.environ.get("HUGGINGFACE_API_KEY")
        headers = {
            "Authorization": f"Bearer {api_key}",
        }
        base_url = os.environ.get(
            "HUGGINGFACE_API_URL",
            "https://api-inference.huggingface.co/models")
        api_url = f'{base_url}/{repo_id}'
        return requests.post(api_url, headers=headers, json=payload)


class HuggingFaceImageGen(HuggingFaceLlm):
    """
    HuggingFace Image Generation class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        return self.query_from_text_model(
            prompt,
            question,
            prompt_enhancement_text,
            unified)

    def image_gen(
        self,
        question: str,
        prompt_enhancement_text: str = None,
        image_extension: str = 'jpg',
    ) -> dict:
        """
        HuggingFace image generation
        """
        ig_response = get_default_resultset()
        if not question:
            return error_resultset(
                error_message='No question supplied',
                message_code='HFIG-E010',
            )

        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt="",
            prompt_enhancement_text=prompt_enhancement_text,
            unified=True,
        )
        if pam_response['error']:
            return pam_response

        if self.params.get("model_name"):
            img_model_name = self.params.get("model_name")
        else:
            img_model_name = os.environ.get("HUGGINGFACE_IMAGE_MODEL_NAME")
        if not img_model_name:
            return error_resultset(
                error_message='No model name supplied',
                message_code='HFIG-E020',
            )
        _ = DEBUG and log_debug(
            '1) huggingface_img_gen' +
            f'\n| question: {question}' +
            f'\n| api_url: {img_model_name}')

        image_bytes = self.hf_query(
            repo_id=img_model_name,
            payload={
                "inputs": pam_response["user_input"],
            }
        ).content

        # Generate a unique filename
        image_filename = f'hf_img_{uuid.uuid4()}.{image_extension}'
        target_path = os.environ.get("IMAGES_DIRECTORY", "./images")
        image_path = f'{target_path}/{image_filename}'

        # Create the temporary local file
        with open(image_path, 'wb') as f:
            f.write(image_bytes)

        # Store the image bytes in AWS
        # upload_result = upload_nodup_file_to_s3(
        #     file_path=image_path,
        #     original_filename=image_filename,
        #     bucket_name=settings.AWS_S3_CHATBOT_ATTACHMENTS_BUCKET,
        #     sub_dir=cac.app_context.get_user_id(),
        # )
        # if upload_result['error']:
        #     return error_resultset(
        #         error_message=upload_result['error_message'],
        #         message_code="HFIG-E030",
        #     )
        # Add the S3 URL to the response
        # upload_result['file_name'] = image_filename
        # upload_result['file_type'] = get_mime_type(image_filename)
        # upload_result['file_size'] = os.path.getsize(image_path)
        # ig_response['resultset'] = {'uploaded_file': upload_result}

        ig_response['response'] = image_path
        ig_response['refined_prompt'] = pam_response['refined_prompt']

        if DEBUG:
            log_debug('2) huggingface_img_gen | ig_response:')
            print(ig_response)

        return ig_response
</file>

<file path="lib/codegen_ai_provider_nvidia.py">
"""
Nvidia API
"""
import os

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
)
from lib.codegen_ai_abstracts import LlmProviderAbstract
from lib.codegen_ai_provider_openai import get_openai_api_response


DEBUG = False


class NvidiaLlm(LlmProviderAbstract):
    """
    Nvidia LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a Nvidia request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response
        model_params = self.get_model_args(
            additional_params={
                "model": (self.model_name or
                          os.environ.get("NVIDIA_MODEL_NAME")),
                "api_key": (self.api_key or
                            os.environ.get("NVIDIA_API_KEY")),
                "base_url": "https://integrate.api.nvidia.com/v1",
                "messages": pam_response['messages'],
            },
            for_openai_api=True,
        )
        # Get the LLM response
        log_debug("nvidia_query | " +
                  f"model_params: {model_params}", debug=DEBUG)
        response = get_openai_api_response(model_params)
        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("nvidia_query | " +
                  f"response: {response}", debug=DEBUG)
        return response
</file>

<file path="lib/codegen_ai_provider_ollama.py">
"""
Ollama API
"""
import os

import ollama
from ollama import Client

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
)
from lib.codegen_ai_abstracts import LlmProviderAbstract


DEBUG = False


class OllamaLlm(LlmProviderAbstract):
    """
    Ollama LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a Ollama request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response
        model_params = self.get_model_args(
            additional_params={
                "messages": pam_response['messages'],
            },
        )
        client_params = self.get_client_args(
            additional_params={
                "model": self.model_name or os.environ.get("OLLAMA_MODEL"),
            },
        )
        log_debug("ollama_query | " +
                  f"model_params: {model_params}", debug=DEBUG)
        if self.params.get("ollama_base_url"):
            client_params["base_url"] = self.params["ollama_base_url"]
        if client_params.get("base_url"):
            log_debug(
                "Using ollama client with base_url:" +
                f' {client_params.get("base_url")}', debug=DEBUG)
            self.log_debug("", debug=DEBUG)
            client = Client(host=client_params.get("base_url"))
            response_raw = client.chat(**model_params)
        else:
            response_raw = ollama.chat(**model_params)
        response['response'] = response_raw['message']['content']
        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("ollama_query | " +
                  f"response: {response}", debug=DEBUG)
        return response
</file>

<file path="lib/codegen_ai_provider_openai.py">
"""
OpenAI API
"""
import os

from openai import OpenAI
from openai.resources.images import ImagesResponse

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
)
from lib.codegen_ai_abstracts import (
    LlmProviderAbstract,
    prepare_model_params,
)


DEBUG = False


def get_openai_api_response(model_params: dict, naming: dict = None) -> dict:
    """
    Returns the OpenAI API response for a LLM request
    """
    response = get_default_resultset()
    configs = prepare_model_params(model_params, naming)
    # Initialize the OpenAI client
    try:
        client = OpenAI(**configs["client_config"])
    except Exception as e:
        response['error'] = True
        response['error_message'] = str(e)
        return response
    # Process the question and text
    try:
        llm_response = client.chat.completions.create(
            **configs["model_config"])
    except Exception as e:
        response['error'] = True
        response['error_message'] = str(e)

    if not response['error']:
        log_debug("get_openai_api_response | " +
                  f"{model_params.get('provider', 'Provider N/A')} " +
                  f" LLM response: {llm_response}", debug=DEBUG)
        try:
            if configs["model_config"].get('stream', False):
                response['response'] = ""
                for chunk in llm_response:
                    if chunk.choices[0].delta.content is not None:
                        print(chunk.choices[0].delta.content, end="")
                        response['response'] += chunk.choices[0].delta.content
            else:
                response['response'] = llm_response.choices[0].message.content

        except Exception as e:
            response['error'] = True
            response['error_message'] = str(e)
    return response


class OpenaiLlm(LlmProviderAbstract):
    """
    OpenAI LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a OpenAI request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response
        model_params = self.get_model_args(
            additional_params={
                "api_key": self.api_key or os.environ.get("OPENAI_API_KEY"),
                "model": self.model_name or os.environ.get("OPENAI_MODEL"),
                "messages": pam_response['messages'],
            },
            for_openai_api=True,
        )
        # Get the LLM response
        log_debug("openai_query | " +
                  f"model_params: {model_params}", debug=DEBUG)
        response = get_openai_api_response(model_params)
        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("openai_query | " +
                  f"response: {response}", debug=DEBUG)
        return response


class OpenaiImageGen(OpenaiLlm):
    """
    OpenAI Image generation class
    """
    def image_gen(
        self,
        question: str,
        prompt_enhancement_text: str = None,
        image_extension: str = 'jpg',
    ) -> dict:
        """
        Perform an OpenAI image generation request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt="",
            prompt_enhancement_text=prompt_enhancement_text,
            unified=True,
        )
        if pam_response['error']:
            return pam_response

        ig_model_name = os.environ.get("OPENAI_IMAGE_GEN_MODEL")
        model_params = {
            # "model": "dall-e-3",
            "model": ig_model_name,
            "prompt": pam_response["user_input"],
            "size": self.params.get("size", "1024x1024"),
            "quality": self.params.get("quality", "standard"),
            "n": 1,
        }

        log_debug("openai_image_gen | " +
                  f"model_params: {model_params}", debug=DEBUG)

        # Get the LLM response
        client = OpenAI(
            api_key=self.api_key or os.environ.get("OPENAI_API_KEY")
        )
        # Process the question and image
        ig_response = client.images.generate(**model_params)

        log_debug(f"openai_image_gen | {ig_model_name}"
                  f" | ig_response: {ig_response}",
                  debug=DEBUG)

        # The 'ImagesResponse' object has an attribute 'data' which is a
        # list of 'Image' objects.
        # We should iterate over this list and extract the URL from each
        # 'Image' object if it exists.

        # Check if the 'ig_response' is an instance of 'ImagesResponse'
        # and extract the URLs from the 'data' attribute
        if isinstance(ig_response, ImagesResponse):
            # Assuming each 'Image' object in the 'data' list has a
            # 'url' attribute
            image_urls = [image.url for image in ig_response.data
                          if hasattr(image, 'url')]
            response['response'] = image_urls
        else:
            # Handle other types of responses or raise an error
            response['error'] = True
            response['error_message'] = "ERROR [IAIG-E030] Unexpected " + \
                "response type received from image generation API."

        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("openai_image_gen | " +
                  f"response: {response}", debug=DEBUG)
        return response
</file>

<file path="lib/codegen_ai_provider_openrouter.py">
"""
X AI (Grok) API
"""
import os

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
)
from lib.codegen_ai_abstracts import LlmProviderAbstract
from lib.codegen_ai_provider_openai import get_openai_api_response


DEBUG = False


class OpenRouterLlm(LlmProviderAbstract):
    """
    OpenRouter LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a OpenRouter request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response
        model_params = self.get_model_args(
            additional_params={
                "model": (self.model_name or
                          os.environ.get("OPENROUTER_MODEL_NAME")),
                "api_key": (self.api_key or
                            os.environ.get("OPENROUTER_API_KEY")),
                "base_url": "https://openrouter.ai/api/v1",
                "messages": pam_response['messages'],
            },
            for_openai_api=True,
        )
        # Get the LLM response
        log_debug("openrouter | " +
                  f"model_params: {model_params}", debug=DEBUG)
        response = get_openai_api_response(model_params)
        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("openrouter | " +
                  f"response: {response}", debug=DEBUG)
        return response
</file>

<file path="lib/codegen_ai_provider_rhymes.py">
"""
Rhymes APIs
"""
import os
import time

import requests
import json

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
)
from lib.codegen_ai_provider_openai import get_openai_api_response
from lib.codegen_ai_abstracts import LlmProviderAbstract

DEBUG = False

RHYMES_SUCCESS_RESPONSES = ["success", "Success", '成功']


class AriaLlm(LlmProviderAbstract):
    """
    Aria LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a Aria request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response
        model_params = self.get_model_args(
            additional_params={
                "model": (self.model_name or
                          os.environ.get("RHYMES_MODEL_NAME")),
                "api_key": (self.api_key or
                            os.environ.get("RHYMES_ARIA_API_KEY")),
                "base_url": "https://api.rhymes.ai/v1",
                "stop": ["<|im_end|>"],
                "messages": pam_response['messages'],
            },
            for_openai_api=True,
        )
        # Get the OpenAI API response
        log_debug("aria_query | " +
                  f"model_params: {model_params}", debug=DEBUG)
        response = get_openai_api_response(model_params)
        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("aria_query | " +
                  f"response: {response}", debug=DEBUG)
        return response


class AllegroLlm(LlmProviderAbstract):
    """
    Allegro text-to-video LLM class
    """
    def video_gen(
        self,
        question: str,
        prompt_enhancement_text: str = None
    ) -> dict:
        """
        Perform a Allegro video generation request
        """
        return self.allegro_request_video(question, prompt_enhancement_text)

    def video_gen_followup(
        self,
        request_response: dict,
        wait_time: int = 60
    ):
        """
        Perform a Allegro video generation request check
        """
        return self.allegro_check_video_generation(request_response, wait_time)

    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        return self.query_from_text_model(
            prompt,
            question,
            prompt_enhancement_text,
            unified)

    def allegro_query(self, model_params: dict) -> dict:
        """
        Perform a Allegro video generation request
        """
        response = get_default_resultset()
        headers = {
            "Authorization": f"{model_params.get('api_key')}",
            "User-Agent": "Apifox/1.0.0 (https://apifox.com)",
        }
        headers.update(model_params.get("headers", {}))
        query = model_params.get("query", {})
        payload = model_params.get("payload", {})
        rhymes_endpoint = model_params.get(
            "base_url", "https://api.rhymes.ai/v1/generateVideoSyn")

        query_string = "&".join([f"{key}={value}" for key, value
                                in query.items()])
        if query_string:
            api_url = f"{rhymes_endpoint}?{query_string}"
        else:
            api_url = rhymes_endpoint

        log_debug("allegro_query | " +
                  f"\nAPI URL: {api_url}" +
                  f"\nAPI headers: {headers}" +
                  f"\nAPI payload: {payload}"
                  f"\nAPI method: {model_params.get('method', 'POST')}",
                  debug=DEBUG)
        try:
            if model_params.get("method", "POST") == "POST":
                model_response = requests.post(
                    api_url, headers=headers,
                    json=payload)
            else:
                model_response = requests.get(api_url, headers=headers)
        except Exception as e:
            response['error'] = True
            response['error_message'] = str(e)
            return response

        if model_response.status_code != 200:
            response['error'] = True
            response['error_message'] = \
                "Request failed with status " \
                f"code {model_response.status_code}"
            return response

        log_debug("allegro_query | API response:" +
                  f"\n{model_response.json()}", debug=DEBUG)
        response['response'] = model_response.json()
        return response

    def allegro_request_video(self, question: str,
                              prompt_enhancement_text: str):
        """
        Perform a Allegro video generation request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=None,
            prompt_enhancement_text=prompt_enhancement_text,
        )
        if pam_response['error']:
            return pam_response

        rand_seed = int(time.time())
        model_params = {
            "api_key": os.environ.get("RHYMES_ALLEGRO_API_KEY"),
            "headers": {
                "Content-Type": "application/json",
            },
            "payload": {
                # "refined_prompt": refined_prompt,
                "refined_prompt": pam_response['refined_prompt'],
                "user_prompt": question,
                "num_step": 50,
                "rand_seed": rand_seed,
                "cfg_scale": 7.5,
            }
        }

        log_debug("allegro_request_video | GENERATE VIDEO | " +
                  f"model_params: {model_params}", debug=DEBUG)

        response = self.allegro_query(model_params)
        response['refined_prompt'] = pam_response['refined_prompt']

        log_debug("allegro_request_video | GENERATION RESULT | " +
                  f"response: {response}", debug=DEBUG)

        if response['error']:
            return response

        if (response["response"].get("message") and
           response["response"]['message'] not in RHYMES_SUCCESS_RESPONSES) \
           or not response["response"].get("data"):
            message = response["response"].get("message",
                                               "No message and no data")
            response['error'] = True
            response['error_message'] = message

        return response

    def allegro_check_video_generation(
        self,
        allegro_response: dict,
        wait_time: int = 60
    ):
        """
        Perform a Allegro video generation request check
        """
        request_id = allegro_response["response"]["data"]
        log_debug("allegro_check_video_generation | " +
                  f"request_id: {request_id}", debug=DEBUG)

        model_params = {
            "api_key": os.environ.get("RHYMES_ALLEGRO_API_KEY"),
            "base_url": 'https://api.rhymes.ai/v1/videoQuery',
            "query": {
                "requestId": request_id,
            },
            "method": "GET",
        }
        log_debug("allegro_check_video_generation | WAIT FOR VIDEO | " +
                  f"model_params: {model_params}", debug=DEBUG)

        video_url = None
        for i in range(10):
            log_debug(f"allegro_check_video_generation | VERIFICATION TRY {i}",
                      debug=DEBUG)
            # Send the follow-up request to the Allegro API
            response = self.allegro_query(model_params)
            log_debug(f"allegro_check_video_generation | VERIFICATION {i} | " +
                      f"response: {response}", debug=DEBUG)
            if response['error']:
                response["ttv_followup_response"] = response["error_message"]
                return response
            if response["response"]['message'] in RHYMES_SUCCESS_RESPONSES \
               and response["response"].get('data'):
                if isinstance(response["response"]["data"], str):
                    # Verify if the string has a json content
                    if "{" in response["response"]["data"] and \
                       response["response"]["data"].endswith("}"):
                        # Get the response["response"]["data"] string from the
                        # first "{"
                        first_bracket = response["response"]["data"].find("{")
                        response["response"]["data"] = \
                            json.loads(
                                response["response"]["data"][first_bracket:]
                            )
                if isinstance(response["response"]["data"], str):
                    video_url = response["response"]["data"]
                else:
                    # The response is a dictionary with an error message
                    # E.g. { "code": 503, "type": "InternalServerException",
                    #        "message": "Prediction failed" }
                    response["ttv_followup_response"] = \
                        response["response"]["data"]
                    response["error"] = True
                    response["error_message"] = \
                        f"[E-RH-ALL-100] Video generation failed" \
                        f" (request_id: {request_id}," \
                        f' response: {response["ttv_followup_response"]})'
                break
            time.sleep(wait_time)

        if not video_url:
            response["error"] = True
            response["error_message"] = response.get("error_message") or \
                f"[E-RH-ALL-200] Video generation failed" \
                f" (request_id: {request_id}, response: {response})"

        response['video_url'] = video_url
        return response
</file>

<file path="lib/codegen_ai_provider_together_ai.py">
"""
Together AI API
"""
import os

from together import Together

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
)
from lib.codegen_ai_abstracts import LlmProviderAbstract


DEBUG = False


class TogetherAiLlm(LlmProviderAbstract):
    """
    Together AI LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a Together AI request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response

        model_additional_params = {
            "model": self.model_name or os.environ.get(
                "TOGETHER_AI_MODEL_NAME"),
            "messages": pam_response['messages'],
            "max_tokens": self.params.get("max_tokens"),
            "temperature": self.params.get("temperature", 0.5),
            "top_p": self.params.get("top_p", 0.7),
            "top_k": self.params.get("top_k", 50),
            "stop": ["<|eot_id|>", "<|eom_id|>"],
            "repetition_penalty": self.params.get("repetition_penalty", 1),
            "stream": self.params.get("stream", False),
        }
        if os.environ.get("TOGETHER_AI_SAFETY_MODEL"):
            model_additional_params["safety_model"] = os.environ.get(
                "TOGETHER_AI_SAFETY_MODEL")
        model_params = self.get_model_args(
            additional_params=model_additional_params,
        )

        client_params = self.get_client_args(
            additional_params={
                "api_key": self.api_key or os.environ.get(
                    "TOGETHER_AI_API_KEY"),
            },
        )
        log_debug("together_ai_query | " +
                  f"model_params: {model_params}", debug=DEBUG)
        client = Together(**client_params)
        response_raw = client.chat.completions.create(**model_params)
        response['response'] = response_raw.choices[0].message.content
        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("together_ai_query | " +
                  f"response: {response}", debug=DEBUG)
        return response
</file>

<file path="lib/codegen_ai_provider_xai.py">
"""
X AI (Grok) API
"""
import os

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
)
from lib.codegen_ai_abstracts import LlmProviderAbstract
from lib.codegen_ai_provider_openai import get_openai_api_response


DEBUG = False


class XaiLlm(LlmProviderAbstract):
    """
    X AI (Grok) LLM class
    """
    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Perform a X AI (Grok) request
        """
        response = get_default_resultset()
        pam_response = self.get_prompts_and_messages(
            user_input=question,
            system_prompt=prompt,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        if pam_response['error']:
            return pam_response
        model_params = self.get_model_args(
            additional_params={
                "model": (self.model_name or
                          os.environ.get("XAI_MODEL_NAME")),
                "api_key": (self.api_key or
                            os.environ.get("XAI_API_KEY")),
                "base_url": "https://api.x.ai/v1",
                "messages": pam_response['messages'],
            },
            for_openai_api=True,
        )
        # Get the LLM response
        log_debug("x_grok | " +
                  f"model_params: {model_params}", debug=DEBUG)
        response = get_openai_api_response(model_params)
        response['refined_prompt'] = pam_response['refined_prompt']
        log_debug("x_grok | " +
                  f"response: {response}", debug=DEBUG)
        return response

# -------------

# import os
# from openai import OpenAI

# XAI_API_KEY = os.getenv("XAI_API_KEY")
# client = OpenAI(
#     api_key=XAI_API_KEY,
#     base_url=,
# )

# completion = client.chat.completions.create(
#     model="grok-beta",
#     messages=[
#         {"role": "system", "content": "You are Grok, a chatbot inspired
#  by"abs the Hitchhikers Guide to the Galaxy."},
#         {"role": "user", "content": "What is the meaning of life, the u
# niverse, and everything?"},
#     ],
# )

# print(completion.choices[0].message)
</file>

<file path="lib/codegen_ai_utilities.py">
"""
AI utilities
"""
from lib.codegen_ai_abstracts import LlmProviderAbstract
from lib.codegen_ai_provider_openai import (
    OpenaiLlm,
    OpenaiImageGen,
)
from lib.codegen_utilities import log_debug


DEBUG = False


class LlmProvider(LlmProviderAbstract):
    """
    Abstract class for LLM providers
    """
    def __init__(self, params: str):
        super().__init__(params)
        if self.params.get("provider") == "openai" or \
           self.params.get("provider") == "chat_openai":
            self.llm = OpenaiLlm(self.params)
        elif self.params.get("provider") == "groq":
            from lib.codegen_ai_provider_groq import GroqLlm
            self.llm = GroqLlm(self.params)
        elif self.params.get("provider") == "nvidia":
            from lib.codegen_ai_provider_nvidia import NvidiaLlm
            self.llm = NvidiaLlm(self.params)
        elif self.params.get("provider") == "ollama":
            from lib.codegen_ai_provider_ollama import OllamaLlm
            self.llm = OllamaLlm(self.params)
        elif self.params.get("provider") == "huggingface":
            from lib.codegen_ai_provider_huggingface import HuggingFaceLlm
            self.llm = HuggingFaceLlm(self.params)
        elif self.params.get("provider") == "together_ai":
            from lib.codegen_ai_provider_together_ai import TogetherAiLlm
            self.llm = TogetherAiLlm(self.params)
        elif self.params.get("provider") == "rhymes":
            from lib.codegen_ai_provider_rhymes import AriaLlm
            self.llm = AriaLlm(self.params)
        elif self.params.get("provider") == "xai":
            from lib.codegen_ai_provider_xai import XaiLlm
            self.llm = XaiLlm(self.params)
        elif self.params.get("provider") == "openrouter":
            from lib.codegen_ai_provider_openrouter import OpenRouterLlm
            self.llm = OpenRouterLlm(self.params)
        else:
            raise ValueError(
                f'Invalid LLM provider: {self.params.get("provider")}')
        self.init_llm()

    def query(
        self,
        prompt: str,
        question: str,
        prompt_enhancement_text: str = None,
        unified: bool = False,
    ) -> dict:
        """
        Abstract method for querying the LLM
        """
        unified = unified or self.get_unified_flag()
        log_debug(
            "LLmProvider.query" +
            f"\n| provider: {self.llm.params.get('provider')}" +
            f"\n| model: {self.llm.params.get('model_name')}" +
            f"\n| unified: {unified}" +
            "\n| no_system_prompt_allowed_providers: "
            f"{self.llm.params.get('no_system_prompt_allowed_providers')}" +
            "\n| no_system_prompt_allowed_models: "
            f"{self.llm.params.get('no_system_prompt_allowed_models')}",
            DEBUG
        )
        llm_response = self.llm.query(
            prompt=prompt,
            question=question,
            prompt_enhancement_text=prompt_enhancement_text,
            unified=unified,
        )
        return llm_response


class ImageGenProvider(LlmProviderAbstract):
    """
    Abstract class for text-to-image providers
    """
    def __init__(self, params: str):
        self.params = params
        self.llm = None
        if self.params.get("provider") == "huggingface":
            from lib.codegen_ai_provider_huggingface import HuggingFaceImageGen
            self.llm = HuggingFaceImageGen(self.params)
        elif self.params.get("provider") == "openai":
            self.llm = OpenaiImageGen(self.params)
        else:
            raise ValueError(
                f'Invalid ImageGen provider: {self.params.get("provider")}')
        self.init_llm()

    def query(self, prompt: str, question: str,
              prompt_enhancement_text: str = None) -> dict:
        """
        Perform a LLM query request
        """
        return self.llm.query(
            prompt, question,
            prompt_enhancement_text)

    def image_gen(
        self,
        question: str,
        prompt_enhancement_text: str = None,
        image_extension: str = 'jpg',
    ) -> dict:
        """
        Perform a image generation request
        """
        return self.llm.image_gen(
            question=question,
            prompt_enhancement_text=prompt_enhancement_text,
            image_extension=image_extension)


class TextToVideoProvider(LlmProviderAbstract):
    """
    Abstract class for text-to-video providers
    """
    def __init__(self, params: str):
        self.params = params
        self.llm = None
        if self.params.get("provider") == "rhymes":
            from lib.codegen_ai_provider_rhymes import AllegroLlm
            self.llm = AllegroLlm(self.params)
        elif self.params.get("provider") == "openai":
            raise NotImplementedError
        else:
            raise ValueError(
                f'Invalid TextToVideo provider: {self.params.get("provider")}')
        self.init_llm()

    def query(self, prompt: str, question: str,
              prompt_enhancement_text: str = None) -> dict:
        """
        Perform a LLM query request
        """
        return self.llm.query(
            prompt, question,
            prompt_enhancement_text)

    def video_gen(
        self,
        question: str,
        prompt_enhancement_text: str = None
    ) -> dict:
        """
        Perform a video generation request
        """
        return self.llm.video_gen(question, prompt_enhancement_text)

    def image_gen(
        self,
        question: str,
        prompt_enhancement_text: str = None,
        image_extension: str = 'jpg',
    ) -> dict:
        """
        Perform a image generation request
        """
        return self.llm.image_gen(
            question=question,
            prompt_enhancement_text=prompt_enhancement_text,
            image_extension=image_extension)

    def video_gen_followup(
        self,
        request_response: dict,
        wait_time: int = 60
    ):
        """
        Perform a video generation request check
        """
        return self.llm.video_gen_followup(request_response, wait_time)
</file>

<file path="lib/codegen_app_ideation_lib.py">
"""
App ideation parameters library
"""


DEBUG = False

# app_config = get_app_config()
# cgsl = StreamlitLib(app_config)


def get_features_data():
    """
    Returns the features data: template, mandatory fields
    The features are related with the form buttons
    """
    return {
        "generate_app_ideas": {
            "system_prompt": "generate_app_ideas_system_prompt.txt",
            "template": "generate_app_ideas_user_prompt.txt",
            "mandatory_fields": [
                # "title",
                # "subtitle",
                "application_subject",
                "timeframe",
                "web_or_mobile",
            ],
        },
        "generate_app_names": {
            "system_prompt": "generate_app_names_system_prompt.txt",
            "template": "generate_app_names_user_prompt.txt",
            "mandatory_fields": [
                # "title",
                # "subtitle",
                "application_subject",
                "web_or_mobile",
            ],
        },
        "generate_app_structure": {
            "system_prompt": "generate_app_structure_system_prompt.txt",
            "template": "generate_app_structure_user_prompt.txt",
            "mandatory_fields": [
                "title",
                # "subtitle",
                "application_subject",
                "web_or_mobile",
            ],
        },
        "generate_presentation": {
            "system_prompt": "generate_app_presentation_system_prompt.txt",
            "template": "generate_app_presentation_user_prompt.txt",
            "mandatory_fields": [
                "title",
                "subtitle",
                "application_subject",
                "web_or_mobile",
                "problem_statement",
                "objective",
                "technologies_used",
                "application_features",
                "how_it_works",
                # "screenshots",
                "benefits",
                "feedback_and_future_development",
                "future_vision",
            ],
        }
    }


def get_features_data_from_prompt():
    """
    Returns the features data: template, mandatory fields
    The features are related with the from-prompt form buttons
    """
    return {
        "generate_app_ideas_from_prompt": {
            "system_prompt":
                "generate_app_ideas_system_prompt.txt",
            "template": "generate_app_ideas_from_question_user_prompt.txt",
            "mandatory_fields": [
                "question",
            ],
        },
        "generate_app_names_from_prompt": {
            "system_prompt":
                "generate_app_names_system_prompt.txt",
            "template": "generate_app_names_from_question_user_prompt.txt",
            "mandatory_fields": [
                "question",
            ],
        },
        "generate_app_structure_from_prompt": {
            "system_prompt":
                "generate_app_structure_system_prompt.txt",
            "template": "generate_app_structure_from_question_user_prompt.txt",
            "mandatory_fields": [
                "question",
            ],
        },
        "generate_presentation_from_prompt": {
            "system_prompt":
                "generate_app_presentation_system_prompt.txt",
            "template":
                "generate_app_presentation_from_question_user_prompt.txt",
            "mandatory_fields": [
                "question",
            ],
        }
    }


def get_fields_data():
    """
    Returns the fields data: description, type, length
    """
    fields_data = {
        "title": {
            "title": "Application name",
            "help": "If you don't have a name, you can use "
                    "[Generate App Names] button to generate one.",
            "type": "text",
        },
        "subtitle": {
            "title": "Subtitle",
            "help": "Super short description for a sub-title",
            "type": "text",
        },
        "application_subject": {
            "title": "Summary",
            "help": "Describe the application in a brief summary.",
        },
        "web_or_mobile": {
            "title": "Application type",
            "help": "",
            "type": "selectbox",
            "options": ["", "Web", "Mobile", "Web and Mobile"],
        },
        "timeframe": {
            "title": "Time frame",
            "help": "Available time to develop the application"
                     "(e.g. 2-3 days, 1 moth, etc.)",
            "type": "text",
        },
        "problem_statement": {
            "title": "Problem Statement",
            "help": "Describe the problem the application addresses. Explain"
                    " why the problem is significant and how the audience"
                    " might relate to it.",
        },
        "objective": {
            "title": "Objective",
            "help": "Explain the application's main goal. Emphasize how the"
                    " objective aligns with solving the problem introduced"
                    " earlier.",
        },
        "technologies_used": {
            "title": "Technologies Used",
            "help": "Mention programming languages, AI models, API providers"
                    "/platforms, etc. Break down which technologies were used"
                    " and why they were chosen for this project.",
        },
        "application_features": {
            "title": "Application Features",
            "help": "Summary of the main features. Provide detailed"
                    " explanations or anecdotes for each feature's utility.",
        },
        "how_it_works": {
            "title": "How It Works",
            "help": "Describe the workflow from user input to API integration"
                    " and outputs. Detail the process flow, ensuring the"
                    " audience understands how each element contributes to the"
                    " functionality.",
        },
        "screenshots": {
            "title": "Screenshots",
            "help": "Include screenshots of the application showcasing key"
                    " parts. Describe each screenshot's relevance, pointing"
                    " out important functionalities.",
            "enabled": False,
        },
        "benefits": {
            "title": "Benefits",
            "help": "Include the main benefits of the application. Persuade"
                    " the audience why these benefits help address the user's"
                    " problem effectively.",
        },
        "feedback_and_future_development": {
            "title": "Feedback and Future Development",
            "help": "Notable positive aspects and potential improvements."
                    " Emphasize user satisfaction and iterate potential"
                    " evolution based on feedback.",
        },
        "future_vision": {
            "title": "Future Vision",
            "help": "Describe possible enhancements like expanding"
                    " capabilities to more complex tasks, and how it can"
                    " evolve to embrace users\' needs. Include possible"
                    " use cases (e.g., training, education, real-time"
                    " support).",
        },
    }
    return fields_data


def get_fields_data_from_prompt():
    """
    Returns the fields data for ideation from question (prompt)
    """
    fields_data = {
        "question": {
            "title": "Question / Prompt",
            "help": "App subject or question to generate the app idea",
            "type": "text",
        },
    }
    return fields_data


def get_buttons_config():
    """
    Returns the buttons configuration
    """
    buttons_config = [
        {
            "text": "Generate App Ideas",
            "key": "generate_app_ideas",
            "enable_config_name": "GENERATE_APP_IDEAS_ENABLED",
            "type": "submit",
        },
        {
            "text": "Generate App Names",
            "key": "generate_app_names",
            "enable_config_name": "GENERATE_APP_NAMES_ENABLED",
            "type": "submit",
        },
        {
            "text": "Generate App Structure",
            "key": "generate_app_structure",
            "enable_config_name": "GENERATE_APP_STRUCTURE_ENABLED",
            "type": "submit",
        },
        {
            "text": "Generate Presentation",
            "key": "generate_presentation",
            "enable_config_name": "GENERATE_PRESENTATION_ENABLED",
            "type": "submit",
        },
        {
            "text": "Generate Video Script",
            "key": "generate_video_script",
            "enable_config_name": "GENERATE_VIDEO_SCRIPT_ENABLED",
            "type": "submit",
        },
    ]
    return buttons_config


def get_buttons_config_for_prompt():
    """
    Returns the buttons configuration for ideation from question (prompt)
    """
    buttons_config = [
        {
            "text": "Generate App Ideas",
            "key": "generate_app_ideas_from_prompt",
            "enable_config_name": "GENERATE_APP_IDEAS_ENABLED",
        },
        {
            "text": "Generate App Names",
            "key": "generate_app_names_from_prompt",
            "enable_config_name": "GENERATE_APP_NAMES_ENABLED",
        },
        {
            "text": "Generate App Structure",
            "key": "generate_app_structure_from_prompt",
            "enable_config_name": "GENERATE_APP_STRUCTURE_ENABLED",
        },
        {
            "text": "Generate Presentation",
            "key": "generate_presentation_from_prompt",
            "enable_config_name": "GENERATE_PRESENTATION_ENABLED",
        },
        {
            "text": "Generate Video Script",
            "key": "generate_video_script_from_prompt",
            "enable_config_name": "GENERATE_VIDEO_SCRIPT_ENABLED",
        },
        # get_response_as_prompt_button_config(
        #     "use_response_as_prompt_app_ideation_tab"),
        # get_prompt_enhancement_button_config(
        #     "prompt_enhancement_app_ideation_tab"),
    ]
    return buttons_config


def get_ideation_form_config():
    """
    Returns the ideation form configuration
    """
    form_config = {
        "title": "Application Ideation Form",
        "name": "application_form",
        "subtitle": "This form will help you generate the initial plan for the"
                    " application idea. Please fill in the following fields:",
        "suffix": "When you are ready, click one of the buttons below to"
                  " generate the application idea.",
        "fields": get_fields_data(),
        "buttons_config": get_buttons_config(),
        "features_data": get_features_data(),
        "form_session_state_key": "application_form_data",
        # "buttons_function": add_buttons_for_app_ideation_tab,
    }
    return form_config


def get_ideation_from_prompt_config():
    """
    Returns the ideation from prompt configuration
    """
    form_config = {
        "title": "Application Ideation from Prompt",
        "name": "application_form_data_from_prompt",
        "subtitle": "This option will help you generate application idea from"
                    " the Question / Prompt",
        "suffix": "When you are ready, click one of the buttons below to"
                  " generate the application idea.",
        "fields": get_fields_data_from_prompt(),
        "buttons_config": get_buttons_config_for_prompt(),
        "features_data": get_features_data_from_prompt(),
        "form_session_state_key": "application_form_data_from_prompt",
        # "buttons_function": add_buttons_for_app_ideation_tab,
    }
    return form_config
</file>

<file path="lib/codegen_db_abstracts.py">
"""
Generic database abstracts
"""
from typing import List, Dict, Union
import os
import json

from lib.codegen_utilities import (
    get_new_item_id,
    get_default_resultset,
)


class DatabaseAbstract:
    """
    Database abstract class
    """
    def __init__(self, db_type, other_data=None):
        """
        Initialize the appropriate database based on db_type
        """
        if other_data is None:
            other_data = {}
        self.other_data = other_data
        self.db = None
        self.db_type = db_type

    def save_item(self, item_data: dict, id: str = None):
        """
        Save the item in the database
        """
        raise NotImplementedError

    def get_list(self, sort_attr: str = None, sort_order: str = "desc"):
        """
        Returns the items in the database
        """
        raise NotImplementedError

    def get_item(self, id: str):
        """
        Returns the item in the database
        """
        raise NotImplementedError

    def delete_item(self, id: str):
        """
        Delete an item from the database
        """
        raise NotImplementedError

    def import_data(self, data: Union[List[Dict], Dict]):
        """
        Import data into the database
        """
        response = get_default_resultset()
        # If data is a list of dictionaries, convert it to a dictionary
        if isinstance(data, list):
            data = {item.get('id', get_new_item_id()): item for item in data}
        for id, item_data in data.items():
            self.save_item(item_data, id)
        response['result'] = f"Imported {len(data)} items"
        return response

    def export_data(self) -> str:
        """
        Export data from the database to JSON
        """
        response = get_default_resultset()
        items = self.get_list()
        # Convert id to str
        for item in items:
            item['id'] = str(item['id'])
        response['json'] = json.dumps(items, indent=4)
        response['result'] = f"Emported {len(items)} items"
        return response

    def import_data_from_file(self, file_path: str = None):
        """
        Import data from a JSON file into the database
        """
        response = get_default_resultset()
        if not file_path:
            response['error'] = True
            response['error_message'] = "file_path is required for " \
                                        "import_data_from_file"
        elif not os.path.exists(file_path):
            response['error'] = True
            response['error_message'] = f"File not found: {file_path}"
        if response['error']:
            return response
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
        except Exception as e:
            response['error'] = True
            response['error_message'] = str(e)
            return response
        response = self.import_data(data)
        response['file_path'] = file_path
        return response

    def export_data_to_file(self, file_path: str = None,
                            overwrite: bool = False):
        """
        Export data from the database to a JSON file
        """
        response = get_default_resultset()
        if not file_path:
            response['error'] = True
            response['error_message'] = "file_path is required for " \
                                        "export_data_to_file"
        elif os.path.exists(file_path) and not overwrite:
            response['error'] = True
            response['error_message'] = f"File found: {file_path}"
        if response['error']:
            return response
        response = self.export_data()
        if response['error']:
            return response
        with open(file_path, 'w') as f:
            f.write(response['json'])
        response['file_path'] = file_path
        return response
</file>

<file path="lib/codegen_db_json.py">
"""
JSON file database
"""
import os
import json
import uuid

from lib.codegen_db_abstracts import DatabaseAbstract


class JsonFileDatabase(DatabaseAbstract):
    """
    JSON file database class
    """
    def __init__(self, db_path):
        self.db_path = db_path
        self.init_db()

    def init_db(self):
        """
        Initialize the JSON file database
        """
        if not os.path.exists(self.db_path):
            with open(self.db_path, 'w') as f:
                json.dump({}, f)

        with open(self.db_path) as f:
            json_db = json.load(f)

        return json_db

    def save_item(self, item_data: dict, id: str = None):
        """
        Save the item in the database
        """
        if not id:
            id = str(uuid.uuid4())
        json_db = self.init_db()
        json_db[id] = dict(item_data)
        with open(self.db_path, 'w') as f:
            json.dump(json_db, f)
        return id

    def get_list(self, sort_attr: str = None, sort_order: str = "desc"):
        """
        Returns the items in the database
        """
        json_db = self.init_db()
        items = []
        for id, item in json_db.items():
            item_to_append = item.copy()
            item_to_append['id'] = id
            items.append(item_to_append)
        if sort_attr:
            items = sorted(items, key=lambda x: x[sort_attr],
                           reverse=sort_order == "desc")
        return items

    def get_item(self, id: str):
        """
        Returns the item in the database
        """
        json_db = self.init_db()
        if id in json_db:
            item = json_db[id]
            item['id'] = id
            return item
        return None

    def delete_item(self, id: str):
        """
        Delete a item from the database
        """
        json_db = self.init_db()
        if id in json_db:
            del json_db[id]
            with open(self.db_path, 'w') as f:
                json.dump(json_db, f)
</file>

<file path="lib/codegen_db_mongodb.py">
"""
MongoDB database
"""
import uuid
import os

from pymongo import MongoClient

from lib.codegen_db_abstracts import DatabaseAbstract


class MongoDBDatabase(DatabaseAbstract):
    """
    MongoDB database class
    """
    def __init__(self, uri, db_name, collection_name):
        self.client = MongoClient(uri)
        self.db = self.client[db_name]
        self.collection = self.db[collection_name]

    def save_item(self, item_data: dict, id: str = None):
        """
        Save the item in the MongoDB collection
        """
        if not id:
            id = str(uuid.uuid4())
        item_data['_id'] = id
        self.collection.replace_one({'_id': id}, item_data, upsert=True)
        return id

    def get_list(self, sort_attr: str = None, sort_order: str = "desc"):
        """
        Returns the items in the MongoDB collection
        """
        sort_order = -1 if sort_order == "desc" else 1
        if sort_attr:
            items = list(self.collection.find().sort(sort_attr, sort_order))
        else:
            items = list(self.collection.find())
        # Assign id from _id field
        for item in items:
            item['id'] = str(item['_id'])  # Convert ObjectId to str
        return items

    def get_item(self, id: str):
        """
        Returns the item from the MongoDB collection
        """
        item = self.collection.find_one({'_id': id})
        if item:
            item['id'] = str(item['_id'])  # Convert ObjectId to str
            return item
        return None

    def delete_item(self, id: str):
        """
        Delete an item from the MongoDB collection
        """
        self.collection.delete_one({'_id': id})

    def import_data_from_file(self, file_path: str = None):
        """
        Import data from a JSON file into the database
        """
        if not file_path:
            file_path = os.environ.get('JSON_DB_PATH')
        return super().import_data_from_file(file_path)

    def export_data_to_file(self, file_path: str = None,
                            overwrite: bool = False):
        """
        Export data from the database to a JSON file
        """
        if not file_path:
            file_path = os.environ.get('JSON_DB_PATH')
        return super().export_data_to_file(file_path, overwrite)
</file>

<file path="lib/codegen_db.py">
"""
Generic database
"""
from lib.codegen_db_abstracts import DatabaseAbstract
from lib.codegen_db_json import JsonFileDatabase
from lib.codegen_db_mongodb import MongoDBDatabase
# from lib.codegen_utilities import log_debug


DEBUG = False


class CodegenDatabase(DatabaseAbstract):
    """
    Generic database class
    """
    def __init__(self, db_type, other_data=None):
        """
        Initialize the appropriate database based on db_type
        """
        if other_data is None:
            other_data = {}
        self.other_data = other_data
        self.db = None
        self.db_type = db_type
        if db_type == 'json':
            db_path = self.other_data.get('JSON_DB_PATH')
            if not db_path:
                raise ValueError("Invalid JSON_DB_PATH in other_data")
            self.db = JsonFileDatabase(db_path)
        elif db_type == 'mongodb':
            uri = self.other_data.get('MONGODB_URI')
            db_name = self.other_data.get('MONGODB_DB_NAME')
            collection_name = self.other_data.get('MONGODB_COLLECTION_NAME')
            if not uri or not db_name or not collection_name:
                raise ValueError("Invalid MONGODB_URI, MONGODB_DB_NAME or "
                                 "MONGODB_COLLECTION_NAME in other_data")
            # log_debug(f"CodegenDatabase | "
            #           f"uri: {uri} | db_name: {db_name} | "
            #           f"collection_name: {collection_name}",
            #           debug=DEBUG)
            self.db = MongoDBDatabase(uri, db_name, collection_name)
        else:
            raise ValueError("Invalid db_type. Must be 'json' or 'mongodb'")

    def save_item(self, item_data: dict, id: str = None):
        """
        Save the item in the database
        """
        return self.db.save_item(item_data, id)

    def get_list(self, sort_attr: str = None, sort_order: str = "desc"):
        """
        Returns the items in the database
        """
        return self.db.get_list(sort_attr, sort_order)

    def get_item(self, id: str):
        """
        Returns the item in the database
        """
        return self.db.get_item(id)

    def delete_item(self, id: str):
        """
        Delete an item from the database
        """
        return self.db.delete_item(id)


# Example usage:
# db = CodegenDatabase("json")
# db.save_item({"name": "Item 1", "value": 100})
# item = db.get_item("some_id")
# db.delete_item("some_id")
</file>

<file path="lib/codegen_general_lib.py">
"""
Streamlit UI library
"""
from typing import Any
import os
# import time
import json
import uuid

from lib.codegen_utilities import (
    log_debug,
    # get_date_time,
    get_new_item_id,
    get_default_resultset,
    error_resultset,
    read_file,
)
# from lib.codegen_db import CodegenDatabase
from lib.codegen_ai_utilities import (
    TextToVideoProvider,
    LlmProvider,
    ImageGenProvider,
)
from lib.codegen_powerpoint import PowerPointGenerator


DEBUG = False


class GeneralLib:
    """
    General utilities class
    """
    def __init__(self, params: dict, session_state: dict = None):
        self.params = dict(params)
        self.session_state = session_state or {
            "model_config_par_temperature": 0.5,
            "model_config_par_max_tokens": 2048,
            "model_config_par_top_p": 1.0,
            "model_config_par_frequency_penalty": 0.0,
            "model_config_par_presence_penalty": 0.0,
        }

    # Conversations database

    # def init_db(self):
    #     """
    #     Initialize the JSON file database
    #     """
    #     db_type = os.getenv('DB_TYPE')
    #     db = None
    #     if db_type == 'json':
    #         db = CodegenDatabase("json", {
    #             "JSON_DB_PATH": os.getenv(
    #                 'JSON_DB_PATH',
    #                 self.get_par_value("CONVERSATION_DB_PATH")
    #             ),
    #         })
    #     if db_type == 'mongodb':
    #         db = CodegenDatabase("mongodb", {
    #             "MONGODB_URI": os.getenv('MONGODB_URI'),
    #             "MONGODB_DB_NAME": os.getenv('MONGODB_DB_NAME'),
    #             "MONGODB_COLLECTION_NAME": 
    #                  os.getenv('MONGODB_COLLECTION_NAME')
    #         })
    #     if not db:
    #         raise ValueError(f"Invalid DB_TYPE: {db_type}")
    #     return db

    # def update_conversation(
    #     self,
    #     item: dict = None,
    #     id: str = None
    # ):
    #     db = self.init_db()
    #     log_debug(f"UPDATE_CONVERSATION | id: {id} | item: {item}",
    #               debug=DEBUG)
    #     db.save_item(item, id)
    #     self.set_new_id(id)

    # def save_conversation(
    #     self, type: str,
    #     question: str,
    #     answer: str,
    #     title: str = None,
    #     refined_prompt: str = None,
    #     other_data: dict = None,
    #     id: str = None
    # ):
    #     """
    #     Save the conversation in the database
    #     """
    #     if not id:
    #         id = get_new_item_id()
    #     if not title:
    #         title = self.generate_title_from_question(question)
    #         title = title[:self.get_title_max_length()]
    #     db = self.init_db()
    #     item = {
    #         "type": type,
    #         "title": title,
    #         "question": question,
    #         "answer": answer,
    #         "refined_prompt": refined_prompt,
    #         "timestamp": time.time(),
    #     }
    #     if not other_data:
    #         other_data = {}
    #     item.update(other_data)
    #     db.save_item(item, id)
    #     self.update_conversations()
    #     self.recycle_suggestions()
    #     self.set_new_id(id)
    #     return id

    # def get_conversations(self):
    #     """
    #     Returns the conversations in the database
    #     """
    #     db = self.init_db()
    #     conversations = db.get_list("timestamp", "desc")
    #     # Add the date_time field to each conversation
    #     for conversation in conversations:
    #         conversation['date_time'] = get_date_time(
    #             conversation['timestamp'])
    #     return conversations

    # def get_conversation(self, id: str):
    #     """
    #     Returns the conversation in the database
    #     """
    #     db = self.init_db()
    #     conversation = db.get_item(id)
    #     if conversation:
    #         # Add the date_time field to the conversation
    #         conversation['date_time'] = get_date_time(
    #             conversation['timestamp'])
    #         return conversation
    #     return None

    # def delete_conversation(self, id: str):
    #     """
    #     Delete a conversation from the database
    #     """
    #     db = self.init_db()
    #     db.delete_item(id)
    #     self.update_conversations()

    # Input management

    def validate_question(self, question: str, assign_global: bool = True):
        """
        Validate the question
        """
        if not question:
            return error_resultset(
                error_message="Please enter a question / prompt",
                message_code="GL-VQ-E010"
            )
        # Update the user input in the conversation
        if assign_global:
            self.session_state["question"] = question
        return True

    # Prompt suggestions

    def get_suggestions_from_ai(self, system_prompt: str, user_prompt: str
                                ) -> dict:
        """
        Get suggestions from the AI
        """
        # The model replacement for suggestions is to avoid use reasoning
        # models like o1-preview/o1-mini because they are expensive and
        # slow, and replace them with a less expensive model like GPT-4o-mini.
        model_replacement = self.get_par_value("SUGGESTIONS_MODEL_REPLACEMENT")
        llm_text_model = self.get_llm_text_model(model_replacement)
        if llm_text_model['error']:
            log_debug("get_suggestions_from_ai | llm_text_model "
                      f"ERROR: {llm_text_model}", debug=DEBUG)
            return llm_text_model
        # Get the model class
        llm_model = llm_text_model['class']
        # Get the suggestions from the AI
        llm_response = llm_model.query(system_prompt, user_prompt)
        log_debug("get_suggestions_from_ai | " +
                  f"response: {llm_response}", debug=DEBUG)
        if llm_response['error']:
            log_debug("get_suggestions_from_ai | llm_response "
                      f"ERROR: {llm_response}", debug=DEBUG)
            return llm_response
        # Clean the suggestions response
        suggestions = llm_response['response']
        suggestions = suggestions.replace("\n", "")
        suggestions = suggestions.replace("\r", "")
        suggestions = suggestions.replace("Suggestions:", "")
        suggestions = suggestions.strip()
        suggestions = suggestions.replace('```json', '')
        suggestions = suggestions.replace('```', '')
        suggestions = suggestions.replace("\\'", '')
        # Load the suggestions
        try:
            suggestions = json.loads(suggestions)
            log_debug("get_suggestions_from_ai | FINAL suggestions:"
                      f" {suggestions}", debug=DEBUG)
        except Exception as e:
            log_debug(f"get_suggestions_from_ai | ERROR {e}", debug=DEBUG)
            return self.get_par_value("DEFAULT_SUGGESTIONS")
        return suggestions

    def show_one_suggestion(self, suggestion: Any):
        """
        Show one suggestion in the main section
        """
        response = ""
        if suggestion:
            if isinstance(suggestion, dict):
                if "title" in suggestion:
                    response += suggestion.get("title") + "\n"
                if "description" in suggestion:
                    response += suggestion.get("description")
            else:
                response = suggestion
        if not response:
            response = "N/A"
        return response

    # Conversation titles

    def get_title_max_length(self):
        return self.get_par_value("CONVERSATION_TITLE_LENGTH", 100)

    def get_title_from_question(self, question: str) -> str:
        """
        Returns the title from the question
        """
        title = question
        title = title.replace("```json", "")
        title = title.replace("```", "")
        title = title.replace("\t", " ")
        title = title.replace("\n", " ")
        title = title.replace("\r", " ")
        title = title.strip()
        return title

    def get_conversation_title(self, conversation: dict):
        return conversation.get(
            "title",
            self.get_title_from_question(conversation['question'])
        )

    def generate_title_from_question(self, question: str) -> str:
        """
        Returns the title from the question
        """
        default_title = self.get_title_from_question(question)
        title_length = self.get_title_max_length()
        # Use small models for title generation
        model_replacement = self.get_par_value("SUGGESTIONS_MODEL_REPLACEMENT")
        llm_text_model = self.get_llm_text_model(model_replacement)
        if llm_text_model['error']:
            log_debug("generate_title_from_question | llm_text_model "
                      f"ERROR: {llm_text_model}", debug=DEBUG)
            return default_title
        llm_model = llm_text_model['class']
        # Prepare the prompt
        prompt = "Give me a title for this question " \
                 f"(max length: {title_length*2}): {question}"
        # Get the title from the AI
        llm_response = llm_model.query(prompt, "", unified=True)
        log_debug("GENERATE_TITLE_FROM_QUESTION | " +
                  f"response: {llm_response}", debug=DEBUG)
        if llm_response['error']:
            log_debug("generate_title_from_question | llm_response "
                      f"ERROR: {llm_response}", debug=DEBUG)
            return default_title
        title = llm_response['response']
        return title

    # Data management

    def format_results(self, results: list):
        return "\n*".join(results)

    # UI

    def show_button_of_type(self, button_config: dict, extra_kwargs: dict,
                            container: Any):
        """
        Show a button based on the button_config
        Args:
            button_config (dict): button configuration
                {
                    "text": "Answer Question",
                    "key": "generate_text",
                    "enable_config_name": "GENERATE_TEXT_ENABLED",
                    "type": "checkbox",
                }
        """
        submitted = None
        button_type = button_config.get("type", "button")
        if button_type == "checkbox":
            submitted = container.checkbox(
                button_config["text"],
                key=button_config["key"],
                **extra_kwargs)
        elif button_type == "spacer":
            container.write(button_config.get("text", ""))
        elif button_type == "submit":
            submitted = container.form_submit_button(
                button_config["text"])
        else:
            # Defaults to button
            submitted = container.button(
                button_config["text"],
                key=button_config["key"],
                **extra_kwargs)
        return submitted

    def get_buttons_submitted_data(self, buttons_submitted: list,
                                   buttons_data: dict,
                                   submit_button_verification: bool = True):
        """
        Reduce the list of buttons submitted to a single boolean value
        to determine if the form was submitted
        """
        submitted = any(buttons_submitted)

        # log_debug(f"buttons_submitted: {buttons_submitted}", debug=DEBUG)

        buttons_submitted_data = {}
        if submitted:
            # Get the button submitted values
            # and create a dictionary with the form data

            curr_item = 0
            for i in range(len(buttons_data)):
                if not submit_button_verification or \
                   buttons_data[i].get("type") == "submit":
                    if buttons_data[i].get("enable_config_name", None):
                        if self.get_par_value(
                                buttons_data[i]["enable_config_name"], True):
                            # log_debug(f"buttons_data[{i}]: {buttons_data[i]}"
                            #     f" -> {buttons_submitted[curr_item]}",
                            buttons_submitted_data[buttons_data[i]["key"]] = \
                                buttons_submitted[curr_item]
                            curr_item += 1
                    else:
                        # log_debug(f"buttons_data[{i}]: {buttons_data[i]} "
                        #     f"-> {buttons_submitted[curr_item]}",
                        #     debug=DEBUG)
                        buttons_submitted_data[buttons_data[i]["key"]] = \
                            buttons_submitted[curr_item]
                        curr_item += 1
        return buttons_submitted_data

    def get_option_index(self, options: list, value: str):
        """
        Returns the index of the option in the list
        """
        for i, option in enumerate(options):
            if option == value:
                return i
        return 0

    def get_selected_feature(self, form: dict, features_data: dict):
        """
        Returns the selected feature
        """
        log_debug(f"get_selected_feature | form: {form}", debug=DEBUG)
        log_debug(f"get_selected_feature | features_data: {features_data}",
                  debug=DEBUG)
        selected_feature = None
        for key in form.get("buttons_submitted_data"):
            for feature in features_data:
                if form["buttons_submitted_data"].get(key) and \
                        key == feature:
                    selected_feature = feature
                    break
        return selected_feature

    def get_form_name(self, form_config: dict):
        """
        Returns the form session state key
        """
        form_name = form_config.get("name", "application_form")
        return f"{form_name}"

    def get_form_session_state_key(self, form_config: dict):
        """
        Returns the form session state key
        """
        form_name = self.get_form_name(form_config)
        form_session_state_key = form_config.get(
            "form_session_state_key",
            f"{form_name}_data")
        return form_session_state_key

    # PPTX generation

    def create_pptx(self, conversation: dict):
        """
        Generates the PowerPoint slides
        """
        log_debug("CREATE_PPTX | enters...", debug=DEBUG)
        pptx_generator = PowerPointGenerator({
            "output_dir": self.params.get("output_dir", "./output"),
            "file_name": uuid.uuid4(),
        })
        answer = conversation.get("answer")
        if not answer:
            error_message = "Conversation answer is empty"
            log_debug(f"CREATE_PPTX | ERROR 1: {error_message}...",
                      debug=DEBUG)
            return error_resultset(
                error_message=error_message,
                message_code="GL-CPW-E010")
        if "```json" in answer:
            # Find the first occurrence of ```json, cut the text before it,
            # and remove the ```json and ``` characters
            answer = answer.split("```json")[1].replace("```json", "")
            answer = answer.replace("```", "")
        try:
            log_debug("CREATE_PPTX | answer:"
                      f" {answer}", debug=DEBUG)
            slides_config = json.loads(answer)
            log_debug("CREATE_PPTX | slides_config:"
                      f" {slides_config}", debug=DEBUG)
        except Exception as e:
            error_message = f"ERROR {e}"
            log_debug(f"CREATE_PPTX | ERROR 2: {error_message}...",
                      debug=DEBUG)
            return error_resultset(
                error_message=error_message,
                message_code="GL-CPW-E020")

        log_debug("CREATE_PPTX | creating presentation...", debug=DEBUG)

        result_file_path = pptx_generator.generate(slides_config)

        log_debug("CREATE_PPTX | result_file_path: "
                  f"{result_file_path}", debug=DEBUG)

        result = get_default_resultset()
        result["presentation_file_path"] = result_file_path
        return result

    # AI

    def get_available_ai_providers(
        self,
        param_name: str,
        param_values: dict = None
    ) -> list:
        """
        Returns the available LLM providers based on the environment variables
        The model will be available if all its variables are set
        """
        if not param_values:
            param_values = os.environ
        result = []
        log_debug(f"get_available_ai_providers | param_name: {param_name}",
                  debug=DEBUG)
        for model_name, model_attr in self.get_par_value(param_name).items():
            # log_debug(f"get_available_ai_providers | "
            #           '\nmodel_attr.get("active", True): '
            #           f'{model_attr.get("active", True)}'
            #           f"\nmodel_name: {model_name} | "
            #           f"\nmodel_attr: {model_attr}",
            #            debug=DEBUG)
            if not model_attr.get("active", True):
                continue
            model_to_add = model_name
            requirements = model_attr.get("requirements", [])
            for var_name in requirements:
                if not param_values.get(var_name):
                    model_to_add = None
                    break
            if model_to_add:
                result.append(model_to_add)
        log_debug(f"get_available_ai_providers | result: {result}",
                  debug=DEBUG)
        return result

    def get_llm_provider(
        self,
        param_name: str,
        session_state_key: str
    ):
        """
        Returns the LLM provider
        """
        default_llm_provider = self.get_par_value("DEFAULT_LLM_PROVIDER")
        if default_llm_provider:
            return default_llm_provider
        provider_list = self.get_available_ai_providers(param_name)
        if not provider_list:
            return ''
        return provider_list[0]

    def get_llm_model(
        self,
        parent_param_name: str,
        parent_session_state_key: str,
        param_name: str,
        session_state_key: str
    ):
        """
        Returns the LLM model
        """
        llm_provider = self.get_llm_provider(
            parent_param_name, parent_session_state_key)
        if not llm_provider:
            return None
        llm_models = self.get_par_value(
            param_name).get(llm_provider, [])
        if not llm_models:
            return None
        return llm_models[0]

    def get_model_options(
        self,
        parent_param_name: str,
        parent_session_state_key: str,
        param_name: str,
    ):
        """
        Returns the model options for the LLM call
        """
        llm_provider = self.get_llm_provider(
            parent_param_name, parent_session_state_key)
        if not llm_provider:
            return []
        return self.get_par_value(param_name, {}).get(llm_provider, [])

    def get_llm_provider_index(
        self,
        param_name: str,
        session_state_key: str
    ):
        available_llm_providers = self.get_available_ai_providers(param_name)
        try:
            llm_provider_index = available_llm_providers.index(
                self.get_llm_provider(
                    param_name,
                    session_state_key
                ))
        except ValueError:
            llm_provider_index = 0
        return llm_provider_index

    def get_llm_model_index(
        self,
        parent_param_name: str,
        parent_session_state_key: str,
        param_name: str,
        session_state_key: str
    ):
        # log_debug(
        #   f">> get_llm_model_index:"
        #   f"\n | parent_param_name: {parent_param_name}"
        #   f"\n | parent_session_state_key: {parent_session_state_key}"
        #   f"\n | param_name: {param_name}"
        #   f"\n | session_state_key: {session_state_key}",
        #   debug=DEBUG)
        available_llm_models = self.get_model_options(
            parent_param_name,
            parent_session_state_key,
            param_name
        )
        selected_llm_model = self.get_llm_model(
            parent_param_name,
            parent_session_state_key,
            param_name,
            session_state_key
        )
        # log_debug(f">> get_llm_model_index: "
        #           "\n | available_llm_models: "
        #           f"{available_llm_models}"
        #           "\n | selected_llm_model: "
        #           f"{selected_llm_model}", debug=DEBUG)
        try:
            llm_model_index = available_llm_models.index(
                selected_llm_model)
        except ValueError:
            llm_model_index = 0
        # log_debug(f">> get_llm_model_index | llm_model_index: "
        #           f"{llm_model_index}", debug=DEBUG)
        return llm_model_index

    def get_model_configurations(self):
        """
        Returns the model configurations
        """
        model_configurations = {}
        for key in self.session_state:
            if key.startswith("model_config_par_"):
                par_name = key.replace("model_config_par_", "")
                model_configurations[par_name] = self.session_state[key]
        return model_configurations

    def get_llm_text_model(self, model_replacement: dict = None):
        """
        Returns the LLM text model
        """
        llm_parameters = {
            "llm_providers_complete_list":
                # self.get_par_value("LLM_PROVIDERS_COMPLETE_LIST"),
                self.get_par_value("LLM_PROVIDERS", {}).keys(),
            "no_system_prompt_allowed_providers":
                self.get_par_value("NO_SYSTEM_PROMPT_ALLOWED_PROVIDERS"),
            "no_system_prompt_allowed_models":
                self.get_par_value("NO_SYSTEM_PROMPT_ALLOWED_MODELS"),
            "llm_model_forced_values":
                self.get_par_value("LLM_MODEL_FORCED_VALUES"),
            "llm_model_params_naming":
                self.get_par_value("LLM_MODEL_PARAMS_NAMING"),
        }
        log_debug("GET_LLM_TEXT_MODEL | llm_parameters # 1: "
                  f"{llm_parameters}", debug=DEBUG)

        llm_parameters.update(self.get_model_configurations())
        log_debug("GET_LLM_TEXT_MODEL | llm_parameters # 2: "
                  f"{llm_parameters}", debug=DEBUG)

        result = get_default_resultset()
        result["llm_provider"] = self.get_llm_provider(
            "LLM_PROVIDERS",
            "llm_provider"
        )
        result["llm_model"] = self.get_llm_model(
            "LLM_PROVIDERS", "llm_provider",
            "LLM_AVAILABLE_MODELS", "llm_model"
        )
        if not result["llm_provider"]:
            result["error"] = True
            result["error_message"] = "LLM Provider not selected"
        elif not result["llm_model"]:
            result["error"] = True
            result["error_message"] = "LLM Model not selected"
        else:
            if model_replacement:
                # To avoid use the OpenAI reasoning models in the suggestions
                result["llm_model"] = model_replacement.get(
                    result["llm_model"], result["llm_model"])
            # The llm parameters will be available in the LLM class
            llm_parameters["provider"] = result["llm_provider"]
            llm_parameters["model_name"] = result["llm_model"]
            result["class"] = LlmProvider(llm_parameters)
        return result

    def get_prompt_enhancement_flag(self):
        """
        Get prompt enhancement flag condition
        """
        if "prompt_enhancement_flag" in self.session_state:
            return self.session_state["prompt_enhancement_flag"]
        return os.environ.get("PROMPT_ENHANCEMENT_FLAG", '0') == '1'

    def text_generation(
        self,
        question: str = None,
        other_data: dict = None,
        settings: dict = None
    ):
        if not other_data:
            other_data = {}
        if not settings:
            settings = {}
        if not question:
            question = self.session_state.get("question")
        if not self.validate_question(question, settings.get("assign_global")):
            return error_resultset(
                error_message='Invalid question',
                message_code='GL-TG-E010',
            )
        llm_text_model_elements = self.get_llm_text_model()
        if llm_text_model_elements['error']:
            return error_resultset(
                error_message=llm_text_model_elements['error_message'],
                message_code='GL-TG-E020',
            )
        other_data.update({
            "ai_provider": llm_text_model_elements['llm_provider'],
            "ai_model": llm_text_model_elements['llm_model'],
        })
        # Generating answer
        llm_text_model = llm_text_model_elements['class']
        if "system_prompt" in other_data:
            prompt = other_data["system_prompt"]
        else:
            prompt = "{question}"
        response = llm_text_model.query(
            prompt, question,
            (self.get_par_value("REFINE_LLM_PROMPT_TEXT") if
                self.get_prompt_enhancement_flag() else None)
        )
        if response['error']:
            other_data["error_message"] = (
                f"ERROR E-100: {response['error_message']}")
        result = get_default_resultset()
        result["resultset"] = {
            "type": "text",
            "question": question,
            "refined_prompt": response.get('refined_prompt'),
            "answer": response.get('response'),
            "other_data": other_data,
        }
        return result

    def image_generation(
        self,
        question: str = None,
        settings: dict = None
    ):
        result = get_default_resultset()
        if not settings:
            settings = {}
        if not question:
            question = self.session_state.get("question")
        if not self.validate_question(question, settings.get("assign_global")):
            return error_resultset(
                error_message='Invalid question',
                message_code='GL-IG-E010',
            )
        llm_provider = self.get_llm_provider(
            "TEXT_TO_IMAGE_PROVIDERS",
            "image_provider"
        )
        llm_model = self.get_llm_model(
            "TEXT_TO_IMAGE_PROVIDERS", "image_provider",
            "TEXT_TO_IMAGE_AVAILABLE_MODELS", "image_model"
        )
        llm_text_model_elements = self.get_llm_text_model()
        if llm_text_model_elements['error']:
            return error_resultset(
                error_message=llm_text_model_elements['error_message'],
                message_code='GL-IG-E020',
            )
        other_data = {
            "ai_provider": llm_provider,
            "ai_model": llm_model,
            "ai_text_model_provider": llm_text_model_elements['llm_provider'],
            "ai_text_model_model": llm_text_model_elements['llm_model'],
        }
        model_params = {
            "provider": llm_provider,
            "model_name": llm_model,
            "text_model_class": llm_text_model_elements['class'],
        }
        model_params.update(self.get_model_configurations())

        llm_model = ImageGenProvider(model_params)

        response = llm_model.image_gen(
            question,
            (self.get_par_value("REFINE_LLM_PROMPT_TEXT") if
                self.get_prompt_enhancement_flag() else None)
        )
        if response['error']:
            result["error"] = True
            result["error_message"] = (
                f"ERROR GL-IG-100: {response['error_message']}")

        result.update({
            "type": "image",
            "question": question,
            "refined_prompt": response.get('refined_prompt'),
            "answer": response.get('response'),
            "other_data": other_data,
        })
        log_debug(f"image_generation | result: {result}", debug=DEBUG)
        return result

    def video_generation(
        self,
        question: str = None,
        previous_response: dict = None,
        settings: dict = None
    ):
        result = get_default_resultset()
        if not settings:
            settings = {}
        llm_provider = self.get_llm_provider(
            "TEXT_TO_VIDEO_PROVIDERS",
            "video_provider"
        )
        llm_model = self.get_llm_model(
            "TEXT_TO_VIDEO_PROVIDERS", "video_provider",
            "TEXT_TO_VIDEO_AVAILABLE_MODELS", "video_model"
        )

        llm_text_model_elements = self.get_llm_text_model()
        if llm_text_model_elements['error']:
            return error_resultset(
                error_message=llm_text_model_elements['error_message'],
                message_code='GL-VG-E020',
            )

        model_params = {
            # "provider": self.get_par_or_env("TEXT_TO_VIDEO_PROVIDER"),
            "provider": llm_provider,
            "model_name": llm_model,
            "text_model_class": llm_text_model_elements['class'],
        }
        model_params.update(self.get_model_configurations())
        ttv_model = TextToVideoProvider(model_params)

        if previous_response:
            response = previous_response.copy()
            video_id = response['id']
        else:
            video_id = get_new_item_id()
            if not question:
                question = self.session_state.get("question")
            if not self.validate_question(question,
               settings.get("assign_global")):
                return error_resultset(
                    error_message='Invalid question',
                    message_code='GL-VG-E010',
                )
            # Requesting the video generation
            response = ttv_model.video_gen(
                question,
                (self.get_par_value("REFINE_VIDEO_PROMPT_TEXT") if
                    self.get_prompt_enhancement_flag() else None)
            )
            if response['error']:
                return error_resultset(
                    error_message=response['error_message'],
                    message_code='GL-VG-E020',
                )

        #  Checking the video generation status
        video_url = None
        ttv_response = response.copy()
        ttv_response['id'] = video_id

        # Save a preliminar conversation with the video generation request
        # follow-up data in the ttv_response attribute
        other_data = {
            "ttv_response": ttv_response,
            "ai_provider": llm_provider,
            "ai_model": llm_model,
            "ai_text_model_provider":
                llm_text_model_elements['llm_provider'],
            "ai_text_model_model": llm_text_model_elements['llm_model'],
        }
        result["resultset"] = {
            "type": "video",
            "question": question,
            "refined_prompt": ttv_response.get('refined_prompt'),
            "answer": video_url,
            "other_data": other_data,
            "id": video_id,
        }

        response = ttv_model.video_gen_followup(ttv_response)
        if response['error']:
            result["error"] = True
            result["error_message"] = (
                f"ERROR GL-VG-E030: {response['error_message']}")
        elif response.get("video_url"):
            video_url = response["video_url"]
        else:
            result["error"] = True
            result["error_message"] = (
                "ERROR GL-VG-E040: Video generation failed."
                " No video URL. Try again later by clicking"
                " the corresponding previous answer.")
            if response.get("ttv_followup_response"):
                other_data["ttv_followup_response"] = \
                    response["ttv_followup_response"]

        if previous_response and result.get("error_message"):
            return error_resultset(
                error_message=result["error_message"],
                message_code='GL-VG-E050',
            )

        # Save the conversation with the video generation result
        result.update({
            "type": "video",
            "question": question,
            "refined_prompt": ttv_response.get('refined_prompt'),
            "answer": video_url,
            "other_data": other_data,
            "id": video_id,
        })
        log_debug(f"video_generation | result: {result}", debug=DEBUG)
        return result

    # General functions

    def get_par_value(self, param_name: str, default_value: str = None):
        """
        Returns the parameter value. If the parameter value is a file path,
        it will be read and returned.
        """
        result = self.params.get(param_name, default_value)
        if result and isinstance(result, str) and result.startswith("[") \
           and result.endswith("]"):
            result = read_file(f"config/{result[1:-1]}")
        return result

    def get_par_or_env(self, param_name: str, default_value: str = None):
        """
        Returns the parameter value or the environment variable value
        """
        if os.environ.get(param_name):
            return os.environ.get(param_name)
        return self.get_par_value(param_name, default_value)
</file>

<file path="lib/codegen_generation_lib.py">
"""
Code Generation Library
"""
import os

from lib.codegen_utilities import (
    # log_debug,
    get_default_resultset,
    error_resultset)
from lib.codegen_general_lib import GeneralLib
from lib.codegen_schema_generator import JsonGenerator

DEBUG = False


class CodeGenLib(GeneralLib):
    """
    Code generation class
    """

    def process_json_and_code_generation(self, question: str = None):
        """
        Generates the JSON file and GS python code for Tools
        """
        if not question:
            return error_resultset(
                error_message='No question supplied',
                message_code='A-PJACG-E010',
            )
        if not self.validate_question(question):
            return error_resultset(
                error_message='Invalid question supplied',
                message_code='A-PJACG-E020',
            )

        llm_text_model_elements = self.get_llm_text_model()
        if llm_text_model_elements['error']:
            return error_resultset(
                error_message=llm_text_model_elements['error_message'],
                message_code='A-PJACG-E030',
            )

        other_data = {
            "ai_provider": llm_text_model_elements['llm_provider'],
            "ai_model": llm_text_model_elements['llm_model'],
            "template": "json_and_code_generation",
        }

        params = {
            "user_input_text": question,
            "use_embeddings": os.environ.get('USE_EMBEDDINGS', '1') == '1',
            "embeddings_sources_dir": self.get_par_value(
                "EMBEDDINGS_SOURCES_DIR", "./embeddings_sources"),
            "provider": llm_text_model_elements['llm_provider'],
            "model": llm_text_model_elements['llm_model'],
        }
        json_generator = JsonGenerator(params=params)
        response = json_generator.generate_json()
        if response['error']:
            other_data["error_message"] = (
                f"A-PJACG-E040: {response['error_message']}")

        other_data.update(response.get('other_data', {}))
        result = get_default_resultset()
        result['resultset'] = {
            "type": "text",
            "question": question,
            "refined_prompt": response.get('refined_prompt'),
            "answer": response.get(
                'response',
                "No response. Check the Detailed Response section."),
            "other_data": other_data,
        }
        return result
</file>

<file path="lib/codegen_ideation_lib.py">
"""
Ideation Library
"""
import os

from lib.codegen_utilities import (
    log_debug,
    get_default_resultset,
    error_resultset)
from lib.codegen_general_lib import GeneralLib

DEBUG = False


class IdeationLib(GeneralLib):
    """
    Ideation class
    """

    def process_ideation_form(self, form: dict, form_config: dict):
        """
        Process the ideation form
        """
        features_data = form_config.get("features_data", {})
        fields_data = form_config.get("fields", {})

        log_debug("process_ideation_form | form: " + f"{form}", debug=DEBUG)
        log_debug("process_ideation_form | form_config: " + f"{features_data}",
                  debug=DEBUG)

        # Validates the submitted form
        if not form:
            return error_resultset(
                error_message="No data received from the form",
                message_code='A-PIF-E030',
            )
        if not form.get("buttons_submitted_data"):
            return error_resultset(
                error_message="Missing buttons submitted data",
                message_code='A-PIF-E040',
            )

        # Verify button pressed
        selected_feature = self.get_selected_feature(form, features_data)
        if not selected_feature:
            return error_resultset(
                error_message="No button pressed... try again please",
                message_code='A-PIF-E050',
            )

        # Verify mandatory field
        error_message = ""
        for key in features_data.get(selected_feature).get("mandatory_fields"):
            if not form.get(key):
                field_name = fields_data.get(key, {}).get("title", key)
                error_message += f"{field_name}, "
        if error_message:
            error_message = error_message[:-2]
            return error_resultset(
                error_message=f"Missing field(s): {error_message}",
                message_code='A-PIF-E060',
            )

        template = features_data.get(selected_feature).get("template")
        if not template:
            self.show_form_error("Missing template")
            return

        system_prompt_template = features_data.get(selected_feature) \
            .get("system_prompt")
        if not system_prompt_template:
            return error_resultset(
                error_message="Missing system prompt",
                message_code='A-PIF-E070',
            )

        # Read the template file
        template_path = f"./config/{template}"
        if not os.path.exists(template_path):
            return error_resultset(
                error_message=f"Missing template file: {template_path}",
                message_code='A-PIF-E080',
            )
        with open(template_path, "r") as f:
            question = f.read()

        # Read the system prompt file
        system_prompt_path = f"./config/{system_prompt_template}"
        if not os.path.exists(system_prompt_path):
            return error_resultset(
                error_message="Missing system prompt file:"
                              f" {system_prompt_path}",
                message_code='A-PIF-E090',
            )
        with open(system_prompt_path, "r") as f:
            system_prompt = f.read()

        # Default values
        if "timeframe" not in form:
            form["timeframe"] = \
                self.get_par_value("IDEATION_DEFAULT_TIMEFRAME")
        if "quantity" not in form:
            form["quantity"] = self.get_par_value("IDEATION_DEFAULT_QTY")

        # Replace the placeholders with the user input
        final_form = {}
        for key in form:
            if key in [
                "screenshots",
                "buttons_submitted_data",
                "buttons_submitted"
            ]:
                continue
            log_debug(f"process_ideation_form | key: {key} | "
                      f"form[key]: {form[key]}", debug=DEBUG)
            final_form[key] = form[key]
            if form[key]:
                question = question.replace(f"{{{key}}}", form[key])

        form_name = self.get_form_name(form_config)
        form_session_state_key = self.get_form_session_state_key(form_config)
        other_data = {
            "subtype": selected_feature,
            "template": template,
            "system_prompt": system_prompt,
            "form_name": form_name,
            "form_data": final_form,
            "form_session_state_key": form_session_state_key,
        }

        log_debug("process_ideation_form | question: " + f"{question}"
                  "\n | other_data: " + f"{other_data}",
                  debug=DEBUG)

        # Call the LLM to generate the ideation
        response = self.text_generation(question, other_data,
                                        {"assign_global": False})

        log_debug("process_ideation_form | response: " + f"{response}",
                  debug=DEBUG)

        error_message = None
        if response['error']:
            return error_resultset(
                error_message=response['error_message'],
                message_code='A-PIF-E100',
            )

        result = get_default_resultset()
        result['resultset'] = {
            "type": "text",
            "question": question,
            "refined_prompt": response.get('refined_prompt'),
            "answer": response.get(
                'response',
                "No response. Check the Detailed Response section."),
            "other_data": other_data,
        }

        return result
</file>

<file path="lib/codegen_llamaindex_abstraction.py">
"""
LlamaIndex abstraction

Reference:
https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom/#example-using-a-custom-llm-model-advanced
"""

from typing import Any

from typing import ClassVar, List
from pydantic import ConfigDict

from llama_index.core.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
)
from llama_index.core.llms.callbacks import llm_completion_callback

from lib.codegen_ai_utilities import LlmProvider


class LlamaIndexCustomLLM(CustomLLM):
    context_window: int = 3900
    num_output: int = 256
    model_name: str = "unknown"
    final_response: str = "TBD"
    model_object: LlmProvider = None

    class Config:
        """
        This fix the warning:
            /home/adminuser/venv/lib/python3.12/site-packages/pydantic/
            _internal/_fields.py:132: UserWarning: Field "model_name" in
            LlamaIndexCustomLLM has conflict with protected namespace "model_".
            You may be able to resolve this warning by setting
            `model_config['protected_namespaces'] = ()`.
            warnings.warn(
            /home/adminuser/venv/lib/python3.12/site-packages/pydantic/
            _internal/_fields.py:132: UserWarning: Field "model_object" in
            LlamaIndexCustomLLM has conflict with protected namespace "model_".
            You may be able to resolve this warning by setting 
            `model_config['protected_namespaces'] = ()`.
            warnings.warn(
        """
        arbitrary_types_allowed = True
        protected_namespaces = ()

    @property
    def metadata(self) -> LLMMetadata:
        """Get LLM metadata."""
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
        )

    @llm_completion_callback()
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        self.final_response = self.query_custom_llm(prompt)
        return CompletionResponse(text=self.final_response)

    @llm_completion_callback()
    def stream_complete(
        self, prompt: str, **kwargs: Any
    ) -> CompletionResponseGen:
        self.final_response = self.query_custom_llm(prompt)
        response = ""
        for token in self.final_response:
            response += token
            yield CompletionResponse(text=response, delta=token)

    def init_custom_llm(self, model_object: LlmProvider):
        self.model_object = model_object
        self.model_name = model_object.model_name

    def query_custom_llm(self, prompt: str, **kwargs: Any) -> dict:
        if not self.model_object:
            raise ValueError("Model object not initialized")
        llm_response = self.model_object.query(
            prompt="",
            question=prompt,
            prompt_enhancement_text="",
            unified=True,
        )
        if llm_response['error']:
            raise ValueError(f'ERROR: {llm_response["error_message"]}')
        return llm_response['response']
</file>

<file path="lib/codegen_powerpoint.py">
"""
PowerPoint generation
"""
import os

# Reference:
# pip install python-pptx
# https://python-pptx.readthedocs.io/en/latest/user/quickstart.html

import pptx
# from pptx.util import Inches

from lib.codegen_utilities import (
    create_dirs,
    read_file,
    log_debug,
)

DEBUG = False

# DEFAULT_POWERPOINT_TEMPLATE = "default_powerpoint_template.pptx"
DEFAULT_POWERPOINT_TEMPLATE = ""


class PowerPointGenerator:
    """
    PowerPoint generator class
    """
    def __init__(self, params: dict = None):
        self.params = params or {}

    def generate(self, slides_config: list):
        """
        Generates the PowerPoint slides
        """
        output_dir = self.params.get("output_dir", "./output")
        file_name = self.params.get("file_name", "app_presentation")
        create_dirs(output_dir)
        target_file_path = f"{output_dir}/{file_name}.pptx"

        template = self.params.get("template", DEFAULT_POWERPOINT_TEMPLATE)
        if template:
            template_path = f"./config/{template}"
            if not os.path.exists(template_path):
                raise ValueError(f"Missing template file: {template_path}")
            pptx_template = read_file(template_path)
            pptx_obj = pptx.Presentation(pptx_template)
        else:
            pptx_obj = pptx.Presentation()

        bullet_slide_layout = pptx_obj.slide_layouts[1]
        for slide_config in slides_config.get("slides", []):
            slide = pptx_obj.slides.add_slide(bullet_slide_layout)
            shapes = slide.shapes

            log_debug("PowerPointGenerator | generate | "
                      f"slide_config: {slide_config}", debug=DEBUG)

            # shapes.title.font.size = Inches(2)
            title_shape = shapes.title
            body_shape = shapes.placeholders[1]

            title_shape.text = slide_config.get("title")
            content_items = slide_config.get("content")
            if isinstance(content_items, str):
                content_items = [{
                    "type": "text",
                    "text": content_items,
                }]
            for content in content_items:
                if content.get("type") == "text":
                    # text = shapes.add_textbox(
                    #     # Inches(content.get("x")),
                    #     # Inches(content.get("y")),
                    #     # Inches(content.get("width")),
                    #     # Inches(content.get("height")),
                    # )
                    # First bullet text
                    tf = body_shape.text_frame
                    text = content.get("text", "")
                    if "\n" in text or "\r" in text or "* " in text:
                        separator = \
                            "\n" if "\n" in text else \
                            "* " if "* " in text else "\r"
                        splitted_text = text.split(separator)
                        for line in splitted_text:
                            p = tf.add_paragraph()
                            p.text = line
                            p.level = 0
                    else:
                        tf.text = text
                    # # Use _TextFrame.text for first bullet'
                    # p = tf.add_paragraph()
                    # p.text = "Sub-bullets"
                    # p.level = 1
                    # # Use _TextFrame.add_paragraph() for subsequent bullets
                    # p = tf.add_paragraph()
                    # p.text = 'Subsequent bullets'
                    # p.level = 2

                elif content.get("type") == "image":
                    image = shapes.add_picture(
                        # Inches(content.get("x")),
                        # Inches(content.get("y")),
                        # Inches(content.get("width")),
                        # Inches(content.get("height")),
                        content.get("image_path"),
                    )
                    # image.width = Inches(content.get("width"))
                    # image.height = Inches(content.get("height"))
                    image.shapes.title.text = content.get("title")
                elif content.get("type") == "table":
                    table = shapes.add_table(
                        # Inches(content.get("x")),
                        # Inches(content.get("y")),
                        # Inches(content.get("width")),
                        # Inches(content.get("height")),
                    )
                    for row in content.get("rows", []):
                        for cell in row:
                            table.cell(row=row.get("row"),
                                       column=cell.get("column")).text = \
                                cell.get("text")
                # else:
                #     raise ValueError(
                #       f"Invalid content type: {content.get('type')}")

            # Add notes
            notes_slide = slide.notes_slide
            text_frame = notes_slide.notes_text_frame
            text_frame.text = ''
            if slide_config.get("speaker_notes"):
                text_frame.text += slide_config.get("speaker_notes")
            if slide_config.get("image_prompt"):
                text_frame.text += "\n\nImage Prompt: " + \
                                   slide_config.get("image_prompt")

        # Save presentation and return the file path
        pptx_obj.save(target_file_path)
        return target_file_path
</file>

<file path="lib/codegen_schema_generator.py">
"""
codegen_schema_generator.py
2024-10-27 | CR
"""
# from typing import Any
import os
# import sys
import time
from datetime import datetime

import json
import pprint

import argparse

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

from lib.codegen_ai_utilities import LlmProvider
from lib.codegen_utilities import (
    get_default_resultset,
    read_file,
)
from lib.codegen_utilities import get_app_config
from lib.codegen_llamaindex_abstraction import LlamaIndexCustomLLM
# from lib.codegen_utilities import log_debug

DEBUG = False
USE_PPRINT = False

DEFAULT_AI_PROVIDER = [
    "chat_openai",
    "groq",
    "ollama",
    "rhymes",
    "nvidia",
]

DEFAULT_MODEL_TO_USE = {
    "chat_openai": "gpt-4-mini",
    "groq": "llama3-8b-8192",
    "ollama": "llama3.2",
              # "llava",
              # "deepseek-coder-v2",
              # "nemotron",
    "rhymes": "aria",
    "nvidia": "nvidia/llama-3.1-nemotron-70b-instruct",
}

DEFAULT_TEMPERATURE = "0.5"
DEFAULT_STREAM = ""

DEFAULT_AGENTS_COUNT = 0

OLLAMA_BASE_URL = ""
# OLLAMA_BASE_URL = "localhost:11434"


# Default prompt to generate the .json files for the frontend and backend
SYSTEM_PROMPT = """
You are a developer specialized in JSON files generation and
Python Langchain Tools implementation.
Your task is to create the JSON files for the frontend and backend
of the given application and the Langchain Tools to perform the search,
insert and update operations.
"""

USER_MESSAGE_PROMPT = """
The given application and its schema/table descriptions are described below:
----------------
{user_input}
----------------
"""

SUFFIX_NO_EMBEDDINGS = """
Based on the following documentation and examples:
{files}
"""

SUFFIX_END = """
Give me the generic CRUD editor configuration JSON files for
the given application, and the python code to implement the Langchain Tools
to perform the search, insert and update operations for the given application
tables.
The JSON files must be build according to the specs and instructions
in the `Generic-CRUD-Editor-Configuration.md` file.
The example files: `frontend/users.json`, `frontend/users_config.json`,
`backend/users.json`, and `backend/users_config.json` are included only as
a reference for you to know how to build the JSON files.
The Python files: `ai_gpt_fn_index.py` and `ai_gpt_fn_tables.py`
are included as a reference for you to know how to implement the
Langchain Tools.
Don't give recommendations, observations, or explanations about the database,
just give me the names and content of JSON files (not the JSON example files)
for the given application and the Langchain Tools python code.
"""


class ArgsClass:
    def __init__(self, params: dict):
        params = params or {}

        def get_param_or_envvar(param_name: str, default_value: str = None):
            return params.get(
                param_name,
                os.environ.get(
                    "LLM_PROVIDER",
                    default_value)
            )

        self.user_input_text = params.get("user_input_text")
        self.user_input_file = params.get("user_input_file")
        self.provider = params.get(
            "provider",
            get_param_or_envvar("LLM_PROVIDER", DEFAULT_AI_PROVIDER[0]))
        self.model = params.get("model")
        # self.model = params.get(
        #     "model",
        #     DEFAULT_MODEL_TO_USE[DEFAULT_AI_PROVIDER[0]])
        self.temperature = params.get("temperature", DEFAULT_TEMPERATURE)
        self.stream = params.get("stream", DEFAULT_STREAM)
        self.ollama_base_url = params.get("ollama_base_url", OLLAMA_BASE_URL)
        self.agents_count = params.get("agents_count", DEFAULT_AGENTS_COUNT)


class JsonGenerator:
    """
    Class to generate the .json files for the frontend and backend
    """
    def __init__(self, params: dict):
        if not params:
            params = {}
        self.params = dict(params)
        self.params.update(get_app_config())
        self.args = self.read_arguments(params)
        self.embeddings_sources_dir = self.params.get(
            "embeddings_sources_dir", "./embeddings_sources")
        self.reference_files = self.get_reference_files()
        self.system_prompt = SYSTEM_PROMPT
        self.user_input = self.get_user_input()
        self.final_input = None
        self.final_summary = None
        self.provider_model_used = None
        self.model_config = {}

    def read_arguments(self, params):
        """
        Decide where to read arguments from
        """
        # if len(sys.argv) > 1:
        if self.params.get("cli"):
            # If it's called from the command line, we need to read the
            # arguments from the command line
            args = self.read_arguments_from_cli()
        else:
            # If it's not called from the command line, we need to read the
            # arguments from the environment variables
            args = ArgsClass(params)
        return args

    def read_arguments_from_cli(self):
        """
        Read arguments from the command line
        """
        parser = argparse.ArgumentParser()
        parser.add_argument(
            '--user_input_text',
            type=str,
            default=None,
            help='User input text to be used as the initial plan. ' +
                 'It\'s mandatory to provide a user input file or text.'
        )
        parser.add_argument(
            '--user_input_file',
            type=str,
            default=None,
            help='User input file to be used as the initial plan. ' +
                 'It\'s mandatory to provide a user input file or text.'
        )
        parser.add_argument(
            '--provider',
            type=str,
            default=DEFAULT_AI_PROVIDER[0],
            help='Provider to use (ollama, nvidia, chat_openai, groq). ' +
            f'Default: {DEFAULT_AI_PROVIDER[0]}'
        )
        parser.add_argument(
            '--model',
            type=str,
            default=DEFAULT_MODEL_TO_USE[DEFAULT_AI_PROVIDER[0]],
            help='Model to use. Default: ' +
                 DEFAULT_MODEL_TO_USE[DEFAULT_AI_PROVIDER[0]]
        )
        parser.add_argument(
            '--temperature',
            type=str,
            default=DEFAULT_TEMPERATURE,
            help=f'Temperature to use. Default: {0.5}'
        )
        parser.add_argument(
            '--stream',
            type=str,
            default=DEFAULT_STREAM,
            help=f'Stream to use. Default: {"1"}'
        )
        parser.add_argument(
            '--agents_count',
            type=int,
            default=DEFAULT_AGENTS_COUNT,
            help=f'Number of agents to use. Default: {DEFAULT_AGENTS_COUNT}'
        )
        parser.add_argument(
            '--ollama_base_url',
            type=str,
            default=OLLAMA_BASE_URL,
            help=f'Ollama base URL. Default: {OLLAMA_BASE_URL}'
        )

        args = parser.parse_args()
        return args

    def read_user_input(self):
        """
        Read the user input from a file
        """
        if self.args.user_input is None:
            raise ValueError("User input file is mandatory")

        if not os.path.exists(self.args.user_input):
            raise FileNotFoundError(
                f"User input file not found: {self.args.user_input}")

        with open(self.args.user_input, 'r') as f:
            user_input = f.read()

        return user_input

    def get_user_input(self):
        """
        Returns the user input text or file
        """
        if self.args.user_input_file:
            user_input = self.read_user_input_file()
        else:
            user_input = self.args.user_input_text

        if self.params.get("use_embeddings"):
            template = SYSTEM_PROMPT + USER_MESSAGE_PROMPT + SUFFIX_END
            user_input = template.format(
                user_input=user_input,
            )
        else:
            template = USER_MESSAGE_PROMPT + SUFFIX_NO_EMBEDDINGS + SUFFIX_END
            user_input = template.format(
                user_input=user_input,
                files="\n".join([
                    f"\nFile: {f.get('name')}" +
                    "\nFile content:" +
                    "\n-----------------" +
                    f"\n{f.get('content')}" +
                    "\n-----------------"
                    for f in self.reference_files
                ]),
            )
        return user_input

    def log_debug(self, message):
        """
        Prints the message for debugging
        """
        if DEBUG:
            print(message)

    def log_debug_structured(self, messages):
        """
        Prints the messages in a structured way for debugging
        """
        if DEBUG:
            if USE_PPRINT:
                pp = pprint.PrettyPrinter(indent=4)
                pp.pprint(messages)
                print("")
            else:
                print(messages)

    def get_elapsed_time_formatted(self, elapsed_time):
        """
        Returns the elapsed time formatted
        """
        if elapsed_time < 60:
            return f"{elapsed_time:.2f} seconds"
        elif elapsed_time < 3600:
            return f"{elapsed_time / 60:.2f} minutes"
        else:
            return f"{elapsed_time / 3600:.2f} hours"

    def log_procesing_time(self, message: str = "", start_time: float = None):
        """
        Returns the current time and prints the message
        """
        if start_time is None:
            start_time = time.time()
            readable_timestamp = datetime.fromtimestamp(start_time) \
                .strftime('%Y-%m-%d %H:%M:%S')
            print("")
            print((message if message else 'Process') +
                  f" started at {readable_timestamp}...")
            return start_time

        end_time = time.time()

        readable_timestamp = datetime.fromtimestamp(end_time) \
            .strftime('%Y-%m-%d %H:%M:%S')
        processing_time = self.get_elapsed_time_formatted(
            end_time - start_time)

        print("")
        print(
            (message if message else 'Process') +
            f" ended at {readable_timestamp}" +
            f". Processing time: {processing_time}")
        return end_time

    def get_model(self, model_to_use: str = None):
        """
        Returns the model to use based on the default model to use
        """
        if not model_to_use:
            model_to_use = self.args.model
        return model_to_use

    def get_llm_model_object(self, model: str):
        """
        Returns the LLM model object
        """
        self.model_config = {
            'model_name': model,
            "provider": self.args.provider,
            "temperature": self.args.temperature,
            "stream": self.args.stream,
            "ollama_base_url": self.args.ollama_base_url,
        }
        # no_system_prompt = (self.args.provider in ["nvidia"])
        self.provider_model_used = \
            f"Provider: {self.args.provider}" + \
            f" | Model: {self.model_config['model_name']}"
        # self.log_debug_structured(self.model_config)
        llm_model = LlmProvider(self.model_config)
        return llm_model

    def get_chat_response(self, model: str, prompt: str, user_input: str):
        """
        Returns the response from the model
        """
        llm_model = self.get_llm_model_object(model)
        llm_response = llm_model.query(
            prompt=prompt,
            question=user_input,
            # unified=no_system_prompt,
        )
        if llm_response['error']:
            raise ValueError(f'ERROR: {llm_response["error_message"]}')
        return llm_response['response']

    def get_index_response(self, model: str, prompt: str, user_input: str):
        """
        Returns the response from the index
        """
        llamaindex_llm = LlamaIndexCustomLLM()
        llamaindex_llm.init_custom_llm(self.get_llm_model_object(model))
        documents = SimpleDirectoryReader(
            self.embeddings_sources_dir).load_data()
        index = VectorStoreIndex.from_documents(documents)
        query_engine = index.as_query_engine(llm=llamaindex_llm)
        response = query_engine.query(user_input)
        self.log_debug(f"get_index_response | response:\n{response}")
        return f"{response}"

    def get_model_response(self, model: str, prompt: str, user_input: str):
        """
        Returns the response from the index or the chat
        """
        if self.params.get("use_embeddings"):
            return self.get_index_response(model, prompt, user_input)
        return self.get_chat_response(model, prompt, user_input)

    def CEO_Agent(self, user_input, is_final=False):
        """
        Main agent that creates initial plan and final summary
        """
        system_prompt = 'You are o1, an AI assistant focused on clear ' + \
            'step-by-step reasoning. Break every task into ' + \
            f'{self.args.agents_count} actionable step(s). ' + \
            'Always answer in short.' \
            if not is_final else \
            'Summarize the following plan and its implementation into a ' + \
            'cohesive final strategy.'

        self.log_debug("")
        self.log_debug(f"CEO messages (is_final: {is_final}):")
        # self.log_debug_structured(messages)

        start_time = self.log_procesing_time(
            f'CEO {"final" if is_final else "initial"} processing...')

        response = self.get_model_response(
            model=self.get_model(),
            prompt=system_prompt,
            user_input=user_input,
            # messages=messages
        )

        self.log_procesing_time(start_time=start_time)
        self.log_debug("")
        self.log_debug(f'CEO {"final" if is_final else "initial"} response:')
        self.log_debug(response)

        return response

    def create_agent(self, step_number):
        """
        Factory method to create specialized agents for each step
        """
        def agent(task):
            system_prompt = (
                f'You are Agent {step_number}, focused ONLY ' +
                f'on implementing step {step_number}. Provide a detailed' +
                ' but concise implementation of this specific step. ' +
                'Ignore all other steps.'
            )
            user_input = (
                    f'Given this task:\n{task}\n\nProvide ' +
                    f'implementation for step {step_number} only'
            )
            self.log_debug("")
            self.log_debug(f'Agent step-{step_number} messages:')
            # self.log_debug_structured(messages)

            start_time = self.log_procesing_time(f"Agent step-{step_number}")
            response = self.get_model_response(
                model=self.get_model(),
                prompt=system_prompt,
                user_input=user_input,
                # messages=messages
            )

            self.log_procesing_time(
                message=f"Agent step-{step_number}",
                start_time=start_time)
            self.log_debug("")
            self.log_debug(f'Agent step-{step_number} response:')
            self.log_debug(response)

            return response

        return agent

    def get_reference_files(self):
        """
        Returns the reference files to be used
        """
        read_file_params = {}
        if self.params.get("use_embeddings"):
            read_file_params = {
                "save_file": True,
                "output_dir": self.embeddings_sources_dir,
            }
        # Read the `schema_generator_ref_files.json` file
        with open('./config/schema_generator_ref_files.json', 'r') as f:
            ref_files = json.load(f)

        # Create a list of dictionaries with the name, path and content of each
        # reference file
        ref_files_list = [
            {
                'name': ref_file['name'],
                'path': ref_file['path'],
                'content': read_file(ref_file['path'], read_file_params),
            } for ref_file in ref_files
        ]
        return ref_files_list

    def save_result(self):
        """
        Saves the final result to a file
        """
        output_file = os.path.join(
            self.params.get('output_dir', "./output"),
            self.params.get('output_file', (
                'final_summary_{date_time}.txt'.format(
                    date_time=datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))))
        )
        with open(output_file, 'w') as f:
            if DEBUG:
                f.write("DEBUG Prompt:\n")
                f.write("\n")
                f.write(self.system_prompt)
                f.write("\n")
            f.write("\n")
            f.write("User input:\n")
            f.write("\n")
            f.write(self.user_input)
            f.write("\n")
            if self.final_input:
                f.write("\n")
                f.write("Final input:\n")
                f.write("\n")
                f.write(self.final_input)
                f.write("\n")
            f.write("\n")
            f.write(f">>> Generated by: {self.provider_model_used}")
            f.write("\n")
            f.write(">>> Final summary:\n")
            f.write("\n")
            f.write(self.final_summary)
            f.write("\n")

    def process_task(self):
        """
        Orchestrate the entire workflow to get the final summary
        """
        start_time = self.log_procesing_time(
            "Main process" +
            f"{self.args.agents_count} agent steps...")

        # Step # 1: Get high level plan from CEO
        initial_plan = self.CEO_Agent(
            f'{self.system_prompt}\n{self.user_input}')

        # Step # 2: Create agents, execute all agent steps, and get detailed
        # implementation for each step
        agents = [self.create_agent(i)
                  for i in range(1, self.args.agents_count + 1)]
        implementations = [agent(initial_plan) for agent in agents]

        # Step # 3: Combine everything to get the final summary from CEO
        self.final_input = \
            f"Initial Plan:\n{initial_plan}" + \
            "\n\nImplementations: \n" + \
            "\n".join(implementations)

        # Step # 4: Final summary
        # self.final_summary = self.CEO_Agent(self.final_input, is_final=True)
        response = self.CEO_Agent(self.final_input, is_final=True)
        self.final_summary = response

        # Save everything to a file
        self.save_result()

        self.log_procesing_time(message="Main process", start_time=start_time)
        return response

    def simple_processing(self):
        """
        Simple processing without agents
        """
        self.log_debug("")
        self.log_debug("Simple Processing messages:")
        # self.log_debug_structured(messages)

        start_time = self.log_procesing_time('Simple Processing...')

        response = self.get_model_response(
            model=self.get_model(),
            prompt=self.system_prompt,
            user_input=self.user_input,
        )

        self.log_procesing_time(start_time=start_time)
        self.log_debug("")
        self.log_debug('Simple Processing response:')
        self.log_debug(response)

        self.final_summary = response
        self.save_result()
        return self.final_summary

        # self.final_summary = response["response"]
        # self.save_result()
        # return response

    def generate_json(self):
        """
        Main entry point to generate the .json files
        """
        response = get_default_resultset()
        if not self.args.user_input_text and not self.args.user_input_file:
            response["error"] = True
            response["error_message"] = "User input text or file is required"
            return response

        if self.args.agents_count == 0:
            # If the number of agents is 0, we don't need to use the agents
            response["response"] = self.simple_processing()
        else:
            # Reasoning with agents
            response["response"] = self.process_task()

        response["other_data"] = {
            "system_prompt": self.system_prompt,
            "user_input": self.user_input,
        }
        if self.final_input:
            response["other_data"]["final_input"] = self.final_input

        return response


if __name__ == "__main__":

    json_generator = JsonGenerator()
    final_result = json_generator.generate_json({
        "cli": True
    })

    print("")
    print("Final result:")
    print(final_result)
    print("")
</file>

<file path="lib/codegen_streamlit_lib.py">
"""
Streamlit UI library
"""
from typing import Any, Callable
import os
import time
import json
import uuid
import html

import streamlit as st

from lib.codegen_utilities import (
    log_debug,
    get_date_time,
    get_new_item_id,
    get_default_resultset,
    read_file,
    is_an_url,
    path_exists,
)
from lib.codegen_db import CodegenDatabase
from lib.codegen_ai_utilities import (
    TextToVideoProvider,
    LlmProvider,
    ImageGenProvider,
)
from lib.codegen_powerpoint import PowerPointGenerator


DEBUG = False


@st.dialog("Form validation")
def show_popup(title: str, message: str, msg_type: str = "success"):
    """
    Show a streamlit popup with a message
    """
    message = message.replace("\n", "<br>")
    message = message.replace("\r", "<br>")
    st.header(f"{title}")
    if msg_type == "success":
        st.success(message)
    elif msg_type == "error":
        st.error(message)
    elif msg_type == "info":
        st.info(message)
    elif msg_type == "warning":
        st.warning(message)


class StreamlitLib:
    """
    Streamlit UI library
    """
    def __init__(self, params: dict):
        self.params = params

    # General utilities and functions

    def set_new_id(self, id: str = None):
        """
        Set the new id global variable
        """
        # if "new_id" not in st.session_state:
        #     st.session_state.new_id = None
        st.session_state.new_id = id

    def get_new_id(self):
        """
        Get the new id global variable
        """
        if "new_id" in st.session_state:
            return st.session_state.new_id
        else:
            return "No new_id"

    def set_query_param(self, name, value):
        """
        Set a URL query parameter
        """
        st.query_params[name] = value

    def timer_message(
        self, message: str, type: str,
        container: st.container = None,
        seconds: int = 10
    ):
        """
        Start a timer
        """
        if not container:
            container = st.empty()
        if type == "info":
            alert = container.info(message)
        elif type == "warning":
            alert = container.warning(message)
        elif type == "success":
            alert = container.success(message)
        elif type == "error":
            alert = container.error(message)
        else:
            raise ValueError(f"Invalid type: {type}")
        time.sleep(seconds)
        # Clear the alert
        alert.empty()

    def success_message(self, message: str, container: st.container = None):
        """
        Display a success message
        """
        self.timer_message(message, "success", container)

    def error_message(self, message: str, container: st.container = None):
        """
        Display an error message
        """
        self.timer_message(message, "error", container)

    def info_message(self, message: str, container: st.container = None):
        """
        Display an info message
        """
        self.timer_message(message, "info", container)

    def warning_message(self, message: str, container: st.container = None):
        """
        Display a warning message
        """
        self.timer_message(message, "warning", container)

    # Conversations database

    def init_db(self):
        """
        Initialize the JSON file database
        """
        db_type = os.getenv('DB_TYPE')
        db = None
        if db_type == 'json':
            db = CodegenDatabase("json", {
                "JSON_DB_PATH": os.getenv(
                    'JSON_DB_PATH',
                    self.get_par_value("CONVERSATION_DB_PATH")
                ),
            })
        if db_type == 'mongodb':
            db = CodegenDatabase("mongodb", {
                "MONGODB_URI": os.getenv('MONGODB_URI'),
                "MONGODB_DB_NAME": os.getenv('MONGODB_DB_NAME'),
                "MONGODB_COLLECTION_NAME": os.getenv('MONGODB_COLLECTION_NAME')
            })
        if not db:
            raise ValueError(f"Invalid DB_TYPE: {db_type}")
        return db

    def update_conversations(self):
        """
        Update the side bar conversations from the database
        """
        st.session_state.conversations = self.get_conversations()

    def update_conversation(
        self,
        item: dict = None,
        id: str = None
    ):
        db = self.init_db()
        log_debug(f"UPDATE_CONVERSATION | id: {id} | item: {item}",
                  debug=DEBUG)
        db.save_item(item, id)
        self.set_new_id(id)

    def save_conversation(
        self, type: str,
        question: str,
        answer: str,
        title: str = None,
        refined_prompt: str = None,
        other_data: dict = None,
        id: str = None
    ):
        """
        Save the conversation in the database
        """
        if not id:
            id = get_new_item_id()
        if not title:
            title = self.generate_title_from_question(question)
            title = title[:self.get_title_max_length()]
        db = self.init_db()
        item = {
            "type": type,
            "title": title,
            "question": question,
            "answer": answer,
            "refined_prompt": refined_prompt,
            "timestamp": time.time(),
        }
        if not other_data:
            other_data = {}
        item.update(other_data)
        db.save_item(item, id)
        self.update_conversations()
        self.recycle_suggestions()
        self.set_new_id(id)
        return id

    def get_conversations(self):
        """
        Returns the conversations in the database
        """
        db = self.init_db()
        conversations = db.get_list("timestamp", "desc")
        # Add the date_time field to each conversation
        for conversation in conversations:
            conversation['date_time'] = get_date_time(
                conversation['timestamp'])
        return conversations

    def get_conversation(self, id: str):
        """
        Returns the conversation in the database
        """
        db = self.init_db()
        conversation = db.get_item(id)
        if conversation:
            # Add the date_time field to the conversation
            conversation['date_time'] = get_date_time(
                conversation['timestamp'])
            return conversation
        return None

    def delete_conversation(self, id: str):
        """
        Delete a conversation from the database
        """
        db = self.init_db()
        db.delete_item(id)
        self.update_conversations()

    # Prompt suggestions

    def reset_suggestions_prompt(self):
        """
        Reset the suggestions prompt
        """
        prompt = self.get_par_value("SUGGESTIONS_PROMPT_TEXT")
        prompt = prompt.format(
            timeframe=self.get_par_value("SUGGESTIONS_DEFAULT_TIMEFRAME"),
            app_type=self.get_par_value("SUGGESTIONS_DEFAULT_APP_TYPE"),
            app_subject=self.get_par_value("SUGGESTIONS_DEFAULT_APP_SUBJECT"),
            qty=self.get_par_value("SUGGESTIONS_QTY", 4),
        )
        st.session_state.suggestions_prompt_text = prompt

    def get_suggestions_from_ai(self, system_prompt: str, user_prompt: str
                                ) -> dict:
        """
        Get suggestions from the AI
        """
        # The model replacement for suggestions is to avoid use reasoning
        # models like o1-preview/o1-mini because they are expensive and
        # slow, and replace them with a less expensive model like GPT-4o-mini.
        model_replacement = self.get_par_value("SUGGESTIONS_MODEL_REPLACEMENT")
        llm_text_model = self.get_llm_text_model(model_replacement)
        if llm_text_model['error']:
            log_debug("get_suggestions_from_ai | llm_text_model "
                      f"ERROR: {llm_text_model}", debug=DEBUG)
            return llm_text_model
        # Get the model class
        llm_model = llm_text_model['class']
        # Get the suggestions from the AI
        llm_response = llm_model.query(system_prompt, user_prompt)
        log_debug("get_suggestions_from_ai | " +
                  f"response: {llm_response}", debug=DEBUG)
        if llm_response['error']:
            log_debug("get_suggestions_from_ai | llm_response "
                      f"ERROR: {llm_response}", debug=DEBUG)
            return llm_response
        # Clean the suggestions response
        suggestions = llm_response['response']
        suggestions = suggestions.replace("\n", "")
        suggestions = suggestions.replace("\r", "")
        suggestions = suggestions.replace("Suggestions:", "")
        suggestions = suggestions.strip()
        suggestions = suggestions.replace('```json', '')
        suggestions = suggestions.replace('```', '')
        suggestions = suggestions.replace("\\'", '')
        # Load the suggestions
        try:
            suggestions = json.loads(suggestions)
            log_debug("get_suggestions_from_ai | FINAL suggestions:"
                      f" {suggestions}", debug=DEBUG)
        except Exception as e:
            log_debug(f"get_suggestions_from_ai | ERROR {e}", debug=DEBUG)
            return self.get_par_value("DEFAULT_SUGGESTIONS")
        return suggestions

    def recycle_suggestions(self):
        """
        Recycle the suggestions from the AI
        """
        system_prompt = self.get_par_value("SUGGESTIONS_PROMPT_SYSTEM")
        # Prepare user prompt from the input text in the main form
        user_prompt = st.session_state.suggestions_prompt_text + \
            "\n\n" + self.get_par_value("SUGGESTIONS_PROMPT_SUFFIX")
        # Add the suggestion quantity
        user_prompt = user_prompt.replace(
            "{qty}",
            str(self.get_par_value("SUGGESTIONS_QTY", 4)))
        # Add the timeframe
        user_prompt = user_prompt.replace(
            "{timeframe}", str(self.get_par_value(
                "SUGGESTIONS_DEFAULT_TIMEFRAME", "48 hours")))
        # Get the suggestions from the selected LLM text model
        st.session_state.suggestion = self.get_suggestions_from_ai(
            system_prompt, user_prompt)

    def show_one_suggestion(self, suggestion: Any):
        """
        Show one suggestion in the main section
        """
        response = ""
        if suggestion:
            if isinstance(suggestion, dict):
                if "title" in suggestion:
                    response += suggestion.get("title") + "\n"
                if "description" in suggestion:
                    response += suggestion.get("description")
            else:
                response = suggestion
        if not response:
            response = "N/A"
        return response

    def show_suggestion_components(self, container: st.container):
        """
        Show the suggestion components in the main section
        """
        if "suggestions_prompt_text" not in st.session_state:
            self.reset_suggestions_prompt()

        if st.session_state.get("generate_suggestions"):
            with st.spinner("Generating suggestions..."):
                self.recycle_suggestions()

        if st.session_state.get("reset_suggestions_prompt"):
            self.reset_suggestions_prompt()

        if st.session_state.get("recycle_suggestions"):
            log_debug("RECYCLE_SUGGESTIONS | Recycling suggestions",
                      debug=DEBUG)
            if self.get_par_value("DYNAMIC_SUGGESTIONS", True):
                with st.spinner("Refreshing suggestions..."):
                    self.recycle_suggestions()
            elif not st.session_state.get("suggestion"):
                st.session_state.suggestion = \
                    self.get_par_value("DEFAULT_SUGGESTIONS")

        if not isinstance(st.session_state.suggestion, dict):
            st.session_state.suggestion = \
                self.get_par_value("DEFAULT_SUGGESTIONS")

        # Show the 4 suggestions in the main section
        if "error" in st.session_state.suggestion:
            with st.expander("ERROR loading suggestions..."):
                st.write(st.session_state.suggestion["error_message"])
        else:
            sug_col1, sug_col2, sug_col3 = st.columns(
                3, gap="small",
            )
            max_length = self.get_title_max_length()
            for i in range(self.get_par_value("SUGGESTIONS_QTY")):
                suggestion = self.show_one_suggestion(
                    st.session_state.suggestion.get(
                        f"s{i+1}"))
                suggestion = suggestion[:max_length] + "..." \
                    if len(suggestion) > max_length else suggestion
                if i % 2 != 0:
                    with sug_col1:
                        sug_col1.button(suggestion, key=f"s{i+1}")
                else:
                    with sug_col2:
                        sug_col2.button(suggestion, key=f"s{i+1}")
            with sug_col3:
                if self.get_par_value("DYNAMIC_SUGGESTIONS", True):
                    sug_col3.button(
                        ":recycle:",
                        key="recycle_suggestions",
                        help="Recycle suggestions buttons",
                    )
                with st.expander("Suggestions Prompt"):
                    st.session_state.suggestions_prompt_text = st.text_area(
                        "Prompt:",
                        st.session_state.suggestions_prompt_text,
                    )
                    st.button(
                        "Generate Suggestions",
                        key="generate_suggestions",
                    )
                    st.button(
                        "Reset Prompt",
                        key="reset_suggestions_prompt",
                    )

        # Process the suggestion button pushed
        # (must be done before the user input)
        for key in st.session_state.suggestion.keys():
            if st.session_state.get(key):
                st.session_state.question = \
                    self.show_one_suggestion(st.session_state.suggestion[key])
                break

    # Conversation titles

    def get_title_max_length(self):
        return self.get_par_value("CONVERSATION_TITLE_LENGTH", 100)

    def get_title_from_question(self, question: str) -> str:
        """
        Returns the title from the question
        """
        title = question
        title = title.replace("```json", "")
        title = title.replace("```", "")
        title = title.replace("\t", " ")
        title = title.replace("\n", " ")
        title = title.replace("\r", " ")
        title = title.strip()
        return title

    def get_conversation_title(self, conversation: dict):
        return conversation.get(
            "title",
            self.get_title_from_question(conversation['question'])
        )

    def generate_title_from_question(self, question: str) -> str:
        """
        Returns the title from the question
        """
        default_title = self.get_title_from_question(question)
        title_length = self.get_title_max_length()
        # Use small models for title generation
        model_replacement = self.get_par_value("SUGGESTIONS_MODEL_REPLACEMENT")
        llm_text_model = self.get_llm_text_model(model_replacement)
        if llm_text_model['error']:
            log_debug("generate_title_from_question | llm_text_model "
                      f"ERROR: {llm_text_model}", debug=DEBUG)
            return default_title
        llm_model = llm_text_model['class']
        # Prepare the prompt
        prompt = "Give me a title for this question " \
                 f"(max length: {title_length*2}): {question}"
        # Get the title from the AI
        llm_response = llm_model.query(prompt, "", unified=True)
        log_debug("GENERATE_TITLE_FROM_QUESTION | " +
                  f"response: {llm_response}", debug=DEBUG)
        if llm_response['error']:
            log_debug("generate_title_from_question | llm_response "
                      f"ERROR: {llm_response}", debug=DEBUG)
            return default_title
        title = llm_response['response']
        return title

    # Conversations management

    def show_conversations(self):
        """
        Show the conversations in the side bar
        """
        title_length = self.get_title_max_length()
        st.header("Previous answers")
        for conversation in st.session_state.conversations:
            col1, col2 = st.columns(2, gap="small")
            with col1:
                title = self.get_conversation_title(conversation)
                help_msg = \
                    f"{conversation['type'].capitalize()} generated on " \
                    f"{conversation['date_time']}\n\nID: {conversation['id']}"
                st.button(
                    title[:title_length],
                    key=f"{conversation['id']}",
                    help=help_msg)
            with col2:
                st.button(
                    "x",
                    key=f"del_{conversation['id']}",
                    on_click=self.delete_conversation,
                    args=(conversation['id'],))

    def set_last_retrieved_conversation(self, id: str, conversation: dict):
        """
        Set the last retrieved conversation
        """
        st.session_state.last_retrieved_conversation = dict(conversation)
        if "id" not in st.session_state.last_retrieved_conversation:
            st.session_state.last_retrieved_conversation["id"] = id

    def get_last_retrieved_conversation(self, id: str):
        """
        Get the last retrieved conversation. If "last_retrieved_conversation"
        entry is found and the id matches, return the buffered conversation.
        Otherwise, retrieve the conversation from the database.

        Args:
            id (str): The conversation ID.

        Returns:
            dict: The conversation dictionary, or None if not found.
        """
        if "last_retrieved_conversation" in st.session_state and \
           id == st.session_state.last_retrieved_conversation["id"]:
            conversation = dict(st.session_state.last_retrieved_conversation)
        else:
            conversation = self.get_conversation(id)
        if conversation:
            self.set_last_retrieved_conversation(id, conversation)
        return conversation

    def show_conversation_debug(self, conversation: dict):
        with st.expander("Detailed Response"):
            st.write(conversation)

    def show_cloud_resource(self, url: str, resource_type: str):
        if resource_type == "image":
            st.image(url)
        elif resource_type == "video":
            st.video(url)
        else:
            st.write(f"Not a video or image: {url}")

    def show_local_resource(self, url: str, resource_type: str):
        if resource_type in ["image", "video"]:
            return self.show_cloud_resource(url, resource_type)
        with open(url, "rb") as url:
            st.download_button(
                label="Download File",
                data=url,
                file_name=os.path.basename(url)
            )

    def verify_and_show_resource(self, url: str, resource_type: str):
        if is_an_url(url):
            self.show_cloud_resource(url, resource_type)
            return
        if not path_exists(url):
            st.write(f"ERROR E-IG-101: file not found: {url}")
        else:
            self.show_local_resource(url, resource_type)

    def show_conversation_content(
        self,
        id: str, container: st.container,
        additional_container: st.container
    ):
        """
        Show the conversation content
        """
        if not id:
            return
        conversation = self.get_last_retrieved_conversation(id)
        if not conversation:
            container.write("ERROR E-600: Conversation not found")
            return
        # log_debug(
        #     "SHOW_CONVERSATION_CONTENT | " +
        #     f"\n | conversation: {conversation}", debug=DEBUG
        # )
        if conversation.get('refined_prompt'):
            with additional_container.expander(
                 f"Enhanced Prompt for {conversation['type'].capitalize()}"):
                st.write(conversation['refined_prompt'])

        if conversation['type'] == "video":
            if conversation.get('answer'):
                # Check for list type entries, and show them individually
                if isinstance(conversation['answer'], list):
                    with container.container():
                        self.show_conversation_debug(conversation)
                        for url in conversation['answer']:
                            st.write(f"Video URL: {url}")
                            self.verify_and_show_resource(url, "video")
                else:
                    with container.container():
                        self.show_conversation_debug(conversation)
                        st.write(f"Video URL: {conversation['answer']}")
                        self.verify_and_show_resource(
                            conversation['answer'], "video")
            else:
                self.video_generation(
                    result_container=container,
                    question=conversation['question'],
                    previous_response=conversation['ttv_response'])

        elif conversation['type'] == "image":
            if conversation.get('answer'):
                # Check for list type entries, and show them individually
                if isinstance(conversation['answer'], list):
                    with container.container():
                        self.show_conversation_debug(conversation)
                        for url in conversation['answer']:
                            self.verify_and_show_resource(url, "image")
                else:
                    with container.container():
                        self.show_conversation_debug(conversation)
                        self.verify_and_show_resource(
                            conversation['answer'], "image")
            else:
                with container.container():
                    self.show_conversation_debug(conversation)
                    st.write("ERROR: No image found as answer")

        else:
            with container.container():
                self.show_conversation_debug(conversation)
                st.write(conversation['answer'])
                if conversation.get("subtype"):
                    if conversation["subtype"] in [
                        "generate_presentation",
                        "generate_app_presentation"
                    ]:
                        extra_button_text = ""
                        if conversation.get("presentation_file_path"):
                            extra_button_text = " again"
                        st.button(
                            f"Generate Presentation{extra_button_text}",
                            on_click=self.create_pptx,
                            args=(conversation,))
                        if conversation.get("presentation_file_path"):
                            self.verify_and_show_resource(
                                conversation["presentation_file_path"],
                                "other")

    def show_conversation_question(self, id: str):
        if not id:
            return
        conversation = self.get_last_retrieved_conversation(id)
        if not conversation:
            st.session_state.question = "ERROR E-700: Conversation not found"
        else:
            st.session_state.question = conversation['question']
            if conversation.get("form_data"):
                form_session_state_key = \
                    self.get_form_session_state_key(conversation)
                st.session_state[form_session_state_key] = \
                    conversation["form_data"]
                # log_debug("SHOW_CONVERSATION_QUESTION | "
                #           f"session_state_key: {form_session_state_key} | "
                #           "form_data: "
                #           f"{st.session_state[form_session_state_key]}",
                #           debug=DEBUG)

    def validate_question(self, question: str, assign_global: bool = True):
        """
        Validate the question
        """
        if not question:
            st.write("Please enter a question / prompt")
            return False
        # Update the user input in the conversation
        if assign_global:
            st.session_state.question = question
        return True

    # Data management

    def format_results(self, results: list):
        return "\n*".join(results)

    def attach_files(self, files):
        """
        Save the files to be attached to the LLM/model call
        """
        if "files_attached" not in st.session_state:
            st.session_state.files_to_attach = []
        if not files:
            return
        for file in files:
            if file:
                st.session_state.files_to_attach.append(file)

    def import_data(self, container: st.container):
        """
        Umport data from a uploaded JSON file into the database
        """

        def process_uploaded_file():
            """
            Process the uploaded file
            """
            uploaded_files = st.session_state.import_data_file
            st.session_state.dm_results = []
            with st.spinner(f"Processing {len(uploaded_files)} files..."):
                for uploaded_file in uploaded_files:
                    uploaded_file_path = uploaded_file.name
                    json_dict = json.loads(uploaded_file.getvalue())
                    db = self.init_db()
                    response = db.import_data(json_dict)
                    if response['error']:
                        item_result = f"File: {uploaded_file_path}" \
                                    f" | ERROR: {response['error_message']}"
                        log_debug(f"IMPORT_DATA | {item_result}", debug=DEBUG)
                        st.session_state.dm_results.append(item_result)
                        continue
                    item_result = f"File: {uploaded_file_path}" \
                                  f" | {response['result']}"
                    st.session_state.dm_results.append(item_result)

        container.file_uploader(
            "Choose a JSON file to perform the import",
            accept_multiple_files=True,
            type="json",
            on_change=process_uploaded_file,
            key="import_data_file",
        )

    def export_data(self, container: st.container):
        """
        Export data from the database and send it to the user as a JSON file
        """
        with st.spinner("Exporting data..."):
            db = self.init_db()
            response = db.export_data()
            if response['error']:
                container.write(f"ERROR {response['error_message']}")
                return
            container.download_button(
                label=f"{response['result']}. Click to download.",
                data=response['json'],
                file_name="data.json",
                mime="application/json",
            )

    def data_management_components(self):
        """
        Show data management components in the side bar
        """
        with st.expander("Data Management"):
            st.write("Import/export data with JSON files")
            sb_col1, sb_col2 = st.columns(2)
            with sb_col1:
                sb_col1.button(
                    "Import Data",
                    key="import_data")
            with sb_col2:
                sb_col2.button(
                    "Export Data",
                    key="export_data")

    # UI

    def get_title(self):
        """
        Returns the title of the app
        """
        return (f"{st.session_state.app_name_version}"
                f" {st.session_state.app_icon}")

    def show_button_of_type(self, button_config: dict, extra_kwargs: dict,
                            container: Any):
        """
        Show a button based on the button_config
        Args:
            button_config (dict): button configuration
                {
                    "text": "Answer Question",
                    "key": "generate_text",
                    "enable_config_name": "GENERATE_TEXT_ENABLED",
                    "type": "checkbox",
                }
        """
        submitted = None
        button_type = button_config.get("type", "button")
        if button_type == "checkbox":
            submitted = container.checkbox(
                button_config["text"],
                key=button_config["key"],
                **extra_kwargs)
        elif button_type == "spacer":
            container.write(button_config.get("text", ""))
        elif button_type == "submit":
            submitted = container.form_submit_button(
                button_config["text"])
        else:
            # Defaults to button
            submitted = container.button(
                button_config["text"],
                key=button_config["key"],
                **extra_kwargs)
        return submitted

    def show_buttons_row(
        self,
        buttons_config: list,
        fill_missing_spaces: bool = False
    ):
        """
        Show buttons based on the buttons_config
        Args:
            buttons_config (listo): list of buttons configurations
                [
                    # Button example with enable config
                    {
                        "text": "Answer Question",
                        "key": "generate_text",
                        "enable_config_name": "TEXT_GENERATION_ENABLED",
                    },
                    # Button example with a function and no enable config
                    {
                        "text": "Enhance prompt",
                        "key": "prompt_enhancement",
                        "on_change": cgsl.prompt_enhancement
                    },

        Returns:
            None
        """
        col = st.columns(len(buttons_config))
        col_index = 0
        submitted = []
        for button in buttons_config:
            extra_kwargs = {}
            for key in ["on_change", "on_click", "args"]:
                if button.get(key, None):
                    extra_kwargs[key] = button[key]
            if button.get("enable_config_name", None):
                with col[col_index]:
                    if self.get_par_value(button["enable_config_name"], True):
                        submitted.append(
                            self.show_button_of_type(
                                button,
                                extra_kwargs,
                                col[col_index]))
                        col_index += 1
                    else:
                        if fill_missing_spaces:
                            st.write("")
                            col_index += 1
            else:
                with col[col_index]:
                    submitted.append(
                        self.show_button_of_type(
                            button,
                            extra_kwargs,
                            col[col_index]))
                    col_index += 1
        return submitted

    def get_buttons_submitted_data(self, buttons_submitted: list,
                                   buttons_data: dict,
                                   submit_button_verification: bool = True):
        """
        Reduce the list of buttons submitted to a single boolean value
        to determine if the form was submitted
        """
        submitted = any(buttons_submitted)

        # log_debug(f"buttons_submitted: {buttons_submitted}", debug=DEBUG)

        buttons_submitted_data = {}
        if submitted:
            # Get the button submitted values
            # and create a dictionary with the form data

            curr_item = 0
            for i in range(len(buttons_data)):
                if not submit_button_verification or \
                   buttons_data[i].get("type") == "submit":
                    if buttons_data[i].get("enable_config_name", None):
                        if self.get_par_value(
                                buttons_data[i]["enable_config_name"], True):
                            # log_debug(f"buttons_data[{i}]: {buttons_data[i]}"
                            #     f" -> {buttons_submitted[curr_item]}",
                            buttons_submitted_data[buttons_data[i]["key"]] = \
                                buttons_submitted[curr_item]
                            curr_item += 1
                    else:
                        # log_debug(f"buttons_data[{i}]: {buttons_data[i]} "
                        #     f"-> {buttons_submitted[curr_item]}",
                        #     debug=DEBUG)
                        buttons_submitted_data[buttons_data[i]["key"]] = \
                            buttons_submitted[curr_item]
                        curr_item += 1
        return buttons_submitted_data

    def get_option_index(self, options: list, value: str):
        """
        Returns the index of the option in the list
        """
        for i, option in enumerate(options):
            if option == value:
                return i
        return 0

    def show_form_fields(self, fields_data: dict, form_data: dict):
        """
        Show the form
        """
        fields_values = {}
        for key in fields_data:
            field = fields_data.get(key)
            if not field.get("enabled", True):
                continue
            value = form_data.get(key, "")
            if field.get("type") == "selectbox":
                field_value = st.selectbox(
                    field.get("title"),
                    field.get("options", []),
                    # key=key,  # If this is set, the value is not assigned
                    help=field.get("help"),
                    index=self.get_option_index(
                        options=field.get("options", []),
                        value=value),
                )
            elif field.get("type") == "radio":
                field_value = st.radio(
                    field.get("title"),
                    field.get("options", []),
                    # key=key,  # If this is set, the value is not assigned
                    help=field.get("help"),
                    index=self.get_option_index(
                        options=field.get("options", []),
                        value=value),
                )
            elif field.get("type") == "text":
                field_value = st.text_input(
                    field.get("title"),
                    value,
                    # key=key,  # If this is set, the value is not assigned
                    help=field.get("help"),
                )
            else:
                field_value = st.text_area(
                    field.get("title"),
                    value,
                    # key=key,  # If this is set, the value is not assigned
                    help=field.get("help"),
                )
            fields_values[key] = field_value
        return fields_values

    def show_form_error(self, message: str):
        """
        Show a form submission error
        """
        show_popup(
            title="The following error(s) were found:",
            message=message,
            msg_type="error")

    def add_buttons_and_return_submitted(self, buttons_config: list):
        """
        Add the buttons to the page, then returns the submitted buttons and the
        buttons configuration
        """
        with st.container():
            submitted = self.show_buttons_row(buttons_config)
            return submitted, buttons_config

    def get_selected_feature(self, form: dict, features_data: dict):
        """
        Returns the selected feature
        """
        log_debug(f"get_selected_feature | form: {form}", debug=DEBUG)
        log_debug(f"get_selected_feature | features_data: {features_data}",
                  debug=DEBUG)
        selected_feature = None
        for key in form.get("buttons_submitted_data"):
            for feature in features_data:
                if form["buttons_submitted_data"].get(key) and \
                        key == feature:
                    selected_feature = feature
                    break
        return selected_feature

    def get_form_name(self, form_config: dict):
        """
        Returns the form session state key
        """
        form_name = form_config.get("name", "application_form")
        return f"{form_name}"

    def get_form_session_state_key(self, form_config: dict):
        """
        Returns the form session state key
        """
        form_name = self.get_form_name(form_config)
        form_session_state_key = form_config.get(
            "form_session_state_key",
            f"{form_name}_data")
        return form_session_state_key

    def show_form(self, form_config: dict):
        """
        Show the configured form
        """
        form_name = self.get_form_name(form_config)
        form_session_state_key = self.get_form_session_state_key(form_config)
        if form_session_state_key not in st.session_state:
            st.session_state[form_session_state_key] = {}
        form_data = st.session_state[form_session_state_key]

        fields_data = form_config.get("fields", {})

        # Clear the form data if it's not the first time the form is shown
        if form_name in st.session_state:
            del st.session_state[form_name]

        with st.form(form_name):
            st.title(form_config.get("title", "Application Form"))

            if form_config.get("subtitle"):
                st.write(form_config.get("subtitle"))

            fields_values = self.show_form_fields(fields_data, form_data)

            if form_config.get("suffix"):
                st.write(form_config.get("suffix"))

            func = form_config.get(
                "buttons_function",
                self.add_buttons_and_return_submitted)
            if form_config.get("buttons_config"):
                buttons_submitted, buttons_data = func(
                    form_config["buttons_config"])
            else:
                buttons_submitted, buttons_data = [], []

        buttons_submitted_data = self.get_buttons_submitted_data(
            buttons_submitted,
            buttons_data)
        if not buttons_submitted_data:
            return None

        st.session_state[form_session_state_key] = dict(fields_values)
        st.session_state[form_session_state_key].update({
            "buttons_submitted_data": buttons_submitted_data
        })
        return st.session_state[form_session_state_key]

    # No-form processing
    def process_no_form_buttons(
        self,
        forms_config_name: str,
        question: str,
        process_form_func: Callable,
        submit_form_func: Callable
    ):
        """
        Process No-Form buttons, like the ones for the
        the ideation-from-prompt feature
        """

        ideation_from_prompt_config = \
            st.session_state.forms_config[forms_config_name]
        ideation_from_prompt_buttons_config = \
            ideation_from_prompt_config.get("buttons_config")
        i = 0
        buttons_submitted = []
        process_form = False
        for button in ideation_from_prompt_buttons_config:
            button_was_clicked = True if st.session_state.get(button["key"]) \
                                else False
            if button_was_clicked:
                process_form = True
            buttons_submitted.append(button_was_clicked)
            i += 1
        data = {
            "buttons_submitted": buttons_submitted,
            "question": question,
        }
        if process_form:
            form = process_form_func(None, "process_form", data)
            if not question:
                self.show_form_error("No question / prompt to process")
            else:
                # Assign here the question to the session state because
                # the question assignment in the process_form_func()
                # when it call the llm is suppressed, to preserve the
                # original question
                st.session_state.question = question
                submit_form_func(
                    form,
                    ideation_from_prompt_config)

    # PPTX generation

    def create_pptx(self, conversation: dict):
        """
        Generates the PowerPoint slides
        """
        log_debug("CREATE_PPTX | enters...", debug=DEBUG)
        pptx_generator = PowerPointGenerator({
            "output_dir": self.params.get("output_dir", "./output"),
            "file_name": uuid.uuid4(),
        })
        answer = conversation.get("answer")
        if not answer:
            error_message = "Conversation answer is empty"
            log_debug(f"CREATE_PPTX | ERROR 1: {error_message}...",
                      debug=DEBUG)
            st.write(error_message)
            return
        if "```json" in answer:
            # Find the first occurrence of ```json, cut the text before it,
            # and remove the ```json and ``` characters
            answer = answer.split("```json")[1].replace("```json", "")
            answer = answer.replace("```", "")
        try:
            log_debug("CREATE_PPTX | answer:"
                      f" {answer}", debug=DEBUG)
            slides_config = json.loads(answer)
            log_debug("CREATE_PPTX | slides_config:"
                      f" {slides_config}", debug=DEBUG)
        except Exception as e:
            error_message = f"ERROR {e}"
            st.write(error_message)
            log_debug(f"CREATE_PPTX | ERROR 2: {error_message}...",
                      debug=DEBUG)
            return

        log_debug("CREATE_PPTX | creating presentation...", debug=DEBUG)

        result_file_path = pptx_generator.generate(slides_config)

        log_debug("CREATE_PPTX | result_file_path: "
                  f"{result_file_path}", debug=DEBUG)

        conversation["presentation_file_path"] = result_file_path
        self.update_conversation(conversation, conversation["id"])
        return result_file_path

    # AI

    def get_available_ai_providers(
        self,
        param_name: str,
        param_values: dict = None
    ) -> list:
        """
        Returns the available LLM providers based on the environment variables
        The model will be available if all its variables are set
        """
        if not param_values:
            param_values = os.environ
        result = []
        for model_name, model_attr in self.get_par_value(param_name).items():
            # log_debug(f"get_available_ai_providers | "
            #           '\nmodel_attr.get("active", True): '
            #           f'{model_attr.get("active", True)}'
            #           f"\nmodel_name: {model_name} | "
            #           f"\nmodel_attr: {model_attr}",
            #           debug=DEBUG)
            if not model_attr.get("active", True):
                continue
            model_to_add = model_name
            requirements = model_attr.get("requirements", [])
            for var_name in requirements:
                if not param_values.get(var_name):
                    model_to_add = None
                    break
            if model_to_add:
                result.append(model_to_add)
        return result

    def get_llm_provider(
        self,
        param_name: str,
        session_state_key: str
    ):
        """
        Returns the LLM provider
        """
        if session_state_key not in st.session_state:
            # return self.get_par_value(param_name)[0]
            provider_list = self.get_available_ai_providers(param_name)
            if not provider_list:
                return ''
            return provider_list[0]
        return st.session_state.get(session_state_key)

    def get_llm_model(
        self,
        parent_param_name: str,
        parent_session_state_key: str,
        param_name: str,
        session_state_key: str
    ):
        """
        Returns the LLM model
        """
        if session_state_key not in st.session_state:
            llm_provider = self.get_llm_provider(
                parent_param_name, parent_session_state_key)
            if not llm_provider:
                return None
            llm_models = self.get_par_value(
                param_name).get(llm_provider, [])
            if not llm_models:
                return None
            return llm_models[0]
        return st.session_state.get(session_state_key)

    def get_model_options(
        self,
        parent_param_name: str,
        parent_session_state_key: str,
        param_name: str,
    ):
        """
        Returns the model options for the LLM call
        """
        llm_provider = self.get_llm_provider(
            parent_param_name, parent_session_state_key)
        if not llm_provider:
            return []
        return self.get_par_value(param_name, {}).get(llm_provider, [])

    def get_llm_provider_index(
        self,
        param_name: str,
        session_state_key: str
    ):
        available_llm_providers = self.get_available_ai_providers(param_name)
        try:
            llm_provider_index = available_llm_providers.index(
                self.get_llm_provider(
                    param_name,
                    session_state_key
                ))
        except ValueError:
            llm_provider_index = 0
        return llm_provider_index

    def get_llm_model_index(
        self,
        parent_param_name: str,
        parent_session_state_key: str,
        param_name: str,
        session_state_key: str
    ):
        # log_debug(
        #   f">> get_llm_model_index:"
        #   f"\n | parent_param_name: {parent_param_name}"
        #   f"\n | parent_session_state_key: {parent_session_state_key}"
        #   f"\n | param_name: {param_name}"
        #   f"\n | session_state_key: {session_state_key}",
        #   debug=DEBUG)
        available_llm_models = self.get_model_options(
            parent_param_name,
            parent_session_state_key,
            param_name
        )
        selected_llm_model = self.get_llm_model(
            parent_param_name,
            parent_session_state_key,
            param_name,
            session_state_key
        )
        # log_debug(f">> get_llm_model_index: "
        #           "\n | available_llm_models: "
        #           f"{available_llm_models}"
        #           "\n | selected_llm_model: "
        #           f"{selected_llm_model}", debug=DEBUG)
        try:
            llm_model_index = available_llm_models.index(
                selected_llm_model)
        except ValueError:
            llm_model_index = 0
        # log_debug(f">> get_llm_model_index | llm_model_index: "
        #           f"{llm_model_index}", debug=DEBUG)
        return llm_model_index

    def set_session_flag(self, session_state_key: str,
                         flag_session_state_key: str):
        flag = False
        if session_state_key in st.session_state:
            if st.session_state.get(session_state_key):
                flag = True
        st.session_state[flag_session_state_key] = flag

    def get_model_configurations(self):
        """
        Returns the model configurations
        """
        model_configurations = {}
        for key in st.session_state:
            if key.startswith("model_config_par_"):
                par_name = key.replace("model_config_par_", "")
                model_configurations[par_name] = st.session_state[key]
        return model_configurations

    def get_llm_text_model(self, model_replacement: dict = None):
        """
        Returns the LLM text model
        """
        llm_parameters = {
            "llm_providers_complete_list":
                # self.get_par_value("LLM_PROVIDERS_COMPLETE_LIST"),
                self.get_par_value("LLM_PROVIDERS", {}).keys(),
            "no_system_prompt_allowed_providers":
                self.get_par_value("NO_SYSTEM_PROMPT_ALLOWED_PROVIDERS"),
            "no_system_prompt_allowed_models":
                self.get_par_value("NO_SYSTEM_PROMPT_ALLOWED_MODELS"),
            "llm_model_forced_values":
                self.get_par_value("LLM_MODEL_FORCED_VALUES"),
            "llm_model_params_naming":
                self.get_par_value("LLM_MODEL_PARAMS_NAMING"),
        }
        log_debug("GET_LLM_TEXT_MODEL | llm_parameters # 1: "
                  f"{llm_parameters}", debug=DEBUG)

        llm_parameters.update(self.get_model_configurations())
        log_debug("GET_LLM_TEXT_MODEL | llm_parameters # 2: "
                  f"{llm_parameters}", debug=DEBUG)

        result = get_default_resultset()
        result["llm_provider"] = self.get_llm_provider(
            "LLM_PROVIDERS",
            "llm_provider"
        )
        result["llm_model"] = self.get_llm_model(
            "LLM_PROVIDERS", "llm_provider",
            "LLM_AVAILABLE_MODELS", "llm_model"
        )
        if not result["llm_provider"]:
            result["error"] = True
            result["error_message"] = "LLM Provider not selected"
        elif not result["llm_model"]:
            result["error"] = True
            result["error_message"] = "LLM Model not selected"
        else:
            if model_replacement:
                # To avoid use the OpenAI reasoning models in the suggestions
                result["llm_model"] = model_replacement.get(
                    result["llm_model"], result["llm_model"])
            # The llm parameters will be available in the LLM class
            llm_parameters["provider"] = result["llm_provider"]
            llm_parameters["model_name"] = result["llm_model"]
            result["class"] = LlmProvider(llm_parameters)
        return result

    def text_generation(self, result_container: st.container,
                        question: str = None, other_data: dict = None,
                        settings: dict = None):
        if not other_data:
            other_data = {}
        if not settings:
            settings = {}
        if not question:
            question = st.session_state.question
        if not self.validate_question(question, settings.get("assign_global")):
            return
        llm_text_model_elements = self.get_llm_text_model()
        if llm_text_model_elements['error']:
            result_container.write(
                f"ERROR E-100-A: {llm_text_model_elements['error_message']}")
            return
        other_data.update({
            "ai_provider": llm_text_model_elements['llm_provider'],
            "ai_model": llm_text_model_elements['llm_model'],
        })
        with st.spinner("Procesing text generation..."):
            # Generating answer
            llm_text_model = llm_text_model_elements['class']
            if "system_prompt" in other_data:
                prompt = other_data["system_prompt"]
            else:
                prompt = "{question}"
            response = llm_text_model.query(
                prompt, question,
                (self.get_par_value("REFINE_LLM_PROMPT_TEXT") if
                 st.session_state.prompt_enhancement_flag else None)
            )
            if response['error']:
                other_data["error_message"] = (
                    f"ERROR E-100: {response['error_message']}")
            self.save_conversation(
                type="text",
                question=question,
                refined_prompt=response.get('refined_prompt'),
                answer=response.get('response'),
                other_data=other_data,
            )
            # result_container.write(response['response'])
            st.rerun()

    def image_generation(self, result_container: st.container,
                         question: str = None,
                         settings: dict = None):
        if not settings:
            settings = {}
        if not question:
            question = st.session_state.question
        if not self.validate_question(question, settings.get("assign_global")):
            return
        llm_provider = self.get_llm_provider(
            "TEXT_TO_IMAGE_PROVIDERS",
            "image_provider"
        )
        llm_model = self.get_llm_model(
            "TEXT_TO_IMAGE_PROVIDERS", "image_provider",
            "TEXT_TO_IMAGE_AVAILABLE_MODELS", "image_model"
        )
        llm_text_model_elements = self.get_llm_text_model()
        if llm_text_model_elements['error']:
            result_container.write(
                f"ERROR E-100-B: {llm_text_model_elements['error_message']}")
            return
        other_data = {
            "ai_provider": llm_provider,
            "ai_model": llm_model,
            "ai_text_model_provider": llm_text_model_elements['llm_provider'],
            "ai_text_model_model": llm_text_model_elements['llm_model'],
        }
        with st.spinner("Procesing image generation..."):
            model_params = {
                # "provider": self.get_par_or_env("TEXT_TO_IMAGE_PROVIDER"),
                "provider": llm_provider,
                "model_name": llm_model,
                "text_model_class": llm_text_model_elements['class'],
            }
            model_params.update(self.get_model_configurations())

            llm_model = ImageGenProvider(model_params)

            response = llm_model.image_gen(
                question,
                (self.get_par_value("REFINE_LLM_PROMPT_TEXT") if
                 st.session_state.prompt_enhancement_flag else None)
            )
            if response['error']:
                # result_container.write(
                #     f"ERROR E-IG-100: {response['error_message']}")
                other_data["error_message"] = (
                    f"ERROR E-IG-100: {response['error_message']}")
            self.save_conversation(
                type="image",
                question=question,
                refined_prompt=response.get('refined_prompt'),
                answer=response.get('response'),
                other_data=other_data,
            )
            # result_container.write(response['response'])
            st.rerun()

    def video_generation(
        self,
        result_container: st.container,
        question: str = None,
        previous_response: dict = None,
        settings: dict = None
    ):
        if not settings:
            settings = {}
        llm_provider = self.get_llm_provider(
            "TEXT_TO_VIDEO_PROVIDERS",
            "video_provider"
        )
        llm_model = self.get_llm_model(
            "TEXT_TO_VIDEO_PROVIDERS", "video_provider",
            "TEXT_TO_VIDEO_AVAILABLE_MODELS", "video_model"
        )

        llm_text_model_elements = self.get_llm_text_model()
        if llm_text_model_elements['error']:
            result_container.write(
                f"ERROR E-100-C: {llm_text_model_elements['error_message']}")
            return

        model_params = {
            # "provider": self.get_par_or_env("TEXT_TO_VIDEO_PROVIDER"),
            "provider": llm_provider,
            "model_name": llm_model,
            "text_model_class": llm_text_model_elements['class'],
        }
        model_params.update(self.get_model_configurations())
        ttv_model = TextToVideoProvider(model_params)

        if previous_response:
            response = previous_response.copy()
            video_id = response['id']
        else:
            video_id = get_new_item_id()
            if not question:
                question = st.session_state.question
            if not self.validate_question(question,
               settings.get("assign_global")):
                return
            with st.spinner("Requesting the video generation..."):
                # Requesting the video generation
                response = ttv_model.video_gen(
                    question,
                    (self.get_par_value("REFINE_VIDEO_PROMPT_TEXT") if
                     st.session_state.prompt_enhancement_flag else None)
                )
                if response['error']:
                    result_container.write(
                        f"ERROR E-200: {response['error_message']}")
                    return

        with st.spinner("Procesing video generation. It can take"
                        " 2+ minutes..."):
            #  Checking the video generation status
            video_url = None
            ttv_response = response.copy()
            ttv_response['id'] = video_id

            # Save a preliminar conversation with the video generation request
            # follow-up data in the ttv_response attribute
            other_data = {
                "ttv_response": ttv_response,
                "ai_provider": llm_provider,
                "ai_model": llm_model,
                "ai_text_model_provider":
                    llm_text_model_elements['llm_provider'],
                "ai_text_model_model": llm_text_model_elements['llm_model'],
            }
            self.save_conversation(
                type="video",
                question=question,
                refined_prompt=ttv_response.get('refined_prompt'),
                answer=video_url,
                other_data=other_data,
                id=video_id,
            )

            response = ttv_model.video_gen_followup(ttv_response)
            if response['error']:
                other_data["error_message"] = (
                    f"ERROR E-300: {response['error_message']}")
            elif response.get("video_url"):
                video_url = response["video_url"]
            else:
                other_data["error_message"] = (
                    "ERROR E-400: Video generation failed."
                    " No video URL. Try again later by clicking"
                    " the corresponding previous answer.")
                if response.get("ttv_followup_response"):
                    other_data["ttv_followup_response"] = \
                        response["ttv_followup_response"]

            if previous_response and other_data.get("error_message"):
                result_container.warning(other_data["error_message"])
                return

            # Save the conversation with the video generation result
            self.save_conversation(
                type="video",
                question=question,
                refined_prompt=ttv_response.get('refined_prompt'),
                answer=video_url,
                other_data=other_data,
                id=video_id,
            )

            if previous_response:
                result_container.video(video_url)
            else:
                st.rerun()

    # Gallery management

    def get_item_urls(self, item_type: str) -> dict:
        """
        Returns a list of video URLs
        Args:
            item_type (str): The type of item to get the URLs for.
                E.g. "video" or "image".
        Returns:
            dict: A standard response dictionary with a "urls" key, which is
                a list of URLs. Also includes a "error" and "error_message"
                keys to report any errors that occurred.
        """
        response = get_default_resultset()
        response['urls'] = []
        for conversation in st.session_state.conversations:
            if conversation['type'] == item_type:
                if conversation.get('answer'):
                    # Check for list type entries, and add them individually
                    # to the list so all entries must be strings urls
                    if isinstance(conversation['answer'], list):
                        for url in conversation['answer']:
                            response['urls'].append(url)
                    else:
                        response['urls'].append(conversation['answer'])
        return response

    def show_gallery(self, galley_type: str):
        """
        Show the gallery of videos or images
        """
        galley_type = galley_type.replace("_gallery", "").lower()
        gdata = {
            "video": {
                "title": "Video Gallery",
                "name": "videos",
                "type": "video",
            },
            "image": {
                "title": "Image Gallery",
                "name": "images",
                "type": "image",
            },
        }
        if not gdata.get(galley_type):
            return

        title = gdata[galley_type].get("title")
        name = gdata[galley_type].get("name")
        item_type = gdata[galley_type].get("type")

        head_col1, head_col2 = st.columns(
            2, gap="small",
            vertical_alignment="bottom")
        with head_col1:
            head_col1.title(
                self.get_title())
            head_col1.write(title)
        with head_col2:
            head_col2.button(
                f"Generate {name.capitalize()}",
                key="go_to_text_generation",
                on_click=self.set_query_param,
                args=("page", "home"),
            )

        # Define video URLs
        item_urls = self.get_item_urls(item_type)
        if not item_urls['urls']:
            st.write(f"** No {name} found. Try again later. **")
            return

        # Display videos in a 3-column layout
        columns = self.get_par_value(f"{item_type.upper()}_GALLERY_COLUMNS", 3)
        cols = st.columns(columns)
        for i, item_url in enumerate(item_urls['urls']):
            with cols[i % columns]:
                if item_type == "video":
                    st.video(item_url)
                elif item_type == "image":
                    st.image(item_url)

    # General functions

    def get_par_value(self, param_name: str, default_value: str = None):
        """
        Returns the parameter value. If the parameter value is a file path,
        it will be read and returned.
        """
        result = self.params.get(param_name, default_value)
        if result and isinstance(result, str) and result.startswith("[") \
           and result.endswith("]"):
            result = read_file(f"config/{result[1:-1]}")
        return result

    def get_par_or_env(self, param_name: str, default_value: str = None):
        """
        Returns the parameter value or the environment variable value
        """
        if os.environ.get(param_name):
            return os.environ.get(param_name)
        return self.get_par_value(param_name, default_value)

    def add_js_script(self, source: str):
        """
        Add a JS script to the page
        """
        # Reference:
        # Injecting JS?
        # https://discuss.streamlit.io/t/injecting-js/22651/5?u=carlos9
        # The following snippet could help you solve your cross-origin issue:
        div_id = uuid.uuid4()
        st.markdown(f"""
            <div style="display:none" id="{div_id}">
                <iframe src="javascript: \
                    var script = document.createElement('script'); \
                    script.type = 'text/javascript'; \
                    script.text = {html.escape(repr(source))}; \
                    var div = window.parent.document."""
                    """getElementById('{div_id}'); \
                    div.appendChild(script); \
                    div.parentElement.parentElement.parentElement."""
                    """style.display = 'none'; \
                "/>
            </div>
        """, unsafe_allow_html=True)
</file>

<file path="lib/codegen_utilities.py">
"""
General utilities
"""
from typing import Any
import time
import os
import json

import uuid
import requests


DEBUG = False


def log_debug(message: Any, debug: bool = DEBUG) -> None:
    """
    Log a debug message if the DEBUG flag is set to True
    """
    if debug:
        print("")
        print(f"DEBUG {time.strftime('%Y-%m-%d %H:%M:%S')}: {message}")


def get_default_resultset() -> dict:
    """
    Returns a default resultset
    """
    return {
        "resultset": {},
        "error_message": "",
        "error": False,
    }


def error_resultset(
    error_message: str,
    message_code: str = ''
) -> dict:
    """
    Return an error resultset.
    """
    message_code = f" [{message_code}]" if message_code else ''
    result = get_default_resultset()
    result['error'] = True
    result['error_message'] = f"{error_message}{message_code}"
    return result


def get_date_time(timestamp: int):
    """
    Returns a formatted date and time
    """
    return time.strftime("%Y-%m-%d %H:%M:%S",
                         time.localtime(timestamp))


def get_new_item_id():
    """
    Get the new unique item id
    """
    return str(uuid.uuid4())


def read_file(file_path, params: dict = None):
    """
    Reads a file and returns its content.
    If the file_path is a URL, it will be downloaded
    If the file_path is a local file, it will be read
    if params.get("save_file") is True, it will be saved in the output_dir
    and return file_path will enclosed by [] to indicate the saved file path
    (e.g. [./output/file_name.txt])
    """
    if not params:
        params = {}
    # If the file path begins with "http", it's a URL
    if file_path.startswith("http"):
        # If the file path begins with "https://github.com",
        # we need to replace it with "https://raw.githubusercontent.com"
        # to get the raw content
        if file_path.startswith("https://github.com"):
            file_path = file_path.replace(
                "https://github.com",
                "https://raw.githubusercontent.com")
            file_path = file_path.replace("blob/", "")
        response = requests.get(file_path)
        if response.status_code == 200:
            content = response.text
        else:
            raise ValueError(f"Error reading file: {file_path}")
    else:
        with open(file_path, 'r') as f:
            content = f.read()

    # Save the file if requested by the "save_file" parameter
    # and return the file path enclosed by []
    if params.get("save_file"):
        # "./output" is the default output directory if the output_dir
        # parameter is not provided
        output_dir = params.get("output_dir", "./output")
        if params.get("file_name"):
            file_name = params.get("file_name")
        else:
            file_name = os.path.basename(file_path)
        target_file_path = save_file(output_dir, file_name, content)
        return f"[{target_file_path}]"
    return content


def is_an_url(element_url_or_path: str):
    """ Returns True if the string is an URL"""
    return element_url_or_path.startswith(
        ("http://", "https://", "ftp://", "file://")
    )


def path_exists(file_path: str):
    """
    Creates the output directory if it doesn't exist
    """
    if is_an_url(file_path):
        return None
    return os.path.exists(file_path)


def create_dirs(output_dir: str):
    """
    Creates the output directory if it doesn't exist
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)


def save_file(output_dir: str, file_name: str, content: Any):
    """
    Saves the file to the output directory
    """
    # If the output_dir path does not exist, create it
    create_dirs(output_dir)
    target_file_path = f"{output_dir}/{file_name}"
    log_debug(f"READ_FILE | Saving file: {target_file_path}", debug=DEBUG)
    with open(target_file_path, 'w') as f:
        f.write(content)
        f.close()
    return target_file_path


def read_config_file(file_path: str):
    """
    Reads a JSON file and returns its content as a dictionary
    """
    with open(file_path, 'r') as f:
        config = json.load(f)
    return config


def get_app_config(config_file_path: str = None):
    """
    Returns the app configuration
    """
    if not config_file_path:
        config_file_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)),
            "../config/app_config.json")
    return read_config_file(config_file_path)
</file>

<file path="output/.gitignore">
*
!.gitignore
</file>

<file path="scripts/run_app.sh">
#!/bin/bash
set -e
# set -o allexport ; . .env ; set +o allexport ;

create_venv() {
	if [ ! -d venv ]; then
        python3 -m venv venv
    fi
	if [ -d venv ]; then
        if ! source venv/bin/activate
        then
            if ! . venv/bin/activate
            then
                echo "Error activating virtual environment"
                exit 1
            fi
        fi
    fi
}

install() {
    create_venv
	if [ -f requirements.txt ]; then
        pip install -r requirements.txt;
    fi
	if [ ! -f requirements.txt ]; then
        pip install --upgrade pip;
        # Dependencies needed for the Agent:
        # fastapi, uvicorn, pydantic, pydantic, pydantic, supabase, asyncpg, nest_asyncio, python-dotenv, llama-index
        # Dependencies needed for the UI:
        # streamlit, requests, python-dotenv, pymongo, python-pptx, openai, ollama, groq, together, llama-index
        if ! pip install \
            fastapi \
            uvicorn \
            pydantic \
            pydantic-ai \
            pydantic-ai[logfire] \
            supabase \
            asyncpg \
            nest_asyncio \
            streamlit \
            requests \
            python-dotenv \
            pymongo \
            python-pptx \
            openai \
            ollama \
            groq \
            together \
            llama-index;
        pip freeze > requirements.txt;
    fi
}

requirements() {
    install
}

run() {
    install
	streamlit run streamlit_app.py
}

ACTION=$1

case $ACTION in
    "install")
        install
        ;;
    "requirements")
        requirements
        ;;
    "run")
        run
        ;;
    *)
        echo "Usage: run_app.sh <install|requirements|run>"
        exit 1
        ;;
esac
</file>

<file path="scripts/run_schema_generator.sh">
#!/bin/bash
# run_schema_generator.sh
# 2024-10-27 | CR
#
set -o allexport ; . .env ; set +o allexport ;
REPO_BASEDIR="`pwd`"
cd "`dirname "$0"`"
SCRIPTS_DIR="`pwd`"
#
if [ ! -d "venv" ]; then
    python -m venv venv
fi
source venv/bin/activate
if [ -f requirements.txt ]; then
    pip install -r requirements.txt
else
    pip install requests ollama openai groq
    pip freeze > requirements.txt
fi

if [ "${CODEGEN_AI_PROVIDER}" = "" ]; then
    CODEGEN_AI_PROVIDER="${LANGCHAIN_DEFAULT_MODEL}"
fi
if [ "${CODEGEN_AI_PROVIDER}" = "" ]; then
    echo "ERROR: CODEGEN_AI_PROVIDER not set"
    exit 1
fi

if [ "${USER_INPUT}" = "" ]; then
    USER_INPUT="./user_input_example.md"
fi

PARAMETERS=""

# Ollama specific parameters
if [ "${OLLAMA_BASE_URL}" != "" ]; then
    PARAMETERS="${PARAMETERS} --ollama_base_url ${OLLAMA_BASE_URL}"
fi
if [ "${OLLAMA_TEMPERATURE}" != "" ]; then
    PARAMETERS="${PARAMETERS} --temperature ${OLLAMA_TEMPERATURE}"
fi
if [ "${OLLAMA_STREAM}" != "" ]; then
    PARAMETERS="${PARAMETERS} --stream ${OLLAMA_STREAM}"
fi

# Model specific parameters

if [ "${CODEGEN_AI_MODEL}" != "" ]; then
    PARAMETERS="${PARAMETERS} --model ${CODEGEN_AI_MODEL}"
else
    if [ "${CODEGEN_AI_PROVIDER}" = "ollama" ]; then
        if [ "${OLLAMA_MODEL}" != "" ]; then
            PARAMETERS="${PARAMETERS} --model ${OLLAMA_MODEL}"
        fi
    fi
    if [ "${CODEGEN_AI_PROVIDER}" = "nvidia" ]; then
        if [ "${NVIDIA_MODEL}" != "" ]; then
            PARAMETERS="${PARAMETERS} --model ${NVIDIA_MODEL}"
        fi
    fi
    if [ "${CODEGEN_AI_PROVIDER}" = "chat_openai" ]; then
        if [ "${OPENAI_MODEL}" != "" ]; then
            PARAMETERS="${PARAMETERS} --model ${OPENAI_MODEL}"
        fi
    fi
    if [ "${CODEGEN_AI_PROVIDER}" = "openai" ]; then
        if [ "${OPENAI_MODEL}" != "" ]; then
            PARAMETERS="${PARAMETERS} --model ${OPENAI_MODEL}"
        fi
    fi
    if [ "${CODEGEN_AI_PROVIDER}" = "groq" ]; then
        if [ "${GROQ_MODEL}" != "" ]; then
            PARAMETERS="${PARAMETERS} --model ${GROQ_MODEL}"
        fi
    fi
    if [ "${CODEGEN_AI_PROVIDER}" = "aria" ]; then
        if [ "${RHYMES_ARIA_MODEL}" != "" ]; then
            PARAMETERS="${PARAMETERS} --model ${RHYMES_ARIA_MODEL}"
        fi
    fi
fi

# Other parameters
if [ "${AGENTS_COUNT}" != "" ]; then
    PARAMETERS="${PARAMETERS} --agents_count ${AGENTS_COUNT}"
fi

echo ""
echo "Executing: python schema_generator.py --user_input \"${USER_INPUT}\" --provider ${CODEGEN_AI_PROVIDER} ${PARAMETERS}"
pwd
echo ""
python lib/codegen_schema_generator.py --user_input "${USER_INPUT}" --provider ${CODEGEN_AI_PROVIDER} ${PARAMETERS}

deactivate
if [ "${CODEGEN_KEEP_VENV}" != "1" ]; then
    rm -rf venv
fi
</file>

<file path="src/codegen_app_ideation.py">
import streamlit as st

from lib.codegen_streamlit_lib import StreamlitLib
from lib.codegen_utilities import get_app_config, log_debug
from lib.codegen_app_ideation_lib import (
    get_ideation_form_config,
    get_ideation_from_prompt_config,
)

DEBUG = False

app_config = get_app_config()
cgsl = StreamlitLib(app_config)


def show_ideation_form(container: st.container):
    """
    Returns the ideation form
    """
    form_config = get_ideation_form_config()
    with container.expander("From Application Ideation Form"):
        form_result = cgsl.show_form(form_config)
    return form_result


def show_ideation_from_prompt(container: st.container, mode: str,
                              data: dict = None):
    """
    Returns the buttons for the ideation from the question (prompt)
    """
    if not data:
        data = {}
    form_config = get_ideation_from_prompt_config()
    form_session_state_key = form_config.get(
        "form_session_state_key")
    buttons_config = form_config.get("buttons_config")

    if mode == "show_form":
        with container.expander("From Prompt"):
            st.title(form_config.get("title", "Application Form"))
            if form_config.get("subtitle"):
                st.write(form_config.get("subtitle"))

            if form_config.get("suffix"):
                st.write(form_config.get("suffix"))
            cgsl.show_buttons_row(buttons_config)

    if mode == "process_form":
        buttons_submitted = data.get("buttons_submitted")
        buttons_submitted_data = cgsl.get_buttons_submitted_data(
            buttons_submitted, buttons_config, False)

        log_debug(f"show_ideation_from_prompt | data: {data}",
                  debug=DEBUG)
        log_debug(f"| buttons_config: {buttons_config}",
                  debug=DEBUG)
        log_debug(f"| buttons_submitted: {buttons_submitted}",
                  debug=DEBUG)
        log_debug(f"| buttons_submitted_data: {buttons_submitted_data}",
                  debug=DEBUG)

        if not buttons_submitted_data:
            return None
        fields_values = {
            "question": data.get("question"),
        }
        st.session_state[form_session_state_key] = dict(fields_values)
        st.session_state[form_session_state_key].update({
            "buttons_submitted_data": buttons_submitted_data
        })
        return st.session_state[form_session_state_key]
</file>

<file path="src/codegen_buttons.py">
"""
This module contains the code for the buttons that are displayed on
the Streamlit app.
"""
import streamlit as st

from lib.codegen_streamlit_lib import StreamlitLib
from lib.codegen_utilities import get_app_config

DEBUG = False

app_config = get_app_config()
cgsl = StreamlitLib(app_config)


def get_response_as_prompt_button_config(key_name: str):
    """
    Returns the response as prompt button config
    """
    return {
        "text": "Use Response as Prompt",
        "key": key_name,
        "enable_config_name": "USE_RESPONSE_AS_PROMPT_ENABLED",
        # "on_click": cgsl.use_response_as_prompt,
        "on_click": cgsl.set_session_flag,
        "args": (key_name, "use_response_as_prompt_flag"),
    }


def get_prompt_enhancement_button_config(key_name: str):
    """
    Returns the prompt enhancement button config
    """
    return {
        "text": "Enhance prompt",
        "key": key_name,
        "type": "checkbox",
        # "on_change": cgsl.prompt_enhancement,
        "on_change": cgsl.set_session_flag,
        "args": (key_name, "prompt_enhancement_flag"),
    }


def get_use_embeddings_button_config(key_name: str):
    """
    Returns the use embeddings button config
    """
    return {
        "text": "Use Embeddings",
        "key": key_name,
        "type": "checkbox",
        "enable_config_name": "USE_EMBEDDINGS_ENABLED",
        "on_change": cgsl.set_session_flag,
        "args": (key_name, "use_embeddings_flag"),
    }


def add_buttons_for_main_tab():
    """
    Add the main tab buttons section to the page
    """
    with st.container():
        buttons_config = [
            {
                "text": "Answer Question",
                "key": "generate_text",
                "enable_config_name": "TEXT_GENERATION_ENABLED",
            },
            {
                "text": "Generate Video",
                "key": "generate_video",
                "enable_config_name": "VIDEO_GENERATION_ENABLED",
            },
            {
                "text": "Generate Image",
                "key": "generate_image",
                "enable_config_name": "IMAGE_GENERATION_ENABLED",
            },
            # {
            #     "text": "",
            #     "type": "spacer",
            # },
            get_response_as_prompt_button_config(
                "use_response_as_prompt_main_tab"),
            get_prompt_enhancement_button_config(
                "prompt_enhancement_main_tab"),
        ]
        cgsl.show_buttons_row(buttons_config)


def add_buttons_for_code_gen_tab():
    """
    Add the code generation tab buttons section to the page
    """
    with st.container():
        buttons_config = [
            {
                "text": "Generate Config & Tools Code",
                "key": "generate_code",
                "enable_config_name": "CODE_GENERATION_ENABLED",
            },
            get_use_embeddings_button_config(
                "use_embeddings_code_gen_tab",
            ),
            {
                "text": "Start App Code",
                "key": "start_app_code",
                "enable_config_name": "START_APP_CODE_ENABLED",
            },
            get_response_as_prompt_button_config(
                "use_response_as_prompt_code_gen_tab"),
            get_prompt_enhancement_button_config(
                "prompt_enhancement_code_gen_tab"),
        ]
        cgsl.show_buttons_row(buttons_config)
</file>

<file path=".dockerignore">
__pycache__
.env
venv
</file>

<file path=".env.example">
PYTHON_VERSION=3.10
#
# App parameters
# APP_NAME="GSAM Ideator and Code Generator"
# MAKER_MAME="Carlos J. Ramirez"
#
##############################
#
# AI Parameters
#
# OpenAI
OPENAI_API_KEY=
# HuggingFace
HUGGINGFACE_API_KEY=
# Together AI
TOGETHER_AI_API_KEY=
# AI/ML API
AIMLAPI_API_KEY=
# Groq
GROQ_API_KEY=
# X AI
XAI_API_KEY=
# OpenRouter
OPENROUTER_API_KEY=
# Rhymes
RHYMES_ARIA_API_KEY=
RHYMES_ALLEGRO_API_KEY=
# Ollama
#OLLAMA_BASE_URL=localhost:11434
# Nvidia
NVIDIA_API_KEY=
#
##############################
#
# Database Parameters
#
# DB_TYPE=json
DB_TYPE=mongodb
#
# JSON database parameters
JSON_DB_PATH=./db/conversations.json
#
# MongoDB database parameters
MONGODB_URI=mongodb+srv://<user>:<password>@<cluster>.mongodb.net
MONGODB_DB_NAME=mongodb_db_name
MONGODB_COLLECTION_NAME=mongodb_collection_name

# GSAM Agent parameters

# For the Supabase version (sample_supabase_agent.py), set your Supabase URL and Service Key.
# Get your SUPABASE_URL from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY=

# For the Postgres version (sample_postgres_agent.py), set your database connection URL.
# Format: postgresql://[user]:[password]@[host]:[port]/[database_name]
# Example: postgresql://postgres:mypassword@localhost:5432/mydb
# For Supabase Postgres connection, you can find this in Database settings -> Connection string -> URI
DATABASE_URL=

# Set this bearer token to whatever you want. This will be changed once the agent is hosted for you on the Studio!
API_BEARER_TOKEN=

# Default LLM provider
DEFAULT_LLM_PROVIDER=openrouter

##############################
#
# LLM defaults
#
# DEFAULT_LLM_PROVIDER=openai
# DEFAULT_LLM_PROVIDER=groq
# DEFAULT_LLM_PROVIDER=nvidia
# DEFAULT_LLM_PROVIDER=ollama
# DEFAULT_LLM_PROVIDER=huggingface
# DEFAULT_LLM_PROVIDER=together_ai  
# DEFAULT_LLM_PROVIDER=rhymes
# DEFAULT_LLM_PROVIDER=openrouter
#
# DEFAULT_TEXT_TO_VIDEO_PROVIDER=rhymes
#
# DEFAULT_TEXT_TO_IMAGE_PROVIDER=openai
# DEFAULT_TEXT_TO_IMAGE_PROVIDER=huggingface
#
##############################

# OpenRouter configuration
OPENROUTER_API_KEY=
OPENROUTER_MODEL_NAME=google/gemini-2.0-flash-exp:free
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
# lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/
</file>

<file path="CHANGELOG.md">
# CHANGELOG

All notable changes to this project will be documented in this file.
This project adheres to [Semantic Versioning](http://semver.org/) and [Keep a Changelog](http://keepachangelog.com/).



## Unreleased
---

### New

### Changes

### Fixes

### Breaks


## Unreleased
---

### New
Add image and video generation capabilities to the oTTomator Live Studio compatible GSAM Agent [GS-166].

### Fixes
Fix the runtime error in the streamlit UI in production [GS-55].


## 0.4.0 (2025-01-29)
---

### New
Add the oTTomator Live Studio compatible GSAM Agent (as part of the oTTomator hackathon) [GS-166].


## 0.3.0 (2025-01-25)
---

### New
Add AI/ML API provider and models [GS-55] [GS-156].
Add SUGGESTIONS_MODEL_REPLACEMENT parameter to avoid use the OpenAI reasoning models in the suggestions generation [GS-55].
Add SUGGESTIONS_DEFAULT_TIMEFRAME parameter to set the default timeframe to 48 hours for suggestions [GS-55].
Add LLM_MODEL_FORCED_VALUES parameter to set fixed values for models like o1-preview that only accepts temperature=1 [GS-55].
Add LLM_MODEL_PARAMS_NAMING parameter to rename the model parameters [GS-55].
Add DeepSeek-V3 model [GS-55].
Add titles to conversations, generated by AI [GS-55].

### Changes
The suggestions generation prompt was enhanced to be one-shot, the suggestions {qty} was added, the {timeframe} token is replaced by the SUGGESTIONS_DEFAULT_TIMEFRAME parameter value and the application subject token {subject} was replaced by a more generic subject text [GS-55].
All prompts were enhanced locating the application subject at the end of each prompt [GS-55].
Restore the original question if App Ideation from prompt was used [GS-55].
User prompt were splited to have system prompts that configure the LLM model behavior [GS-55].

### Fixes
Fix the error "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead." using OpenAI o1-preview/o1-mini models.
Fix the "unified" flag assignment in get_unified_flag() because it was returning False always [GS-55].


## 0.2.0 (2024-11-19)
---

### New
Add ideation from a user's prompt in the App Ideation section [GS-154].
Add the "Generate App Ideas" button to the App Ideation section [GS-154].
Add the X AI Grok model [GS-157]. 
Add "timeframe" to ideation form [GS-154].
Add model advanced configuration (temperature, max. tokens, top P, frequency penalty, presense penalty) [GS-154].
Add "add_js_script()" to inject any JS code to streamlit [GS-155].

### Changes
All prompts were enhanced.
LLM_PROVIDERS, TEXT_TO_IMAGE_PROVIDERS, and TEXT_TO_VIDEO_PROVIDERS list were converted to dict with all the providers having a list of "requirements" and "active" attributes [GS-154].

### Fixes
Fix the error when the image does not exist [GS-154].
Fix errors with the OpenAI image generation [GS-154].

### Breaks
LLM_PROVIDERS_COMPLETE_LIST config parameter removed. Instead the LLM_PROVIDERS list was converted to a dict with all the providers having a list of "requirements" and "active" attributes [GS-154].


## 0.1.0 (2024-11-10)
---

### New
Add: ideation form to get the application description and other data,  and generate names, database structure and PowerPoint presentations [GS-154].
Add: forms constructor and processor [GS-154].
Add: PowerPoint presentation generation [GS-154].

### Fixes
Fix: the "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`" warning in the application startup [GS-154].
Fix: the "save_file() missing 1 required positional argument: 'content'" running the Code generator [GS-154].


## 0.0.2 (2024-11-09)
---

### New
Add tabs to organize the multiple app and code generation options [GS-154].
Add use response as prompt feature [GS-154].
Add llamaindex embeddings index query on code generator [GS-154].
Add get_unified_flag() to configure providers and models that only accepts user messages, not system prompt, like o1-preview [GS-154].
Add prepare_model_params() to normalize the client and model parameters preparartion [GS-154].
Add LlamaIndexCustomLLM to abstract the llamaindex models with codegen LlmProvider [GS-154].
Add show_conversation_debug() to give the usert with detailled model responses [GS-154].

### Changes
Main streamlit UI layout elements separated in different functions [GS-154].
All model type (text, video, image) uses the model configuration UI selection [GS-154].
"Invalid LLM/ImageGen/TextToVideo provider" detailed error [GS-154].
read_file() allows to save the read files in a local directory to allow llamaindex embeddings to read it [GS-154].

### Fixes
Fix the way enhance prompt feature works, because it was not working properly with the absense of prompt text model [GS-154].


## 0.0.1 (2024-11-08)
---

### New
Project started for the [Llama Impact Hackathon](https://lablab.ai/event/llama-impact-hackathon) [GS-154].
Add frontend UI in stramlit.io [GS-152].
Add code generation backend to create the Genericsuite JSON files and Tools Python code [GS-149].
Add video generation and follow-up data in the conversation database [GS-153].
Add image generation [GS-152].
Add the video gallery page [GS-153].
Add the image gallery page [GS-55].
Add MongoDB support [GS-152].
Add Together.AI support [GS-158].
Add Meta Llama models support [GS-119].
Add prompt enhancement support [GS-152].
Add data management pull down section in the side bar [GS-152].
Add import and export database items to JSON files [GS-152].
Add DatabaseAbstract class to normalize the database classes structure [GS-152].
Add initial version of the GS mini-library, chat feature, image generation [GS-149].
Add Streamlit UI library to normalize all Streamlit specific methods and include it in the codegen library [GS-55].
Add get_new_item_id() to normalize the new "id" creation [GS-152].
Add code generation results processing and save in conversations [SS-149].
Add configuration in JSON files [GS-55].
Add parameters values that can be read from a file path, e.g. "[refine_video_prompt.txt]" [GS-55].
Add suggestions user customizable prompt [SS-149].
Add conversations buffer to speed up the separated questions and content display [SS-149].
Add models selection [GS-55].
Add buttons to generate project ideas, names, presentation and video script [GS-55].
</file>

<file path="Dockerfile">
FROM ottomator/base-python:latest

# Build argument for port with default value
ARG PORT=8001
ENV PORT=${PORT}

WORKDIR /app

# Copy requirements first to leverage Docker cache
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application code
COPY . .

# Expose the port from build argument
EXPOSE ${PORT}

# Command to run the application
# Feel free to change sample_supabase_agent to sample_postgres_agent
CMD ["sh", "-c", "uvicorn gsam_ottomator_agent_app:app --host 0.0.0.0 --port ${PORT}"]
</file>

<file path="gsam_ottomator_agent_app.py">
"""
GSAM Ottomator Agent
"""
from typing import Annotated

import os
from dotenv import load_dotenv

from pydantic import BaseModel
from fastapi import Depends, HTTPException, Header, Request
from fastapi.responses import FileResponse

from lib.codegen_utilities import log_debug

from gsam_ottomator_agent.gsam_supabase_agent import (
    init_fastapi_app as init_fastapi_app_supabase,
    verify_token as verify_token_supabase,
    gsam_supabase_agent,
    AgentRequest as SupaBaseAgentRequest,
    AgentResponse as SupaBaseAgentResponse
)
from gsam_ottomator_agent.gsam_postgres_agent import (
    init_fastapi_app as init_fastapi_app_postgres,
    verify_token as verify_token_postgres,
    gsam_postgres_agent,
    AgentRequest as PostgresAgentRequest,
    AgentResponse as PostgresAgentResponse
)


# FastAPI headers reference:
# https://fastapi.tiangolo.com/tutorial/header-param-models/#forbid-extra-headers
# https://fastapi.tiangolo.com/tutorial/header-params/#header-parameters-with-a-pydantic-model

class CommonHeaders(BaseModel):
    host: str
    scheme: str
    # save_data: bool
    # if_modified_since: str | None = None
    # traceparent: str | None = None
    # x_tag: list[str] = []


DEBUG = False

# Load environment variables
load_dotenv()

agent_db_type = "supabase" if os.getenv("SUPABASE_URL") else "postgres"

if agent_db_type == "postgres":
    app = init_fastapi_app_postgres()
else:
    app = init_fastapi_app_supabase()


@app.post("/api/gsam-supabase-agent", response_model=SupaBaseAgentResponse)
async def gsam_supabase_agent_endpoint(
    agent_request: SupaBaseAgentRequest,
    authenticated: bool = Depends(verify_token_supabase),
    request: Request = None
):
    if agent_db_type != "supabase":
        raise HTTPException(
            status_code=400,
            detail="Invalid agent database type [GSAE-E010]"
        )
    result = await gsam_supabase_agent(agent_request, authenticated,
                                       dict(request))
    return result


@app.post("/api/gsam-postgres-agent", response_model=PostgresAgentResponse)
async def gsam_postgres_agent_endpoint(
    agent_request: PostgresAgentRequest,
    authenticated: bool = Depends(verify_token_postgres),
    request: Request = None
):
    if agent_db_type != "postgres":
        raise HTTPException(
            status_code=400,
            detail="Invalid agent database type [GPAE-E010]"
        )
    result = await gsam_postgres_agent(agent_request, authenticated,
                                       dict(request))
    return result


@app.get("/api/image/{image_name}")
async def get_image(image_name: str):
    image_path = f"./images/{image_name}"
    if not os.path.exists(image_path):
        raise HTTPException(
            status_code=404,
            detail=f"Image {image_name} not found")
    # return FileResponse(image_path, media_type="image/jpg")
    return FileResponse(image_path)

if __name__ == "__main__":
    import uvicorn
    # Feel free to change the port here if you need
    uvicorn.run(app, host="0.0.0.0", port=8001)
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 Carlos J. Ramirez

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="Makefile">
# .DEFAULT_GOAL := local
# .PHONY: tests
SHELL := /bin/bash

# General Commands
help:
	cat Makefile

create_venv:
	sh scripts/run_app.sh create_venv

install: 
	sh scripts/run_app.sh install

requirements:
	sh scripts/run_app.sh requirements

test: install
	pytest tests --junitxml=report.xml

# Development Commands
lint: install
	prospector

types: install
	mypy .

coverage: install
	coverage run -m unittest discover tests;
	coverage report

format: install
	yapf -i *.py **/*.py **/**/*.py

format_check: install
	yapf --diff *.py **/*.py **/**/*.py

pycodestyle: install
	pycodestyle

qa: lint types tests format_check pycodestyle

# Application Specific Commands
run:
	sh scripts/run_app.sh run
</file>

<file path="README.md">
# GSAM ✨ GenericSuite App Maker

Author: [Carlos J. Ramirez](https://github.com/tomkat-cr/genericsuite-app-maker)

AI tool to enhance the software development ideation and AI models, providers and features selection and test.

<!-- ![Hackathon Cover image](./assets/llama-impact-hackathon-cover.png) -->

## Introduction

[GSAM](https://gsam-app.streamlit.app) is a tool to help on the software development process for any Application. It allows to generate code, images, video or answers from a text prompt, and kick start code to be used with the [GenericSuite](https://genericsuite.carlosjramirez.com) library.

## GSAM Python FastAPI agent

The [GSAM Python FastAPI agent](./gsam_ottomator_agent/README.md) is the implementation compatible with the [OTTomator](https://ottomator.ai) [autogen_studio](https://studio.ottomator.ai). For more information, see the [GSAM Python FastAPI agent](./gsam_ottomator_agent/README.md) documentation.

## Key Features

* Answer question with LLM inference, using Meta Llama models, Together.ai, HuggingFace, Groq, Ollama, Nvidia NIMs, and OpenAI.
* Image Generation: using HuggingFace and the Flux or OpenAI Dall-E models.
* Video Generation: using Rhymes AI Allegro model.
* Galleries to show the generated images and videos.
* Ability to change the Provider and Model used for all the LLM Inferences, image and video generations.
* Suggestions to generate App ideas, and the hability to customize the suggestion generation prompt.
* Code Generation: suggest the JSON configuration files and Langchain Tools Python code from an App description to be used with the [GenericSuite](https://genericsuite.carlosjramirez.com) library.
* Use LlamaIndex to generate code and JSON files using vectorized data instead of send all the attachments to the LLM.
* Store each user interaction (question, answer, image, video, code) in a MongoDB database, and retrieve it later.
* Database Management: import and export data from MongoDB to JSON files.
* Prompt Engineering: there's an option to allow the prompts/questions optimization to take more advantage from the Model's capabilities.
* Naming: generate name ideas for the App.
* App Structure: generate the App description and database table structures.
* App Presentation: generate PowerPoint presentation for the App, including the content, speaker notes, and image generation prompts.

## Technology Used

* Meta Llama models: Llama 3.2 3B, Llama 3.1 8B, 70B, and 405B
* Together.ai
* Huggingface Inference API
* Flux.1 image generation model
* Rhymes Allegro video generation model
* LlamaIndex framework.
* StreamLit
* MongoDB Atlas
* Python 3.10


## Getting Started

### Prerequisites

- [Python](https://www.python.org/downloads/) 3.10 or higher
- [Git](https://www.atlassian.com/git/tutorials/install-git)
- Make: [Mac](https://formulae.brew.sh/formula/make) | [Windows](https://stackoverflow.com/questions/32127524/how-to-install-and-use-make-in-windows)

### Installation

Clone the repository:
```bash
git clone https://github.com/tomkat-cr/genericsuite-app-maker.git
```

Navigate to the project directory:

```bash
cd genericsuite-app-maker
```

### Create the .env file

Create a `.env` file in the root directory of the project:

```bash
# You can copy the .env.example file in the root directory of the project
cp .env.example .env
```

The `.env` file should have the following content:

```bash
PYTHON_VERSION=3.10
#
# AI Parameters
#
# OpenAI
OPENAI_API_KEY=
# HuggingFace
HUGGINGFACE_API_KEY=
# Together AI
TOGETHER_AI_API_KEY=
# AI/ML API
AIMLAPI_API_KEY=
# Groq
GROQ_API_KEY=
# X AI
XAI_API_KEY=
# RHYMES parameters
RHYMES_ALLEGRO_API_KEY=
RHYMES_ARIA_API_KEY=
# Ollama
#OLLAMA_BASE_URL=localhost:11434
# Nvidia
NVIDIA_API_KEY=
#
# Database Parameters
#
DB_TYPE=mongodb
# DB_TYPE=json
#
# MongoDB database parameters
MONGODB_URI=mongodb+srv://<user>:<password>@<cluster>.mongodb.net
MONGODB_DB_NAME=gsam_db
MONGODB_COLLECTION_NAME=conversations
#
# JSON database parameters
# JSON_DB_PATH=./db/conversations.json
```

Replace the `..._API_KEY` access tokens with your Together.ai, OpenAI, Huggingface, Groq, Nvidia, and Rhymes API keys, respectively.

The API Keys specified in the `.env` file will enable the the corresponding LLMs, image generators, and video generators to be available in the Model Selection panel.

To use a MongoDB database, comment out `DB_TYPE=json`, uncomment `# DB_TYPE=mongodb`, and replace `YOUR_MONGODB_URI`, `YOUR_MONGODB_DB_NAME`, and `YOUR_MONGODB_COLLECTION_NAME` with your actual MongoDB URI, database name, and collection name, respectively.

### Run the Application

```bash
# With Make
make run
```

```bash
# Without Make
sh scripts/run_app.sh run
```

## Usage

Go to your favorite Browser and open the URL provided by the application.

* Locally:<BR/>
  [http://localhost:8501](http://localhost:8501)

* Official App:<BR/>
  [https://gsam-app.streamlit.app/](https://gsam-app.streamlit.app/)

### Prompt Suggestions

- The Prompt Suggestions under the title can be generated from AI using the `Suggestions Prompt` pull-down section. Enter the Prompt in the text box and click the `Generate Suggestions` button. Click on `Reset Prompt` to set the default prompt.
- Any suggestion text can be copied to the Question box by clicking on it.

### Models Selection

- The LLM Chat, Image and Video generarion Providers and Models can be selected using the `Models Selection` pull-down section.

### Text-to-Text Generation

* Enter your text prompt in the provided text box or select one of the suggested prompts.
* Check the `Enhance prompt` checkbox to allow the LLM to optimize the prompt to take more advantage from the Model's capabilities.
* Select the `Main` tab and click the `Answer Question` button.
* The answer will appear in below the queston box.
* Click the `Use Response as Prompt` button to use the answer as the prompt for the next question.
* All questions and answers are available in the side menu.

### Text-to-Image Generation

* Enter your text prompt in the provided text box or select one of the suggested prompts.
* Check the `Enhance prompt` checkbox to allow the LLM to optimize the prompt.
* Select the `Main` tab and click the `Generate Image` button.
* Sit back and watch as [GSAM](https://gsam-app.streamlit.app) transforms your text into a high-quality image.
* After a few seconds, the image will appear.
* All images are available in the side menu and can be viewed in the gallery clicking the `Image Gallery` button.

### Text-to-Video Generation

* Enter your text prompt in the provided text box or select one of the suggested prompts.
* Check the `Enhance prompt` checkbox to allow the LLM to optimize the prompt.
* Select the `Main` tab and click the `Generate Video` button.
* Sit back and watch as [GSAM](https://gsam-app.streamlit.app) transforms your text into a high-quality video.
* After 2+ minutes, the video will appear in the video container.
* All videos are available in the side menu and can be viewed in the gallery clicking the `Video Gallery` button.

### App ideation

Click the `App ideation` tab to have access to the app ideation page. This page allows you to specify the application name, description, and other elements to generate naing ideas, app extended description and database structure, and presentation.

* **Generate App Names**: In the Application Ideation Form section, fill in the required fields (Application name, Subtitle, Summary and App Type) and click the `Generate App Names` button at the bottom to generate ideas on how to name you application.

* **Generate App Structure**: In the Application Ideation Form section, fill in the required fields (Application name, Subtitle, Summary and App Type) and click the `Generate App Structure` button at the bottom to generate ideas on how to descroibe and structure your application.

* **Generate Presentation**: In the Application Ideation Form section, fill in the all the form fields and click the `Generate Presentation` button at the bottom to generate the slides structure and create the PowerPoint file.

### Code Generation

This option allows the JSON configuration files and Langchain Tools Python code generation from an App description to be used with the [GenericSuite](https://genericsuite.carlosjramirez.com) library.

* Enter your text prompt in the provided text box or select one of the suggested prompts.
* Check the `Use Embeddings` checkbox to use LlamaIndex in the code and JSON files generate, using vectorized data instead of send all the attachments to the LLM.
* Check the `Enhance prompt` checkbox to allow the LLM to optimize the prompt.
* Select the `Code Generation` tab and click the `Generate Config & Tools Code` button.
* Sit back and watch as [GSAM](https://gsam-app.streamlit.app) transforms your text into a high-quality code.
* After a few seconds, the code will appear in the code container.

### Notes

- Each entry in the side menu has an `x` button to delete it.
- Depending on the `DB_TYPE` parameter, the side menu items are stored in MongoDB or in a JSON file localted in the `db` folder.
- You can add additional LLM / Image / Video providers and models in the [./config/app_config.json](./config/app_config.json) file, as well as configure all other GSAM parameters.
- All the system prompts used by GSAM are located in the [./config](./config) directory.

## Screenshots

Main Page
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.53.34 PM.png)

LLM Inference
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.55.46 PM.png)

Suggestions Generation & Model Selection
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.57.23 PM.png)

Suggestion Applied to Prompt
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.58.55 PM.png)

Code Generation
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.54.23 PM.png)

App Ideation Page [1]
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.54.39 PM.png)

App Ideation Page [2]
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.58.12 PM.png)

Presentation Generation
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.54.45 PM.png)

Image Generation
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.55.32 PM.png)

Image Gallery
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.54.55 PM.png)

Video Gallery
![App Screenshot](./assets/screenshots/Screenshot%202024-11-10%20at%205.55.02 PM.png)

## Contributors

[Carlos J. Ramirez](https://www.linkedin.com/in/carlosjramirez/)

Please feel free to suggest improvements, report bugs, or make a contribution to the code.

## License

This project is licensed under the terms of the MIT license. See the [LICENSE](LICENSE) file for details.

## Acknowledgements

* [Meta](https://www.llama.com/) for developing the Meta Llama powerful models.
* [Streamlit](https://streamlit.io/) for providing a user-friendly interface for interacting with the application.
* Open-source community for inspiring and supporting collaborative innovation.
* Users and contributors for their feedback and support.

## Credits

This project is developed and maintained by [Carlos J. Ramirez](https://www.linkedin.com/in/carlosjramirez/). For more information or to contribute to the project, visit [GenericSuite App Maker on GitHub](https://github.com/tomkat-cr/genericsuite-app-maker).

Happy Coding!

## Contributing

This agent is part of the oTTomator agents collection. For contributions or issues, please refer to the main repository guidelines.
</file>

<file path="requirements.txt">
aiohappyeyeballs==2.4.4
aiohttp==3.11.11
aiosignal==1.3.2
altair==5.5.0
annotated-types==0.7.0
anthropic==0.45.2
anyio==4.8.0
asyncpg==0.30.0
attrs==25.1.0
beautifulsoup4==4.12.3
blinker==1.9.0
cachetools==5.5.1
certifi==2024.12.14
charset-normalizer==3.4.1
click==8.1.8
cohere==5.13.11
colorama==0.4.6
dataclasses-json==0.6.7
Deprecated==1.2.18
deprecation==2.1.0
dirtyjson==1.0.8
distro==1.9.0
dnspython==2.7.0
eval_type_backport==0.2.2
executing==2.2.0
fastapi==0.115.7
fastavro==1.10.0
filelock==3.17.0
filetype==1.2.0
frozenlist==1.5.0
fsspec==2024.12.0
gitdb==4.0.12
GitPython==3.1.44
google-auth==2.38.0
googleapis-common-protos==1.66.0
gotrue==2.11.2
greenlet==3.1.1
griffe==1.5.5
groq==0.15.0
h11==0.14.0
h2==4.1.0
hpack==4.1.0
httpcore==1.0.7
httpx==0.28.1
httpx-sse==0.4.0
huggingface-hub==0.28.0
hyperframe==6.1.0
idna==3.10
importlib_metadata==8.5.0
Jinja2==3.1.5
jiter==0.8.2
joblib==1.4.2
jsonpath-python==1.0.6
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
llama-cloud==0.1.11
llama-index==0.12.14
llama-index-agent-openai==0.4.3
llama-index-cli==0.4.0
llama-index-core==0.12.14
llama-index-embeddings-openai==0.3.1
llama-index-indices-managed-llama-cloud==0.6.4
llama-index-llms-openai==0.3.14
llama-index-multi-modal-llms-openai==0.4.2
llama-index-program-openai==0.3.1
llama-index-question-gen-openai==0.3.0
llama-index-readers-file==0.4.4
llama-index-readers-llama-parse==0.4.0
llama-parse==0.5.20
logfire==3.4.0
logfire-api==3.4.0
lxml==5.3.0
markdown-it-py==3.0.0
MarkupSafe==3.0.2
marshmallow==3.26.0
mdurl==0.1.2
mistralai==1.5.0
multidict==6.1.0
mypy-extensions==1.0.0
narwhals==1.24.1
nest-asyncio==1.6.0
networkx==3.4.2
nltk==3.9.1
numpy==2.2.2
ollama==0.4.7
openai==1.60.2
opentelemetry-api==1.29.0
opentelemetry-exporter-otlp-proto-common==1.29.0
opentelemetry-exporter-otlp-proto-http==1.29.0
opentelemetry-instrumentation==0.50b0
opentelemetry-proto==1.29.0
opentelemetry-sdk==1.29.0
opentelemetry-semantic-conventions==0.50b0
packaging==24.2
pandas==2.2.3
pillow==10.4.0
postgrest==0.19.3
propcache==0.2.1
protobuf==5.29.3
pyarrow==19.0.0
pyasn1==0.6.1
pyasn1_modules==0.4.1
pydantic==2.10.6
pydantic-ai==0.0.20
pydantic-ai-slim==0.0.20
pydantic-graph==0.0.20
pydantic_core==2.27.2
pydeck==0.9.1
Pygments==2.19.1
pymongo==4.11
pypdf==5.2.0
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
python-pptx==1.0.2
pytz==2024.2
PyYAML==6.0.2
realtime==2.2.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
rich==13.9.4
rpds-py==0.22.3
rsa==4.9
shellingham==1.5.4
six==1.17.0
smmap==5.0.2
sniffio==1.3.1
soupsieve==2.6
SQLAlchemy==2.0.37
starlette==0.45.3
storage3==0.11.1
streamlit==1.41.1
StrEnum==0.4.15
striprtf==0.0.26
supabase==2.12.0
supafunc==0.9.2
tabulate==0.9.0
tenacity==9.0.0
tiktoken==0.8.0
together==1.3.14
tokenizers==0.21.0
toml==0.10.2
tornado==6.4.2
tqdm==4.67.1
typer==0.15.1
types-requests==2.32.0.20241016
typing-inspect==0.9.0
typing_extensions==4.12.2
tzdata==2025.1
urllib3==2.3.0
uvicorn==0.34.0
websockets==13.1
wrapt==1.17.2
XlsxWriter==3.2.2
yarl==1.18.3
zipp==3.21.0
</file>

<file path="streamlit_app.py">
"""
VitexBrain App
"""
import os
from dotenv import load_dotenv
import streamlit as st

from lib.codegen_streamlit_lib import StreamlitLib
from lib.codegen_utilities import get_app_config
from lib.codegen_utilities import log_debug

from lib.codegen_schema_generator import JsonGenerator
from lib.codegen_app_ideation_lib import (
    get_ideation_form_config,
    get_ideation_from_prompt_config,
)
from src.codegen_app_ideation import (
    show_ideation_form,
    show_ideation_from_prompt,
)
from src.codegen_buttons import (
    add_buttons_for_main_tab,
    add_buttons_for_code_gen_tab,
)

DEBUG = False

app_config = get_app_config()
cgsl = StreamlitLib(app_config)


# Code Generator specific


def process_json_and_code_generation(
    result_container: st.container,
    question: str = None
):
    """
    Generates the JSON file and GS python code for Tools
    """
    if not question:
        question = st.session_state.question
    if not cgsl.validate_question(question):
        return

    llm_text_model_elements = cgsl.get_llm_text_model()
    if llm_text_model_elements['error']:
        result_container.write(
            f"ERROR E-100-D: {llm_text_model_elements['error_message']}")
        return
    other_data = {
        "ai_provider": llm_text_model_elements['llm_provider'],
        "ai_model": llm_text_model_elements['llm_model'],
        "template": "json_and_code_generation",
    }

    with st.spinner("Procesing code generation..."):
        params = {
            "user_input_text": question,
            "use_embeddings": st.session_state.use_embeddings_flag,
            "embeddings_sources_dir": cgsl.get_par_value(
                "EMBEDDINGS_SOURCES_DIR", "./embeddings_sources"),
            "provider": llm_text_model_elements['llm_provider'],
            "model": llm_text_model_elements['llm_model'],
        }
        json_generator = JsonGenerator(params=params)
        response = json_generator.generate_json()
        if response['error']:
            other_data["error_message"] = (
                f"ERROR E-100: {response['error_message']}")
        other_data.update(response.get('other_data', {}))
        cgsl.save_conversation(
            type="text",
            question=question,
            refined_prompt=response.get('refined_prompt'),
            answer=response.get(
                'response',
                "No response. Check the Detailed Response section."),
            other_data=other_data,
        )
        # result_container.write(response['response'])
        st.rerun()


def process_use_response_as_prompt():
    """
    Process the use_response_as_prompt button pushed
    """
    if st.session_state.use_response_as_prompt_flag:
        if "last_retrieved_conversation" in st.session_state:
            conversation = dict(st.session_state.last_retrieved_conversation)
            st.session_state.question = conversation['answer']
    st.session_state.use_response_as_prompt_flag = False


def process_ideation_form(form: dict, form_config: dict):
    """
    Process the ideation form
    """
    result_container = st.container()
    features_data = form_config.get("features_data", {})
    fields_data = form_config.get("fields", {})

    log_debug("process_ideation_form | form: " + f"{form}", debug=DEBUG)
    log_debug("process_ideation_form | form_config: " + f"{features_data}",
              debug=DEBUG)

    # Validates the submitted form
    if not form:
        cgsl.show_form_error("No data received from the form")
        return
    if not form.get("buttons_submitted_data"):
        cgsl.show_form_error("Missing buttons submitted data")
        return

    # Verify button pressed
    selected_feature = cgsl.get_selected_feature(form, features_data)
    if not selected_feature:
        cgsl.show_form_error("No button pressed... try again please")
        return

    # Verify mandatory field
    error_message = ""
    for key in features_data.get(selected_feature).get("mandatory_fields"):
        if not form.get(key):
            field_name = fields_data.get(key, {}).get("title", key)
            error_message += f"{field_name}, "
    if error_message:
        error_message = error_message[:-2]
        cgsl.show_form_error(f"Missing field(s): {error_message}")
        return

    template = features_data.get(selected_feature).get("template")
    if not template:
        cgsl.show_form_error("Missing template")
        return

    system_prompt_template = features_data.get(selected_feature) \
        .get("system_prompt")
    if not system_prompt_template:
        cgsl.show_form_error("Missing system prompt")
        return

    # Read the template file
    template_path = f"./config/{template}"
    if not os.path.exists(template_path):
        cgsl.show_form_error(f"Missing template file: {template_path}")
        return
    with open(template_path, "r") as f:
        question = f.read()

    # Read the system prompt file
    system_prompt_path = f"./config/{system_prompt_template}"
    if not os.path.exists(system_prompt_path):
        cgsl.show_form_error(
            f"Missing system prompt file: {system_prompt_path}")
        return
    with open(system_prompt_path, "r") as f:
        system_prompt = f.read()

    # Default values
    if "timeframe" not in form:
        form["timeframe"] = cgsl.get_par_value("IDEATION_DEFAULT_TIMEFRAME")
    if "quantity" not in form:
        form["quantity"] = cgsl.get_par_value("IDEATION_DEFAULT_QTY")

    # Replace the placeholders with the user input
    final_form = {}
    for key in form:
        if key in [
            "screenshots",
            "buttons_submitted_data",
            "buttons_submitted"
        ]:
            continue
        log_debug(f"process_ideation_form | key: {key} | "
                  f"form[key]: {form[key]}", debug=DEBUG)
        final_form[key] = form[key]
        if form[key]:
            question = question.replace(f"{{{key}}}", form[key])

    form_name = cgsl.get_form_name(form_config)
    form_session_state_key = cgsl.get_form_session_state_key(form_config)
    other_data = {
        "subtype": selected_feature,
        "template": template,
        "system_prompt": system_prompt,
        "form_name": form_name,
        "form_data": final_form,
        "form_session_state_key": form_session_state_key,
    }

    log_debug("process_ideation_form | question: " + f"{question}"
              "\n | other_data: " + f"{other_data}",
              debug=DEBUG)

    # Call the LLM to generate the ideation
    response = cgsl.text_generation(result_container, question, other_data,
                                    {"assign_global": False})

    log_debug("process_ideation_form | response: " + f"{response}",
              debug=DEBUG)

    error_message = None
    if response['error']:
        error_message = f"ERROR E-900-B: {response['error_message']}"
        cgsl.show_form_error(error_message)

    cgsl.save_conversation(
        type="text",
        question=question,
        refined_prompt=response.get('refined_prompt'),
        answer=response.get(
            'response',
            "No response. Check the Detailed Response section."),
        other_data=other_data,
    )

    # Restore the original question if App Ideation from prompt was used
    # original_question = other_data.get("form_data", {}).get("question")
    # if original_question:
    #     st.session_state.question = original_question

    st.rerun()


# UI elements


def get_question_label(tab: str = "main"):
    """
    Returns the question label based on the tab
    """
    label = "Question / Prompt:"
    if tab == "app_ideation" or tab == "code_gen":
        label = "App description:"
    st.session_state.question_label = label
    return label


def add_title():
    """
    Add the title section to the page
    """

    # Emoji shortcodes
    # https://streamlit-emoji-shortcodes-streamlit-app-gwckff.streamlit.app/

    with st.container():
        col = st.columns(
            2, gap="small",
            vertical_alignment="bottom")
        with col[0]:
            st.title(cgsl.get_title())
        with col[1]:
            sub_col = st.columns(
                2, gap="small",
                vertical_alignment="bottom")
            col_index = 0
            if cgsl.get_par_value("VIDEO_GENERATION_ENABLED", True):
                with sub_col[col_index]:
                    st.button(
                        "Video Gallery",
                        on_click=cgsl.set_query_param,
                        args=("page", "video_gallery"))
                col_index += 1
            if cgsl.get_par_value("IMAGE_GENERATION_ENABLED", True):
                with sub_col[col_index]:
                    st.button(
                        "Image Gallery",
                        on_click=cgsl.set_query_param,
                        args=("page", "image_gallery"))


def add_suggestions():
    """
    Add the suggestions section to the page
    """
    suggestion_container = st.empty()
    cgsl.show_suggestion_components(suggestion_container)

    # Show the siderbar selected conversation's question and answer in the
    # main section
    # (must be done before the user input)
    for conversation in st.session_state.conversations:
        if st.session_state.get(conversation['id']):
            cgsl.show_conversation_question(conversation['id'])
            break


def add_models_selection():
    """
    Add the models selection to the page
    """
    # available_llm_providers = cgsl.get_par_value("LLM_PROVIDERS")
    available_llm_providers = cgsl.get_available_ai_providers("LLM_PROVIDERS")
    llm_provider_index = cgsl.get_llm_provider_index(
        "LLM_PROVIDERS",
        "llm_provider")
    llm_model_index = cgsl.get_llm_model_index(
        "LLM_PROVIDERS", "llm_provider",
        "LLM_AVAILABLE_MODELS", "llm_model")

    # available_image_providers = cgsl.get_par_value("TEXT_TO_IMAGE_PROVIDERS")
    available_image_providers = cgsl.get_available_ai_providers(
        "TEXT_TO_IMAGE_PROVIDERS")
    image_provider_index = cgsl.get_llm_provider_index(
        "TEXT_TO_IMAGE_PROVIDERS",
        "image_provider")
    image_model_index = cgsl.get_llm_model_index(
        "TEXT_TO_IMAGE_PROVIDERS", "image_provider",
        "TEXT_TO_IMAGE_AVAILABLE_MODELS", "image_model")

    # available_video_providers = cgsl.get_par_value("TEXT_TO_VIDEO_PROVIDERS")
    available_video_providers = cgsl.get_available_ai_providers(
        "TEXT_TO_VIDEO_PROVIDERS")
    video_provider_index = cgsl.get_llm_provider_index(
        "TEXT_TO_VIDEO_PROVIDERS",
        "video_provider")
    video_model_index = cgsl.get_llm_model_index(
        "TEXT_TO_VIDEO_PROVIDERS", "video_provider",
        "TEXT_TO_VIDEO_AVAILABLE_MODELS", "video_model")

    # log_debug("image_provider_index: " + f"{image_provider_index}",
    #           debug=DEBUG)
    # log_debug("image_model_index: " + f"{image_model_index}", debug=DEBUG)
    # log_debug("video_provider_index: " + f"{video_provider_index}",
    #           debug=DEBUG)

    with st.expander("Models Selection"):
        # LLM Provider and Model
        col = st.columns(2, gap="small", vertical_alignment="bottom")
        with col[0]:
            st.selectbox(
                "LLM Provider",
                available_llm_providers,
                key="llm_provider",
                index=llm_provider_index,
                help="Select the provider to use for the LLM call")
        with col[1]:
            st.selectbox(
                "LLM Model",
                cgsl.get_model_options(
                    "LLM_PROVIDERS",
                    "llm_provider",
                    "LLM_AVAILABLE_MODELS"
                ),
                key="llm_model",
                index=llm_model_index,
                help="Select the model to use for the LLM call")

        # Image Provider and Model
        col = st.columns(2, gap="small", vertical_alignment="bottom")
        with col[0]:
            st.selectbox(
                "Text-to-Image Provider",
                available_image_providers,
                key="image_provider",
                index=image_provider_index,
                help="Select the provider to use for the text-to-image call")
        with col[1]:
            st.selectbox(
                "Text-to-Image Model",
                cgsl.get_model_options(
                    "TEXT_TO_IMAGE_PROVIDERS",
                    "image_provider",
                    "TEXT_TO_IMAGE_AVAILABLE_MODELS",
                ),
                key="image_model",
                index=image_model_index,
                help="Select the model to use for the text-to-image call")

        # Video Provider and Model
        col = st.columns(2, gap="small", vertical_alignment="bottom")
        with col[0]:
            st.selectbox(
                "Text-to-Video Provider",
                available_video_providers,
                key="video_provider",
                index=video_provider_index,
                help="Select the provider to use for the text-to-video call")
        with col[1]:
            st.selectbox(
                "Text-to-Video Model",
                cgsl.get_model_options(
                    "TEXT_TO_VIDEO_PROVIDERS",
                    "video_provider",
                    "TEXT_TO_VIDEO_AVAILABLE_MODELS",
                ),
                key="video_model",
                index=video_model_index,
                help="Select the model to use for the text-to-video call")

    with st.expander("Model configuration (advanced)"):
        # Temperature slider | default: 1.00
        col = st.columns(4, gap="small", vertical_alignment="bottom")
        with col[0]:
            st.slider(
                "Temperature",
                min_value=0.0,
                max_value=2.0,
                value=0.5,
                step=0.01,
                key="model_config_par_temperature",
                help="Controls the randomness of the output. Lower values make"
                     " the output more deterministic.",
            )

        # Max tokens slider | default: 2048
        col = st.columns(4, gap="small", vertical_alignment="bottom")
        with col[0]:
            st.slider(
                "Max Tokens",
                min_value=0,
                max_value=4095,
                value=2048,
                step=1,
                key="model_config_par_max_tokens",
                help="The maximum number of tokens to generate.",
            )

        # Top P slider | default: 1.00
        col = st.columns(4, gap="small", vertical_alignment="bottom")
        with col[0]:
            st.slider(
                "Top P",
                min_value=0.0,
                max_value=1.0,
                value=1.0,
                step=0.01,
                key="model_config_par_top_p",
                help="The cumulative probability of the top tokens to"
                     " generate.",
            )

        # Frequency penalty slider | default: 0.00
        col = st.columns(4, gap="small", vertical_alignment="bottom")
        with col[0]:
            st.slider(
                "Frequency Penalty",
                min_value=0.0,
                max_value=2.0,
                value=0.0,
                step=0.01,
                key="model_config_par_frequency_penalty",
                help="The higher the value, the more diverse the output.",
            )

        # Presence penalty slider | default: 0.00
        col = st.columns(4, gap="small", vertical_alignment="bottom")
        with col[0]:
            st.slider(
                "Presence Penalty",
                min_value=0.0,
                max_value=2.0,
                value=0.0,
                step=0.01,
                key="model_config_par_presence_penalty",
                help="The higher the value, the more diverse the output.",
            )


def add_attachments():
    """
    Add the attachments section to the page
    """
    if not cgsl.get_par_value("ADD_ATTACHMENTS_ENABLED", False):
        return
    with st.expander("Attachments"):
        st.file_uploader(
            "Choose file(s) to be attached to the conversation",
            accept_multiple_files=True,
            on_change=cgsl.attach_files,
            key="attach_files",
        )


def add_user_input():
    """
    Add the user input section to the page and return the question object
    """
    with st.container():
        question = st.text_area(
            st.session_state.question_label,
            st.session_state.question)
    return question


def add_results_containers():
    """
    Add the results containers to the page
    """
    with st.container():
        additional_result_container = st.empty()
        result_container = st.empty()
    return additional_result_container, result_container


def add_show_selected_conversation(
        result_container: st.container,
        additional_result_container: st.container):
    """
    Show the selected conversation's question and answer in the
    main section
    """
    if "new_id" not in st.session_state:
        return
    with st.container():
        cgsl.show_conversation_question(st.session_state.new_id)
        cgsl.show_conversation_content(
            st.session_state.new_id,
            result_container,
            additional_result_container)
        st.session_state.new_id = None


def add_sidebar():
    """
    Add the sidebar to the page and return the data management container
    """
    with st.sidebar:
        app_desc = cgsl.get_par_value("APP_DESCRIPTION")
        app_desc = app_desc.replace(
            "{app_name}",
            f"**{st.session_state.app_name}**")
        st.sidebar.write(app_desc)

        cgsl.data_management_components()
        data_management_container = st.empty()

        # Show the conversations in the side bar
        cgsl.show_conversations()
    return data_management_container


def add_check_buttons_pushed(
        result_container: st.container,
        additional_result_container: st.container,
        data_management_container: st.container,
        parameters_container: st.container,
        question: str):
    """
    Check buttons pushed
    """

    # Process the generate_video button pushed
    if st.session_state.get("generate_video"):
        cgsl.video_generation(result_container, question)

    # Process the generate_image button pushed
    if st.session_state.get("generate_image"):
        cgsl.image_generation(result_container, question)

    # Process the generate_text button pushed
    if st.session_state.get("generate_text"):
        cgsl.text_generation(result_container, question)

    # Process the generate_code button pushed
    if st.session_state.get("generate_code"):
        process_json_and_code_generation(result_container, question)

    # Show the selected conversation's question and answer in the
    # main section
    for conversation in st.session_state.conversations:
        if st.session_state.get(conversation['id']):
            cgsl.show_conversation_content(
                conversation['id'], result_container,
                additional_result_container)
            break

    # Perform data management operations
    if st.session_state.get("import_data"):
        cgsl.import_data(data_management_container)

    if st.session_state.get("export_data"):
        cgsl.export_data(data_management_container)

    if "dm_results" in st.session_state and st.session_state.dm_results:
        cgsl.success_message(
            "Operation result:\n\n" +
            f"{cgsl.format_results(st.session_state.dm_results)}",
            container=data_management_container)
        st.session_state.dm_results = None

    # Process the ideation-from-prompt buttons
    cgsl.process_no_form_buttons(
        "ideation_from_prompt", question,
        show_ideation_from_prompt, process_ideation_form)


def add_footer():
    """
    Add the footer to the page
    """
    st.caption(f"© 2024 {st.session_state.maker_name}. All rights reserved.")


# Pages


def page_1():
    # Get suggested questions initial value
    with st.spinner("Loading App..."):
        if "suggestion" not in st.session_state:
            if cgsl.get_par_value("DYNAMIC_SUGGESTIONS", True):
                cgsl.recycle_suggestions()
            else:
                st.session_state.suggestion = \
                    cgsl.get_par_value("DEFAULT_SUGGESTIONS")

    # Main content

    # Title
    add_title()

    # Suggestions
    add_suggestions()

    # Models selection
    add_models_selection()

    # Attachments
    add_attachments()

    # Process the use_response_as_prompt button pushed
    process_use_response_as_prompt()

    # User input
    question = add_user_input()

    # Additional parameters SECTION
    _, parameters_container = add_results_containers()

    # Tabs defintion
    tab1, tab2, tab3 = st.tabs(["Main", "App Ideation", "Code Generation"])

    # When a tab is changed, reset the question label
    # tab1.on_change("active", lambda: get_question_label("main"))
    # tab2.on_change("active", lambda: get_question_label("app_ideation"))
    # tab3.on_change("active", lambda: get_question_label("code_gen"))

    with tab1:
        # Buttons
        add_buttons_for_main_tab()

    with tab2:
        # Idea from Form
        form = show_ideation_form(tab2)
        if form:
            process_ideation_form(form, get_ideation_form_config())

        # Idea from Prompt
        st.session_state.forms_config["ideation_from_prompt"] = \
            get_ideation_from_prompt_config()
        show_ideation_from_prompt(tab2, "show_form")

    with tab3:
        # Buttons
        add_buttons_for_code_gen_tab()

    # Results containers
    additional_result_container, result_container = add_results_containers()

    # Show the selected conversation's question and answer in the
    # main section
    add_show_selected_conversation(
        result_container,
        additional_result_container)

    # Sidebar
    data_management_container = add_sidebar()

    # Check buttons pushed
    add_check_buttons_pushed(
        result_container,
        additional_result_container,
        data_management_container,
        parameters_container,
        question,
    )

    # Footer
    with st.container():
        add_footer()


# Page 2: Video Gallery
def page_2():
    cgsl.show_gallery("video")
    # Footer
    add_footer()


# Page 3: Image Gallery
def page_3():
    cgsl.show_gallery("image")
    # Footer
    add_footer()


# Main


# Main function to render pages
def main():
    load_dotenv()

    st.session_state.app_name = cgsl.get_par_or_env("APP_NAME")
    st.session_state.app_version = cgsl.get_par_or_env("APP_VERSION")
    st.session_state.app_name_version = \
        f"{st.session_state.app_name} v{st.session_state.app_version}"
    st.session_state.maker_name = cgsl.get_par_or_env("MAKER_MAME")
    st.session_state.app_icon = cgsl.get_par_or_env("APP_ICON", ":sparkles:")

    if "question" not in st.session_state:
        st.session_state.question = ""
    if "prompt_enhancement_flag" not in st.session_state:
        st.session_state.prompt_enhancement_flag = False
    if "use_response_as_prompt_flag" not in st.session_state:
        st.session_state.use_response_as_prompt_flag = False
    if "use_embeddings_flag" not in st.session_state:
        st.session_state.use_embeddings_flag = True
    if "conversations" not in st.session_state:
        cgsl.update_conversations()
    if "question_label" not in st.session_state:
        get_question_label()
    if "forms_config" not in st.session_state:
        st.session_state.forms_config = {}

    # Streamlit app code
    st.set_page_config(
        page_title=st.session_state.app_name_version,
        page_icon=st.session_state.app_icon,
        layout="wide",
        initial_sidebar_state="auto",
    )

    # Query params to handle navigation
    page = st.query_params.get("page", cgsl.get_par_value("DEFAULT_PAGE"))

    # Page navigation logic
    if page == "video_gallery":
        page_2()
    elif page == "image_gallery":
        page_3()
    # if page == "home":
    #     page_1()
    else:
        # Defaults to home
        page_1()


if __name__ == "__main__":
    main()
</file>

</files>
