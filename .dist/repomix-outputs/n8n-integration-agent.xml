This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
demo.py
N8N_Agent_Creator__Sub_Flow.json
N8N_Agent_creator.json
n8n_integration_agent.py
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="demo.py">
"""
Demo script for N8N Integration Agent functionality.
This script demonstrates the caching and performance analysis capabilities.
"""

import os
import json
import asyncio

# Mock implementation for demonstration purposes
class MockN8NIntegrationAgent:
    def __init__(self, enable_caching=True, performance_tracking=True):
        self.enable_caching = enable_caching
        self.performance_tracking = performance_tracking
        self.cache = {}
        self.optimization_suggestions = set()
        self.performance_metrics = {}
        print(f"Initialized MockN8NIntegrationAgent with caching={enable_caching}, performance_tracking={performance_tracking}")
    
    def generate_cache_key(self, *args):
        """Generate a cache key from the arguments"""
        return "_".join(str(arg) for arg in args)
    
    def set_cached_result(self, key, value):
        """Store a result in the cache"""
        self.cache[key] = {
            "value": value,
            "timestamp": 123456789,
            "ttl": 3600
        }
        print(f"Cached result with key: {key}")
    
    def get_cached_result(self, key):
        """Get a result from the cache"""
        if key in self.cache:
            print(f"Cache hit for key: {key}")
            return self.cache[key]["value"]
        print(f"Cache miss for key: {key}")
        return None
    
    def clear_cache(self, pattern=None):
        """Clear the cache"""
        before_size = len(self.cache)
        if pattern:
            keys_to_remove = [k for k in self.cache.keys() if pattern in k]
            for k in keys_to_remove:
                del self.cache[k]
        else:
            self.cache.clear()
        print(f"Cleared cache: {before_size} items before, {len(self.cache)} items after")
        return {
            "status": "success",
            "cache_size_before": before_size,
            "cache_size_after": len(self.cache)
        }
    
    def get_cache_stats(self):
        """Get statistics about the cache"""
        print(f"Cache stats: {len(self.cache)} items in cache")
        return {
            "status": "success",
            "total_items": len(self.cache),
            "expired_items": 0,
            "active_items": len(self.cache)
        }
    
    def export_optimization_suggestions(self):
        """Export optimization suggestions"""
        # Add some example suggestions
        self.optimization_suggestions.add("Use Split In Batches nodes for parallel processing")
        self.optimization_suggestions.add("Cache results of expensive operations")
        self.optimization_suggestions.add("Implement retry logic for unreliable APIs")
        
        print(f"Exported {len(self.optimization_suggestions)} optimization suggestions")
        return {
            "status": "success",
            "total_suggestions": len(self.optimization_suggestions),
            "all_suggestions": list(self.optimization_suggestions)
        }
    
    def export_performance_analysis(self, workflow_json):
        """Export performance analysis for a workflow"""
        analysis = {
            "status": "success",
            "workflow_metrics": {
                "node_count": 10,
                "connection_count": 12,
                "api_node_count": 3
            },
            "complexity_analysis": {
                "complexity_score": 15.5,
                "complexity_level": "Medium"
            },
            "performance_risks": [
                "High number of API calls may cause performance issues"
            ]
        }
        print(f"Exported performance analysis for workflow")
        return analysis

async def demo():
    """Run a demonstration of the agent's capabilities"""
    print("Starting N8N Integration Agent Demo")
    
    # Create the agent
    agent = MockN8NIntegrationAgent()
    
    # Demonstrate caching
    print("\n=== Caching Demonstration ===")
    cache_key = agent.generate_cache_key("template", "data_processing", "customization1")
    
    # First access (cache miss)
    result = agent.get_cached_result(cache_key)
    if result is None:
        print("Generating new result...")
        result = {"data": "This is a generated result"}
        agent.set_cached_result(cache_key, result)
    
    # Second access (cache hit)
    cached_result = agent.get_cached_result(cache_key)
    print(f"Retrieved from cache: {cached_result}")
    
    # Get cache statistics
    cache_stats = agent.get_cache_stats()
    print(f"Cache statistics: {json.dumps(cache_stats, indent=2)}")
    
    # Demonstrate performance analysis
    print("\n=== Performance Analysis Demonstration ===")
    workflow_json = {
        "nodes": [{"id": "1", "type": "HttpRequest"}, {"id": "2", "type": "Set"}],
        "connections": {"main": [{"node": "1", "type": "main", "index": 0}]}
    }
    
    performance_analysis = agent.export_performance_analysis(workflow_json)
    print(f"Performance analysis: {json.dumps(performance_analysis, indent=2)}")
    
    # Demonstrate optimization suggestions
    print("\n=== Optimization Suggestions Demonstration ===")
    optimization_suggestions = agent.export_optimization_suggestions()
    print(f"Optimization suggestions: {json.dumps(optimization_suggestions, indent=2)}")
    
    # Demonstrate cache clearing
    print("\n=== Cache Clearing Demonstration ===")
    clear_result = agent.clear_cache()
    print(f"Cache clearing result: {json.dumps(clear_result, indent=2)}")
    
    print("\nDemo completed successfully!")

if __name__ == "__main__":
    asyncio.run(demo())
</file>

<file path="N8N_Agent_Creator__Sub_Flow.json">
{
  "name": "N8N Agent Creator (Sub-Flow)",
  "nodes": [
    {
      "parameters": {
        "inputSource": "passthrough"
      },
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1,
      "position": [
        -1860,
        -80
      ],
      "id": "d93ff158-5277-47be-ab1c-d78789606fef",
      "name": "Workflow Input Trigger"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "$input.item.json.body = {\n    \"model\": \"sonar-reasoning-pro\",\n    \"messages\": [\n        { \"role\": \"user\", \"content\": `You are an expert in n8n workflow automation.\n\nYour task is to:\n1. **Convert the structured workflow plan into a functional JSON-based n8n workflow**.\n2. **Ensure that OpenAI is used for any intent classification or content generation tasks.**\n3. **Use Perplexity only for information retrieval** (not text transformation).\n4. **Ensure all nodes are properly connected** with valid parameters.\n5. **Return only the JSON output**, with no explanations.\n6. **Example JSON Snippets under each Step are given to refernce and use**.\n\n## Notes for using Chat model under the AI Agent Node:\n- If classifying intent → **Use OpenAI Chat Model (GPT-4o)**.\n- If rewriting email text → **Use OpenAI Chat Model (GPT-4o)**.\n- If searching the web → **Use Perplexity Search API**.\n- Ensure all variable references are **valid**.\n\n1) PropertyName issues: There are no reasons to get propertyName is property value errors, and you generate perfect syntax.\n\n2) Comments Removed: No inline comments (using //) cause the JSON to be invalid.\n\n3) Expressions: The expressions using ={{ ... }} remain intact. These are valid in n8n's JSON configuration as they are processed at runtime.\n\n4) Structure: The structure with \"nodes\" and \"connections\" is preserved as expected by n8n.\n\n5) Add the name of the workflow inside the JSON as well. It should be in proper syntax and shouldn't break the JSON\n\n\n## Here is the structured workflow plan you have to work on:\nQUERY: \"${$json.query}\"\nPLAN: \"${$json.choices[0].message.content}\"\n\n\n` }\n    ]\n};\n\nreturn $input.item;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -160,
        360
      ],
      "id": "daf8c4da-f163-42c9-add6-56330da299fb",
      "name": "Create JSON Body"
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "o3-mini",
          "mode": "id"
        },
        "messages": {
          "values": [
            {
              "content": "=You are an expert in JSON validation and workflow automation. Your task is to **verify and correct an n8n workflow JSON**.\n\n## **Validation Rules**\nEnsure that the JSON does not suffer from any of the following issues:\n1. **PropertyName Issues**: There should be no missing property names or property value errors. The JSON syntax must be correct.\n2. **Comments Removed**: Do not include inline comments (e.g., `// comment`). JSON must be fully valid.\n3. **Expressions Remain Intact**: Expressions using must remain unchanged. These are valid in n8n's JSON configuration and processed at runtime.\n4. **Structure Validity**: The `\"nodes\"` and `\"connections\"` structure must be preserved exactly as expected by n8n.\n\n## **Output Rules**\n- **Return only the corrected JSON** with no extra text, comments, explanations, or formatting.\n- **Do not include any introductory or concluding remarks** (e.g., \"Here is your corrected JSON\").\n- **Ensure the JSON is formatted correctly** and can be imported into n8n without modification.\n- Check decimal points to typeVersion values - Usually are in decimals (1.0, 3.0, 1.7) instead of integers\n- Check if any AI Agent nodes parameter structure with promptType and text format is correct or not\n- Ensure the Set nodes to use the proper structure with named string values in arrays\n- Ensure there are no missing object in all NODES like options objects in various nodes where required\n\n\n\nI'll also provide a similar pre-built n8n workflow JSON file which you can reference in reviewing the JSON sample.\nHere's a reference JSON workflow:\n\nAgent Title: {{ $json.generated_title }}\nAgent Summary: {{ $json.agent_summary }}\nJSON File:\n{{ $json.json_file }}\n\nHere is the JSON sample to review:\n{{ $json.choices[0].message.content }}",
              "role": "system"
            }
          ]
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.8,
      "position": [
        280,
        860
      ],
      "id": "6c0dc3c6-6aff-459b-88e3-db3445166925",
      "name": "Validate JSON",
      "credentials": {
        "openAiApi": {
          "id": "bBtD8KtcEPsOhElY",
          "name": "Prompt Advisers OpenAI Account"
        }
      }
    },
    {
      "parameters": {
        "operation": "create",
        "workflowObject": "={{ $json.message.content }}",
        "requestOptions": {}
      },
      "type": "n8n-nodes-base.n8n",
      "typeVersion": 1,
      "position": [
        300,
        1080
      ],
      "id": "7b3eb7e9-fa6c-48da-b5de-3a4f9e63b20c",
      "name": "n8n",
      "credentials": {
        "n8nApi": {
          "id": "UcW98TG9vHtiDFq2",
          "name": "Mark's PropmtAdviser Account"
        }
      }
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "987fcd43-60b1-4a68-a1f5-23f2d7d4ae28",
              "name": "Workflow URL",
              "value": "=https://promptadvisers.app.n8n.cloud/workflow/{{ $json.id }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        460,
        1080
      ],
      "id": "37b3db9a-bf99-47f5-a74e-098608b252ac",
      "name": "Edit Fields"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "$input.item.json.body = {\n    \"model\": \"sonar-pro\",\n    \"messages\": [\n        { \"role\": \"user\", \"content\": `You are an expert in JSON validation for n8n workflows.\n\nYour task is to:\n1. **Validate that OpenAI is used for all LLM tasks** (intent classification, content generation).\n2. **Ensure Perplexity is only used for web search** (not rewriting text).\n3. **Check that all nodes are properly connected**—no missing links or broken automation.\n4. **Ensure variable references are correct** to prevent execution errors.\n5. **Return only the corrected JSON workflow**, with no explanations.\n\n## Validation Checklist:\n- OpenAI Chat Model is used for LLM tasks.\n- Perplexity is used **only** for search.\n- No missing or disconnected nodes.\n- JSON structure is valid and functional.\n\n1) PropertyName issues: There are no reasons to get propertyName is property value errors, and you generate perfect syntax.\n\n2) Comments Removed: No inline comments (using //) cause the JSON to be invalid.\n\n3) Expressions: The expressions using ={{ ... }} remain intact. These are valid in n8n's JSON configuration as they are processed at runtime.\n\n4) Structure: The structure with \"nodes\" and \"connections\" is preserved as expected by n8n.\n\nFirst I'll provide a pre-built similar AI workflow that you can reference in validation the sample JSON. Here's a similar AI agent workflow with JSON file that you can reference (only use this as reference):\n\nAgent Title: \"${$json.generated_title}\"\nAgent Summary: \"${$json.agent_summary}\"\nJSON File:\n\"${$json.json_file}\"\n\nHere is the JSON workflow to validate:\n\n${$json.choices[0].message.content}\n\nPlease return the corrected JSON workflow.` }\n    ]\n};\n\nreturn $input.item;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -160,
        580
      ],
      "id": "c6af3701-5eec-4567-9356-aea5699296e9",
      "name": "Create JSON Body1"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "$input.item.json.body = {\n    \"model\": \"sonar-pro\",\n    \"messages\": [\n        { \"role\": \"user\", \"content\": \"You are an automation expert specializing in n8n workflows.\\n\\n\" +\n        \n        \"Your job is to take the following request and generate a structured, step-by-step workflow that follows n8n's best practices.\\n\\n\" +\n\n        \"## Decision on Which node to Chose:\\n\" +\n          \"- When a step with AI is involved, analyze and chose which node to use: 1. AI Agent Node, 2. OpenAI GPT-40 Node\\n\" +\n          \"- Use AI Agent Node attached with OpenAI GPT-4o as the Model for any heavy LLM tasks where chat history or memory is needed.\\n\" +\n        \"- Use AI agent Node if a Chat node is used to Chat with AI or communication with Vector Store and retrieval of data from Vector is needed\\n\" + \n        \"- If AI content generation, simple summarization or other generic task is needed, use OpenAI chat completion instead of a generic AI Agent Node.\\n\\n\" +\n          \n        \"## Workflow Structuring Rules:\\n\" +\n        \"- Use the correct input trigger (Webhook, Slack, API, Chat).\\n\" +\n        \"- Use Perplexity only for web search, NOT for content transformation.\\n\" +\n        \"- Ensure the workflow is fully connected with no broken paths.\\n\\n\" +\n          \n        \"## Required Output:\\n\" +\n        \"- Provide a structured execution plan with:\\n\" +\n        \"  1. Trigger Node (Slack, API, Webhook, etc.).\\n\" +\n        \"  2. Processing Nodes (AI classification, AI Agent Node, API requests, etc.).\\n\" +\n        \"  3. Action Nodes (Sending responses, updating databases, etc.).\\n\" +\n        \"  4. Final Step (Webhook response or UI message confirmation).\\n\\n\" +\n       \n        \"\\n\\n## Here is the request which you have to work on:\\n\\n\" + \n        $json.query \n        }\n    ]\n};\n\nreturn $input.item;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -160,
        -80
      ],
      "id": "65db3bcb-e025-4163-a8fb-03dfe132bed0",
      "name": "Create JSON Body2"
    },
    {
      "parameters": {
        "model": "text-embedding-3-large",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "typeVersion": 1.2,
      "position": [
        -1400,
        200
      ],
      "id": "d2ca391d-f9c0-4ad7-a216-b34989eca7a3",
      "name": "Embeddings OpenAI1",
      "credentials": {
        "openAiApi": {
          "id": "bBtD8KtcEPsOhElY",
          "name": "Prompt Advisers OpenAI Account"
        }
      }
    },
    {
      "parameters": {
        "modelId": {
          "__rl": true,
          "value": "gpt-4o",
          "mode": "list",
          "cachedResultName": "GPT-4O"
        },
        "messages": {
          "values": [
            {
              "content": "=You are an intelligent vector store who has a large database of n8n workflows and ai agents stored in your vector database.\n\nYour job is to generate 1 short searching keywords/prompts that allow to search the vector store with most similar workflows to the query given below. The searching prompt can be a single keyword or a few words because the vector store will be finding workflows based on the similarity ranking following the embeddings model attached to it.\n\nAlso the keyword \"AI\" & \"AI Agent\" is mostly favoured. So if there is a searching keyword with any of the two then mix them as well\n\nOutput in JSON format in the following structure:\n{\n   \"prompt\": \"\"\n}\n\nHere's the query:\n{{ $json.query }}"
            }
          ]
        },
        "jsonOutput": true,
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.openAi",
      "typeVersion": 1.8,
      "position": [
        -1680,
        -80
      ],
      "id": "363f811c-c60b-40d1-89b7-a88e80a3246a",
      "name": "OpenAI",
      "credentials": {
        "openAiApi": {
          "id": "bBtD8KtcEPsOhElY",
          "name": "Prompt Advisers OpenAI Account"
        }
      }
    },
    {
      "parameters": {
        "model": "text-embedding-3-large",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.embeddingsOpenAi",
      "typeVersion": 1.2,
      "position": [
        -1040,
        180
      ],
      "id": "66b773db-98fe-4e1a-8ad7-ffafef675fc9",
      "name": "Embeddings OpenAI2",
      "credentials": {
        "openAiApi": {
          "id": "bBtD8KtcEPsOhElY",
          "name": "Prompt Advisers OpenAI Account"
        }
      }
    },
    {
      "parameters": {
        "mode": "load",
        "pineconeIndex": {
          "__rl": true,
          "value": "n8n-templates",
          "mode": "list",
          "cachedResultName": "n8n-templates"
        },
        "prompt": "={{ $json.message.content.prompt }}",
        "topK": 1,
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "typeVersion": 1,
      "position": [
        -1340,
        -80
      ],
      "id": "536bab0c-1185-47a5-af77-6c30af979195",
      "name": "Find one workflow",
      "credentials": {
        "pineconeApi": {
          "id": "GO2UfDObJ0c6SULt",
          "name": "Prompt Advisers Pinecone Account"
        }
      }
    },
    {
      "parameters": {
        "mode": "load",
        "pineconeIndex": {
          "__rl": true,
          "value": "n8n-templates",
          "mode": "list",
          "cachedResultName": "n8n-templates"
        },
        "prompt": "={{ $('OpenAI').item.json.message.content.prompt }}",
        "topK": 5,
        "options": {
          "metadata": {
            "metadataValues": [
              {
                "name": "generated_title",
                "value": "={{ $json.document.metadata.generated_title }}"
              }
            ]
          }
        }
      },
      "type": "@n8n/n8n-nodes-langchain.vectorStorePinecone",
      "typeVersion": 1,
      "position": [
        -1000,
        -80
      ],
      "id": "d747e4bf-bbe8-4355-90a1-d55d772d20ff",
      "name": "Find remaining chunks",
      "credentials": {
        "pineconeApi": {
          "id": "GO2UfDObJ0c6SULt",
          "name": "Prompt Advisers Pinecone Account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Initialize an empty string to store the concatenated json_file values\nlet mergedJsonFile = \"\";\n\n// Get all input items\nlet items = $input.all();\n\n// Sort items by chunk_index in ascending order\nitems.sort((a, b) => a.json.document.metadata.chunk_index - b.json.document.metadata.chunk_index);\n\n// Loop over the sorted items and concatenate their json_file values\nfor (const item of items) {\n    if (item.json.document.metadata.json_file) {\n        mergedJsonFile += item.json.document.metadata.json_file; // Concatenates the strings in order\n    }\n}\n\n// Return a single output with the merged json_file\nreturn [{ json: { json_file: mergedJsonFile } }];\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -600,
        -80
      ],
      "id": "64b9739d-4dc0-4dbd-877d-ab130526c062",
      "name": "Combine the JSON File of Chunks"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "05db9593-b34e-40f6-8970-45ccd8466338",
              "name": "query",
              "value": "={{ $('Workflow Input Trigger').first().json.query }}",
              "type": "string"
            },
            {
              "id": "e7180c38-855e-429f-9399-854c03a504c2",
              "name": "json_file",
              "value": "={{ $json.json_file }}",
              "type": "string"
            },
            {
              "id": "f1c0e153-5055-476a-ad96-47fbb70ed573",
              "name": "generated_title",
              "value": "={{ $('Find one workflow').first().json.document.metadata.generated_title }}",
              "type": "string"
            },
            {
              "id": "17d896bc-37ff-4a59-b88e-055cabd465ed",
              "name": "agent_summary",
              "value": "={{ $('Find one workflow').first().json.document.metadata.agent_summary }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -380,
        -80
      ],
      "id": "7ecdf947-47ce-4c35-bc66-4b47bce3db8a",
      "name": "Edit Fields1"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "05db9593-b34e-40f6-8970-45ccd8466338",
              "name": "query",
              "value": "={{ $('Workflow Input Trigger').first().json.query }}",
              "type": "string"
            },
            {
              "id": "e7180c38-855e-429f-9399-854c03a504c2",
              "name": "json_file",
              "value": "={{ $('Create JSON Body2').item.json.json_file }}",
              "type": "string"
            },
            {
              "id": "f1c0e153-5055-476a-ad96-47fbb70ed573",
              "name": "generated_title",
              "value": "={{ $('Find one workflow').first().json.document.metadata.generated_title }}",
              "type": "string"
            },
            {
              "id": "17d896bc-37ff-4a59-b88e-055cabd465ed",
              "name": "agent_summary",
              "value": "={{ $('Find one workflow').first().json.document.metadata.agent_summary }}",
              "type": "string"
            },
            {
              "id": "e55e94e9-d7a3-46e4-bdbd-b70c1b6055ec",
              "name": "choices",
              "value": "={{ $json.choices }}",
              "type": "array"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -380,
        140
      ],
      "id": "c6d4117e-bdcf-4bcb-b412-3324352753a2",
      "name": "Edit Fields2"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "05db9593-b34e-40f6-8970-45ccd8466338",
              "name": "query",
              "value": "={{ $('Workflow Input Trigger').first().json.query }}",
              "type": "string"
            },
            {
              "id": "e7180c38-855e-429f-9399-854c03a504c2",
              "name": "json_file",
              "value": "={{ $('Create JSON Body2').item.json.json_file }}",
              "type": "string"
            },
            {
              "id": "f1c0e153-5055-476a-ad96-47fbb70ed573",
              "name": "generated_title",
              "value": "={{ $('Find one workflow').first().json.document.metadata.generated_title }}",
              "type": "string"
            },
            {
              "id": "17d896bc-37ff-4a59-b88e-055cabd465ed",
              "name": "agent_summary",
              "value": "={{ $('Find one workflow').first().json.document.metadata.agent_summary }}",
              "type": "string"
            },
            {
              "id": "e55e94e9-d7a3-46e4-bdbd-b70c1b6055ec",
              "name": "choices",
              "value": "={{ $json.choices }}",
              "type": "array"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        60,
        860
      ],
      "id": "ec66520d-118e-4c51-8970-5bd9b5a9b52b",
      "name": "Edit Fields3"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "05db9593-b34e-40f6-8970-45ccd8466338",
              "name": "query",
              "value": "={{ $('Workflow Input Trigger').first().json.query }}",
              "type": "string"
            },
            {
              "id": "e7180c38-855e-429f-9399-854c03a504c2",
              "name": "json_file",
              "value": "={{ $('Create JSON Body2').item.json.json_file }}",
              "type": "string"
            },
            {
              "id": "f1c0e153-5055-476a-ad96-47fbb70ed573",
              "name": "generated_title",
              "value": "={{ $('Find one workflow').first().json.document.metadata.generated_title }}",
              "type": "string"
            },
            {
              "id": "17d896bc-37ff-4a59-b88e-055cabd465ed",
              "name": "agent_summary",
              "value": "={{ $('Find one workflow').first().json.document.metadata.agent_summary }}",
              "type": "string"
            },
            {
              "id": "e55e94e9-d7a3-46e4-bdbd-b70c1b6055ec",
              "name": "choices",
              "value": "={{ $json.choices }}",
              "type": "array"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -380,
        580
      ],
      "id": "a717e31e-67a7-4950-87e7-7d351e9b2aeb",
      "name": "Edit Fields4"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "$input.item.json.body = {\n    \"model\": \"sonar-pro\",\n    \"messages\": [\n        { \n            \"role\": \"user\", \n            \"content\": `You are an expert in n8n workflow automation.\n\nYour task is to:\n1. **Analyze the Structure Plan of an AI agent Workflow**.\n2. **Add a valid N8N JSON Snippet of each node under each step. For example if there is a step to add AI Agent Node then Add an example AI Agent node JSON Snippet for easier reference. For example:\n\n\\`\\`\\`json\n{\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"promptType\": \"define\",\n        \"text\": \"=**System Prompt:**\\\\n\\\\nYou are an AI assistant designed to process new leads and generate appropriate responses. Your role includes analyzing lead notes, categorizing them, and generating an email from the system to inform the relevant contact about the inquiry. Do not send the email as if it is directly from the customer; instead, draft it as a notification from the system summarizing the inquiry.\\\\n\\\\n### **Process Flow**\\\\n\\\\n1. **Analyzing Lead Notes:**\\\\n - Extract key details such as the customer name, organization, contact information, and their specific request. \\\\n - Determine if the inquiry relates to products, services, or solutions offered by the company.\\\\n\\\\n2. **Finding the Appropriate Contact(s):**\\\\n - Search the contact database to find the responsible person(s) for the relevant product, service, or solution. \\\\n - If one person is responsible, provide their email. \\\\n - If multiple people are responsible, list all emails separated by commas.\\\\n\\\\n3. **Generating an Email Notification:**\\\\n - Draft a professional email as a notification from the system.\\\\n - Summarize the customer's inquiry.\\\\n - Include all relevant details to assist the recipient in addressing the inquiry.\\\\n\\\\n4. **Handling Invalid Leads:**\\\\n - If the inquiry is unrelated to products, services, or solutions (e.g., job inquiries or general product inquiries), classify it as invalid and return: \\\\n \\\\\\\\\\\\\\\"Invalid Lead - Not related to products, services, or solutions.\\\\\\\\\\\\\\\"\\\\n\\\\n\n      ,\n        \"options\": {}\n      },\n      \"id\": \"23291d25-3e1a-4b0d-9b1d-d066e8c04a1f\",\n      \"name\": \"Customer Lead AI Agent\",\n      \"type\": \"@n8n/n8n-nodes-langchain.agent\",\n      \"position\": [-640, 460],\n      \"typeVersion\": 1.7\n    }\n  ],\n  \"connections\": {\n    \"Customer Lead AI Agent\": {\n      \"main\": [[]]\n    }\n  },\n  \"pinData\": {},\n  \"meta\": {\n    \"instanceId\": \"aaadb797535f05587ee95b776c942a7c3f7a46fd7aa0c9b6a9d64e1e595f8af1\"\n  }\n}\n\\`\\`\\`\n\n3. You will also be given a sample N8N JSON workflow so if you don't know any node snippet find it from there. And if its not in the sample then search the internet.\n\n## **Notes for using models under the AI Agent Nodes:**\n- If classifying intent → **Use OpenAI Chat Model (GPT-4o)**.\n- If rewriting email text → **Use OpenAI Chat Model (GPT-4o)**.\n- If searching the web → **Use Perplexity Search API**.\n- Ensure all variable references are **valid**.\n\n\n\nHere's a similar AI agent workflow with JSON file that you should reference to find node snippets as well in creating a valid structured workflow:\n\nAgent Title: \"${$json.generated_title}\"\nAgent Summary: \"${$json.agent_summary}\"\nJSON File:\n\"${$json.json_file}\"\n\n\n## **Here is the structured workflow plan you have to work on:**\nQUERY: \"${$json.query}\"\nPLAN: \"${$json.choices[0].message.content}\"\n\n## OUTPUT Requirements:\n- Output the Structure plan As is with the JSON Snippets added\n`\n        }\n    ]\n};\n\nreturn $input.item;\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -160,
        140
      ],
      "id": "a8a6f178-2344-455c-aeb8-f26d069a55ca",
      "name": "Create JSON Body4"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "05db9593-b34e-40f6-8970-45ccd8466338",
              "name": "query",
              "value": "={{ $('Workflow Input Trigger').first().json.query }}",
              "type": "string"
            },
            {
              "id": "e7180c38-855e-429f-9399-854c03a504c2",
              "name": "json_file",
              "value": "={{ $('Create JSON Body4').item.json.json_file }}",
              "type": "string"
            },
            {
              "id": "f1c0e153-5055-476a-ad96-47fbb70ed573",
              "name": "generated_title",
              "value": "={{ $('Find one workflow').first().json.document.metadata.generated_title }}",
              "type": "string"
            },
            {
              "id": "17d896bc-37ff-4a59-b88e-055cabd465ed",
              "name": "agent_summary",
              "value": "={{ $('Find one workflow').first().json.document.metadata.agent_summary }}",
              "type": "string"
            },
            {
              "id": "e55e94e9-d7a3-46e4-bdbd-b70c1b6055ec",
              "name": "choices",
              "value": "={{ $json.choices }}",
              "type": "array"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -380,
        360
      ],
      "id": "65bfaaa3-b1f3-4a11-ad6b-6f307ea6ad52",
      "name": "Edit Fields6"
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.perplexity.ai/chat/completions",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ $json.body }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        60,
        140
      ],
      "id": "8e206260-9689-4058-9438-e6206ef7c66f",
      "name": "Perplexity: Add Snippets",
      "credentials": {
        "httpHeaderAuth": {
          "id": "Y9DJmWSU5RHlmbJQ",
          "name": "Prompt Advisers Perplexity Credentials"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.perplexity.ai/chat/completions",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ $json.body }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        60,
        -80
      ],
      "id": "b90a2c22-5cb9-4f74-87be-99d1e6dba1f4",
      "name": "Perplexity: Create Structure Plan",
      "credentials": {
        "httpHeaderAuth": {
          "id": "Y9DJmWSU5RHlmbJQ",
          "name": "Prompt Advisers Perplexity Credentials"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.perplexity.ai/chat/completions",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ $json.body }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        60,
        360
      ],
      "id": "51c24a6c-f4aa-4d2f-bf33-4695d388d49d",
      "name": "Perplexity: Create JSON",
      "credentials": {
        "httpHeaderAuth": {
          "id": "Y9DJmWSU5RHlmbJQ",
          "name": "Prompt Advisers Perplexity Credentials"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.perplexity.ai/chat/completions",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ $json.body }}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        60,
        580
      ],
      "id": "92bc15ba-6e64-4adc-82df-27a8dfedbfa7",
      "name": "Perplexity: Validate JSON",
      "credentials": {
        "httpHeaderAuth": {
          "id": "Y9DJmWSU5RHlmbJQ",
          "name": "Prompt Advisers Perplexity Credentials"
        }
      }
    }
  ],
  "pinData": {
    "Workflow Input Trigger": [
      {
        "json": {
          "query": "AI meeting booking agent using Google Calendar. It should be able to schedule, reschedule, and cancel meetings in Google Calendar, check participants' availability, and send calendar invites. Additionally, the agent should handle time zone differences and provide reminders for upcoming meetings."
        }
      }
    ]
  },
  "connections": {
    "Workflow Input Trigger": {
      "main": [
        [
          {
            "node": "OpenAI",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create JSON Body": {
      "main": [
        [
          {
            "node": "Perplexity: Create JSON",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Validate JSON": {
      "main": [
        [
          {
            "node": "n8n",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "n8n": {
      "main": [
        [
          {
            "node": "Edit Fields",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create JSON Body1": {
      "main": [
        [
          {
            "node": "Perplexity: Validate JSON",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create JSON Body2": {
      "main": [
        [
          {
            "node": "Perplexity: Create Structure Plan",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings OpenAI1": {
      "ai_embedding": [
        [
          {
            "node": "Find one workflow",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI": {
      "main": [
        [
          {
            "node": "Find one workflow",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Embeddings OpenAI2": {
      "ai_embedding": [
        [
          {
            "node": "Find remaining chunks",
            "type": "ai_embedding",
            "index": 0
          }
        ]
      ]
    },
    "Find one workflow": {
      "main": [
        [
          {
            "node": "Find remaining chunks",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Find remaining chunks": {
      "main": [
        [
          {
            "node": "Combine the JSON File of Chunks",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Combine the JSON File of Chunks": {
      "main": [
        [
          {
            "node": "Edit Fields1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Edit Fields1": {
      "main": [
        [
          {
            "node": "Create JSON Body2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Edit Fields2": {
      "main": [
        [
          {
            "node": "Create JSON Body4",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Edit Fields3": {
      "main": [
        [
          {
            "node": "Validate JSON",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Edit Fields4": {
      "main": [
        [
          {
            "node": "Create JSON Body1",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create JSON Body4": {
      "main": [
        [
          {
            "node": "Perplexity: Add Snippets",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Edit Fields6": {
      "main": [
        [
          {
            "node": "Create JSON Body",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Perplexity: Add Snippets": {
      "main": [
        [
          {
            "node": "Edit Fields6",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Perplexity: Create Structure Plan": {
      "main": [
        [
          {
            "node": "Edit Fields2",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Perplexity: Create JSON": {
      "main": [
        [
          {
            "node": "Edit Fields4",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Perplexity: Validate JSON": {
      "main": [
        [
          {
            "node": "Edit Fields3",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1",
    "callerPolicy": "any"
  },
  "versionId": "c547cbb4-41d6-460e-9232-8951681951c5",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "aaadb797535f05587ee95b776c942a7c3f7a46fd7aa0c9b6a9d64e1e595f8af1"
  },
  "id": "m3VURatpsyZna9N6",
  "tags": []
}
</file>

<file path="N8N_Agent_creator.json">
{
  "name": "N8N Agent creator",
  "nodes": [
    {
      "parameters": {
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "typeVersion": 1.1,
      "position": [
        -340,
        0
      ],
      "id": "0890a4b2-9bf0-4cf7-9478-9c2df59200e7",
      "name": "When chat message received",
      "webhookId": "2892f8b8-fd03-4c02-bef4-f207e52f5e74"
    },
    {
      "parameters": {
        "options": {
          "systemMessage": "You are a helpful assistant"
        }
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 1.7,
      "position": [
        200,
        0
      ],
      "id": "0cf29866-d6c5-42da-93ce-b1f4522795da",
      "name": "AI Agent"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "value": "gpt-4o-mini",
          "mode": "id"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        160,
        260
      ],
      "id": "beb981b1-fc42-4b9a-ba1f-830c30d535cc",
      "name": "OpenAI Chat Model",
      "credentials": {
        "openAiApi": {
          "id": "bBtD8KtcEPsOhElY",
          "name": "Prompt Advisers OpenAI Account"
        }
      }
    },
    {
      "parameters": {
        "name": "N8N_agent_Creator",
        "description": "Call this tool to create a AI agent after you have received the 'query' variable.\n",
        "workflowId": {
          "__rl": true,
          "value": "m3VURatpsyZna9N6",
          "mode": "list",
          "cachedResultName": "N8N Agent Creator (Sub-Flow)"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {},
          "matchingColumns": [],
          "schema": [],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        }
      },
      "type": "@n8n/n8n-nodes-langchain.toolWorkflow",
      "typeVersion": 2,
      "position": [
        540,
        320
      ],
      "id": "61ff3241-a50c-4f11-bc6a-e8093707211d",
      "name": "Create AI Agent"
    }
  ],
  "pinData": {},
  "connections": {
    "When chat message received": {
      "main": [
        [
          {
            "node": "AI Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "AI Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Create AI Agent": {
      "ai_tool": [
        [
          {
            "node": "AI Agent",
            "type": "ai_tool",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "0a82db76-cfab-4483-802c-e5957d32a589",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "aaadb797535f05587ee95b776c942a7c3f7a46fd7aa0c9b6a9d64e1e595f8af1"
  },
  "id": "G6ab3xs0kjSsg00h",
  "tags": []
}
</file>

<file path="n8n_integration_agent.py">
"""
N8N Integration Agent - Intelligent agent for integrating SolnAI with N8N workflows.
Built on AutoGen v0.4.7 with Claude 3.7 Sonnet integration.
"""

import os
import sys
import logging
import json
import requests
import time
import hashlib
import datetime
import statistics
from typing import Dict, List, Any, Optional, Union, Tuple, Callable, Set
import asyncio

# Import AutoGen components
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
from autogen_core.model_context import BufferedChatCompletionContext

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class N8NIntegrationAgent:
    """
    N8N Integration Agent for connecting SolnAI with N8N workflows.
    
    This agent leverages AutoGen v0.4.7 and Claude 3.7 Sonnet to:
    1. Create and manage N8N workflows
    2. Trigger N8N workflows from SolnAI
    3. Process data from N8N workflows
    4. Generate workflow templates based on natural language descriptions
    5. Debug and optimize N8N workflows
    """
    
    def __init__(
        self, 
        api_key: Optional[str] = None,
        n8n_api_url: Optional[str] = None,
        n8n_api_key: Optional[str] = None,
        enable_caching: bool = True,
        cache_ttl: int = 3600,  # Default: 1 hour
        performance_tracking: bool = True  # Enable performance tracking by default
    ):
        """
        Initialize the N8N Integration Agent.
        
        Args:
            api_key: Anthropic API key (optional, will use environment variable if not provided)
            n8n_api_url: URL of the N8N API (optional, will use environment variable if not provided)
            n8n_api_key: API key for N8N (optional, will use environment variable if not provided)
        """
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("Anthropic API key is required. Set ANTHROPIC_API_KEY environment variable or pass it to the constructor.")
        
        # Set up N8N API connection
        self.n8n_api_url = n8n_api_url or os.environ.get("N8N_API_URL")
        self.n8n_api_key = n8n_api_key or os.environ.get("N8N_API_KEY")
        
        if not self.n8n_api_url:
            logger.warning("N8N API URL not provided. Some functionality may be limited.")
        
        if not self.n8n_api_key:
            logger.warning("N8N API key not provided. Some functionality may be limited.")
        
        # Add caching support
        self.enable_caching = enable_caching
        self.cache_ttl = cache_ttl
        self.cache = {}  # Simple in-memory cache
        
        # Add performance tracking and optimization
        self.performance_tracking = performance_tracking
        self.optimization_suggestions = set()  # Store unique optimization suggestions
        self.performance_metrics = {}  # Store performance metrics by workflow ID
        self.execution_times = {}  # Store execution times for performance analysis
        
        # Initialize the model client
        self.model_client = AnthropicChatCompletionClient(
            model="claude-3-7-sonnet",
            api_key=self.api_key,
            temperature=0.2,  # Lower temperature for more deterministic results
        )
        
        # Initialize the N8N assistant agent
        self.n8n_assistant = AssistantAgent(
            name="n8n_assistant",
            model_client=self.model_client,
            system_message="""You are an expert in N8N workflow automation and integration.
            Your task is to help users create, manage, and optimize N8N workflows that integrate with SolnAI.

            Follow these guidelines:
            1. Create well-structured N8N workflows based on user requirements
            2. Integrate SolnAI agents with N8N workflows
            3. Optimize workflows for efficiency and reliability
            4. Debug workflow issues
            5. Provide clear explanations of workflow functionality
            
            When working with N8N:
            - Follow N8N best practices for workflow design
            - Use appropriate nodes for different tasks
            - Ensure proper error handling
            - Consider security and authentication
            - Document workflows clearly
            """,
            model_context=BufferedChatCompletionContext(buffer_size=20),  # Keep a buffer of 20 messages
        )
        
        # Initialize best practices knowledge base
        self.best_practices = {
            "parallel_execution": [
                "Use Split In Batches nodes for parallel processing",
                "Minimize dependencies between parallel branches",
                "Use Merge nodes to consolidate results"
            ],
            "caching": [
                "Cache results of expensive operations",
                "Implement conditional execution to skip redundant operations",
                "Use function nodes for custom caching logic"
            ],
            "api_optimization": [
                "Batch API requests when possible",
                "Use pagination for large datasets",
                "Implement retry logic for unreliable APIs",
                "Cache API responses when appropriate"
            ],
            "data_transformation": [
                "Use Set node for efficient data mapping",
                "Process data in batches for large datasets",
                "Use Function nodes for complex transformations"
            ],
            "error_handling": [
                "Implement error handling on all critical nodes",
                "Use Error Trigger nodes for recovery workflows",
                "Log errors with appropriate context"
            ]
        }
        
        # Initialize the user proxy agent
        self.user_proxy = UserProxyAgent(
            name="user_proxy",
            human_input_mode="NEVER",
            code_execution_config={
                "work_dir": ".",
                "use_docker": False,
            },
        )
    
    async def analyze_workflow_performance(self, workflow_json: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze the performance characteristics of an N8N workflow.
        
        This method examines the workflow structure to identify:
        - Potential bottlenecks
        - Excessive API calls
        - Inefficient data processing
        - Missing error handling
        - Opportunities for parallelization
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            
        Returns:
            Dict with analysis results and optimization suggestions
        """
        logger.info(f"Analyzing workflow performance")
        
        # Extract basic workflow metrics
        node_count = len(workflow_json.get("nodes", []))
        connection_count = len(workflow_json.get("connections", {}).get("main", []))
        
        # Categorize nodes by type
        node_types = {}
        api_nodes = []
        transform_nodes = []
        trigger_nodes = []
        error_handling_nodes = []
        
        for node in workflow_json.get("nodes", []):
            node_type = node.get("type", "")
            node_types[node_type] = node_types.get(node_type, 0) + 1
            
            # Identify API nodes (HTTP Request, API, etc.)
            if "http" in node_type.lower() or "api" in node_type.lower():
                api_nodes.append(node)
            
            # Identify transformation nodes (Set, Function, etc.)
            if node_type in ["Set", "Function", "FunctionItem", "Move Binary Data"]:
                transform_nodes.append(node)
            
            # Identify trigger nodes
            if "trigger" in node_type.lower():
                trigger_nodes.append(node)
            
            # Identify error handling nodes
            if "error" in node_type.lower():
                error_handling_nodes.append(node)
        
        # Create a message with the workflow analysis request
        workflow_str = json.dumps(workflow_json, indent=2)
        analysis_message = TextMessage(
            content=f"""I need to analyze the performance of an N8N workflow.

Here is the workflow JSON:
```json
{workflow_str}
```

Please analyze this workflow for performance issues and optimization opportunities. Focus on:
1. Identifying potential bottlenecks
2. Finding excessive or redundant API calls
3. Detecting inefficient data processing
4. Locating missing error handling
5. Discovering opportunities for parallelization

Provide specific optimization suggestions with explanations.""",
            source="user"
        )
        
        # Get workflow analysis response
        response = await self.n8n_assistant.on_messages(
            [analysis_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract optimization suggestions
        suggestions = self._extract_optimization_suggestions(response.chat_message.content)
        
        # Add suggestions to the set of all suggestions
        self.optimization_suggestions.update(suggestions)
        
        # Create the analysis result
        result = {
            "workflow_metrics": {
                "node_count": node_count,
                "connection_count": connection_count,
                "node_types": node_types,
                "api_node_count": len(api_nodes),
                "transform_node_count": len(transform_nodes),
                "trigger_node_count": len(trigger_nodes),
                "error_handling_node_count": len(error_handling_nodes),
            },
            "performance_analysis": response.chat_message.content,
            "optimization_suggestions": list(suggestions),
        }
        
        return result
    
    async def optimize_for_parallel_execution(self, workflow_json: Dict[str, Any]) -> Dict[str, Any]:
        """
        Optimize an N8N workflow for parallel execution.
        
        This method analyzes the workflow to identify tasks that can be executed in parallel,
        and restructures the workflow to take advantage of N8N's parallel processing capabilities.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            
        Returns:
            Dict with the parallelized workflow
        """
        logger.info(f"Optimizing workflow for parallel execution")
        
        # Create a message with the parallelization request
        workflow_str = json.dumps(workflow_json, indent=2)
        parallelization_message = TextMessage(
            content=f"""I need to optimize an N8N workflow for parallel execution.

Here is the current workflow JSON:
```json
{workflow_str}
```

Please restructure this workflow to maximize parallel execution by:
1. Identifying independent operations that can run in parallel
2. Using Split In Batches nodes to process data in parallel
3. Adding Merge nodes to consolidate results
4. Ensuring proper error handling for parallel branches

Provide the optimized workflow JSON and explain the parallelization strategy.""",
            source="user"
        )
        
        # Get parallelization response
        response = await self.n8n_assistant.on_messages(
            [parallelization_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the parallelized workflow JSON from the response
        parallelized_workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "original_workflow": workflow_json,
            "explanation": response.chat_message.content,
        }
        
        if parallelized_workflow_json:
            result["parallelized_workflow"] = parallelized_workflow_json
        
        return result
    
    async def implement_workflow_caching(self, workflow_json: Dict[str, Any], cache_config: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Implement intelligent caching strategies in an N8N workflow.
        
        This method modifies the workflow to add caching for appropriate operations,
        reducing redundant API calls and improving performance.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            cache_config: Optional configuration for caching behavior
            
        Returns:
            Dict with the cached workflow
        """
        logger.info(f"Implementing caching in workflow")
        
        # Set default cache configuration if not provided
        if cache_config is None:
            cache_config = {
                "ttl": self.cache_ttl,
                "cache_api_calls": True,
                "cache_expensive_operations": True,
            }
        
        # Create a message with the caching implementation request
        workflow_str = json.dumps(workflow_json, indent=2)
        cache_config_str = json.dumps(cache_config, indent=2)
        
        caching_message = TextMessage(
            content=f"""I need to implement caching in an N8N workflow to improve performance.

Here is the current workflow JSON:
```json
{workflow_str}
```

Caching configuration:
```json
{cache_config_str}
```

Please modify this workflow to implement caching for:
1. API calls that may be repeated
2. Expensive data processing operations
3. Operations with stable inputs that produce stable outputs

Use Function nodes to implement the caching logic, with appropriate TTL values.
Provide the modified workflow JSON with caching implemented and explain your caching strategy.""",
            source="user"
        )
        
        # Get caching implementation response
        response = await self.n8n_assistant.on_messages(
            [caching_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the cached workflow JSON from the response
        cached_workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "original_workflow": workflow_json,
            "cache_config": cache_config,
            "explanation": response.chat_message.content,
        }
        
        if cached_workflow_json:
            result["cached_workflow"] = cached_workflow_json
        
        return result
    
    async def batch_process_workflow(self, workflow_json: Dict[str, Any], batch_size: int = 10) -> Dict[str, Any]:
        """
        Optimize a workflow for processing large datasets using batching.
        
        This method modifies the workflow to implement batch processing strategies,
        improving performance when dealing with large datasets.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            batch_size: Size of batches for processing
            
        Returns:
            Dict with the batched workflow
        """
        logger.info(f"Optimizing workflow for batch processing with batch size {batch_size}")
        
        # Create a message with the batch processing request
        workflow_str = json.dumps(workflow_json, indent=2)
        batching_message = TextMessage(
            content=f"""I need to optimize an N8N workflow for batch processing of large datasets.

Here is the current workflow JSON:
```json
{workflow_str}
```

Please modify this workflow to implement batch processing with a batch size of {batch_size} by:
1. Adding Split In Batches nodes at appropriate points
2. Configuring parallel processing for the batches
3. Adding Merge nodes to consolidate batch results
4. Ensuring proper error handling for batch processing

Provide the modified workflow JSON and explain your batch processing strategy.""",
            source="user"
        )
        
        # Get batch processing response
        response = await self.n8n_assistant.on_messages(
            [batching_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the batched workflow JSON from the response
        batched_workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "original_workflow": workflow_json,
            "batch_size": batch_size,
            "explanation": response.chat_message.content,
        }
        
        if batched_workflow_json:
            result["batched_workflow"] = batched_workflow_json
        
        return result
    
    async def optimize_data_transformation(self, workflow_json: Dict[str, Any]) -> Dict[str, Any]:
        """
        Optimize data transformation operations in an N8N workflow.
        
        This method identifies and improves inefficient data transformation patterns,
        reducing execution time and resource usage.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            
        Returns:
            Dict with the optimized workflow
        """
        logger.info(f"Optimizing data transformations in workflow")
        
        # Create a message with the data transformation optimization request
        workflow_str = json.dumps(workflow_json, indent=2)
        transformation_message = TextMessage(
            content=f"""I need to optimize data transformation operations in an N8N workflow.

Here is the current workflow JSON:
```json
{workflow_str}
```

Please analyze and optimize the data transformation operations by:
1. Identifying inefficient transformation patterns
2. Replacing multiple transformations with more efficient alternatives
3. Optimizing JSON and string manipulations
4. Reducing unnecessary data copying or manipulation
5. Using Set nodes instead of Function nodes where appropriate

Provide the optimized workflow JSON and explain your optimization strategy.""",
            source="user"
        )
        
        # Get data transformation optimization response
        response = await self.n8n_assistant.on_messages(
            [transformation_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the optimized workflow JSON from the response
        optimized_workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "original_workflow": workflow_json,
            "explanation": response.chat_message.content,
        }
        
        if optimized_workflow_json:
            result["optimized_workflow"] = optimized_workflow_json
        
        return result
    
    async def reduce_api_calls(self, workflow_json: Dict[str, Any]) -> Dict[str, Any]:
        """
        Optimize an N8N workflow by reducing redundant API calls.
        
        This method identifies patterns of redundant API calls and restructures the workflow
        to minimize the number of external calls while maintaining functionality.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            
        Returns:
            Dict with the optimized workflow
        """
        logger.info(f"Optimizing workflow to reduce API calls")
        
        # Create a message with the API call reduction request
        workflow_str = json.dumps(workflow_json, indent=2)
        api_optimization_message = TextMessage(
            content=f"""I need to optimize an N8N workflow by reducing redundant API calls.

Here is the current workflow JSON:
```json
{workflow_str}
```

Please analyze and optimize the workflow to reduce API calls by:
1. Identifying redundant or repeated API calls
2. Implementing caching for API responses
3. Batching multiple single-item API calls into bulk operations
4. Using pagination efficiently for large datasets
5. Implementing conditional execution to avoid unnecessary API calls

Provide the optimized workflow JSON and explain your API call reduction strategy.""",
            source="user"
        )
        
        # Get API call reduction response
        response = await self.n8n_assistant.on_messages(
            [api_optimization_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the optimized workflow JSON from the response
        optimized_workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "original_workflow": workflow_json,
            "explanation": response.chat_message.content,
        }
        
        if optimized_workflow_json:
            result["optimized_workflow"] = optimized_workflow_json
        
        return result
    
    async def create_workflow(self, workflow_description: str) -> Dict[str, Any]:
        """
        Create an N8N workflow based on a natural language description.
        
        Args:
            workflow_description: Natural language description of the workflow
            
        Returns:
            Dict with the created workflow
        """
        logger.info(f"Creating N8N workflow from description")
        
        # Check cache if caching is enabled
        if self.enable_caching:
            cache_key = self.generate_cache_key("create_workflow", workflow_description)
            cached_result = self.get_cached_result(cache_key)
            if cached_result is not None:
                logger.info(f"Using cached workflow for description")
                return cached_result
        
        # Create a message with the workflow creation request
        workflow_message = TextMessage(
            content=f"""I need to create an N8N workflow based on the following description:

{workflow_description}

Please create a complete N8N workflow JSON that implements this functionality.
The workflow should follow N8N best practices and include proper error handling.
Please provide the workflow JSON and a brief explanation of how it works.""",
            source="user"
        )
        
        # Get workflow creation response
        response = await self.n8n_assistant.on_messages(
            [workflow_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the workflow JSON from the response
        workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "description": workflow_description,
            "explanation": response.chat_message.content,
        }
        
        if workflow_json:
            result["workflow_json"] = workflow_json
            
            # Deploy the workflow if N8N API is configured
            if self.n8n_api_url and self.n8n_api_key:
                deployment_result = self._deploy_workflow(workflow_json)
                result["deployment"] = deployment_result
                
            # If the workflow was successfully created, analyze its performance
            if self.performance_tracking:
                try:
                    performance_analysis = await self.analyze_workflow_performance(workflow_json)
                    result["performance_analysis"] = performance_analysis
                except Exception as e:
                    logger.error(f"Error analyzing workflow performance: {e}")
                
            # Cache the result if caching is enabled
            if self.enable_caching:
                self.set_cached_result(cache_key, result)
        
        return result
    
    async def optimize_workflow(self, workflow_json: Dict[str, Any], optimization_goal: str = "performance") -> Dict[str, Any]:
        """
        Optimize an existing N8N workflow.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            optimization_goal: Goal of the optimization (performance, reliability, readability)
            
        Returns:
            Dict with the optimized workflow
        """
        logger.info(f"Optimizing N8N workflow for {optimization_goal}")
        
        # Check cache if caching is enabled
        if self.enable_caching:
            # Create a stable representation of the workflow JSON for caching
            workflow_str_for_cache = json.dumps(workflow_json, sort_keys=True)
            cache_key = self.generate_cache_key("optimize_workflow", workflow_str_for_cache, optimization_goal)
            cached_result = self.get_cached_result(cache_key)
            if cached_result is not None:
                logger.info(f"Using cached optimization result for {optimization_goal}")
                return cached_result
        
        # First, analyze the workflow performance if performance tracking is enabled
        if self.performance_tracking:
            try:
                performance_analysis = await self.analyze_workflow_performance(workflow_json)
                # Use the performance analysis to guide optimization
                optimization_suggestions = performance_analysis.get("optimization_suggestions", [])
            except Exception as e:
                logger.error(f"Error analyzing workflow performance: {e}")
                optimization_suggestions = []
        else:
            optimization_suggestions = []
        
        # Choose the appropriate optimization method based on the goal
        if optimization_goal == "performance":
            # For performance optimization, try parallel execution first
            try:
                parallel_result = await self.optimize_for_parallel_execution(workflow_json)
                if "parallelized_workflow" in parallel_result:
                    workflow_json = parallel_result["parallelized_workflow"]
            except Exception as e:
                logger.error(f"Error optimizing for parallel execution: {e}")
            
            # Then try to reduce API calls
            try:
                api_result = await self.reduce_api_calls(workflow_json)
                if "optimized_workflow" in api_result:
                    workflow_json = api_result["optimized_workflow"]
            except Exception as e:
                logger.error(f"Error reducing API calls: {e}")
            
            # Finally, optimize data transformations
            try:
                transform_result = await self.optimize_data_transformation(workflow_json)
                if "optimized_workflow" in transform_result:
                    workflow_json = transform_result["optimized_workflow"]
            except Exception as e:
                logger.error(f"Error optimizing data transformations: {e}")
        
        # Create a message with the workflow optimization request
        workflow_str = json.dumps(workflow_json, indent=2)
        suggestions_str = "\n".join([f"- {s}" for s in optimization_suggestions[:5]]) if optimization_suggestions else ""
        
        # Prepare the suggestions part separately to avoid nested f-strings
        suggestions_part = ""
        if suggestions_str:
            suggestions_part = f"Based on performance analysis, consider these suggestions:\n{suggestions_str}\n"
            
        optimization_message = TextMessage(
            content=f"""I have an N8N workflow that I want to optimize for {optimization_goal}.

Here is the current workflow JSON:
```json
{workflow_str}
```

{suggestions_part}Please optimize this workflow for {optimization_goal}. Consider the following:
1. Removing unnecessary nodes
2. Improving error handling
3. Optimizing data flow
4. Reducing API calls
5. Improving readability and maintainability

Please provide the optimized workflow JSON and explain the changes you made.""",
            source="user"
        )
        
        # Get workflow optimization response
        response = await self.n8n_assistant.on_messages(
            [optimization_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the optimized workflow JSON from the response
        optimized_workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "original_workflow": workflow_json,
            "optimization_goal": optimization_goal,
            "explanation": response.chat_message.content,
        }
        
        if optimized_workflow_json:
            result["optimized_workflow"] = optimized_workflow_json
            
            # If caching is enabled, cache the result
            if self.enable_caching:
                self.set_cached_result(cache_key, result)
        
        return result
    
    async def debug_workflow(self, workflow_json: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """
        Debug an N8N workflow that is producing errors.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            error_message: Error message from the workflow execution
            
        Returns:
            Dict with the debugged workflow
        """
        logger.info(f"Debugging N8N workflow")
        
        # Check cache if caching is enabled
        if self.enable_caching:
            # Create a stable representation of the workflow JSON and error for caching
            workflow_str_for_cache = json.dumps(workflow_json, sort_keys=True)
            cache_key = self.generate_cache_key("debug_workflow", workflow_str_for_cache, error_message)
            cached_result = self.get_cached_result(cache_key)
            if cached_result is not None:
                logger.info(f"Using cached debug result")
                return cached_result
        
        # Create a message with the workflow debugging request
        workflow_str = json.dumps(workflow_json, indent=2)
        debug_message = TextMessage(
            content=f"""I have an N8N workflow that is producing the following error:

Error message:
```
{error_message}
```

Here is the current workflow JSON:
```json
{workflow_str}
```

Please debug this workflow and fix the issues causing the error.
Provide the fixed workflow JSON and explain what was causing the error and how you fixed it.""",
            source="user"
        )
        
        # Get workflow debugging response
        response = await self.n8n_assistant.on_messages(
            [debug_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the fixed workflow JSON from the response
        fixed_workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "original_workflow": workflow_json,
            "error_message": error_message,
            "explanation": response.chat_message.content,
        }
        
        if fixed_workflow_json:
            result["fixed_workflow"] = fixed_workflow_json
            
            # If the workflow was successfully fixed, analyze its performance
            if self.performance_tracking and fixed_workflow_json:
                try:
                    performance_analysis = await self.analyze_workflow_performance(fixed_workflow_json)
                    result["performance_analysis"] = performance_analysis
                except Exception as e:
                    logger.error(f"Error analyzing fixed workflow performance: {e}")
            
            # If caching is enabled, cache the result
            if self.enable_caching:
                self.set_cached_result(cache_key, result)
        
        return result
    
    async def integrate_solnai_agent(self, workflow_json: Dict[str, Any], agent_type: str, integration_point: str) -> Dict[str, Any]:
        """
        Integrate a SolnAI agent into an N8N workflow.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            agent_type: Type of SolnAI agent to integrate (research, code_executor, data_analysis, etc.)
            integration_point: Description of where in the workflow to integrate the agent
            
        Returns:
            Dict with the integrated workflow
        """
        logger.info(f"Integrating {agent_type} agent into N8N workflow")
        
        # Check cache if caching is enabled
        if self.enable_caching:
            # Create a stable representation of the workflow JSON and integration parameters for caching
            workflow_str_for_cache = json.dumps(workflow_json, sort_keys=True)
            cache_key = self.generate_cache_key("integrate_solnai_agent", workflow_str_for_cache, agent_type, integration_point)
            cached_result = self.get_cached_result(cache_key)
            if cached_result is not None:
                logger.info(f"Using cached integration result for {agent_type} agent")
                return cached_result
        
        # Create a message with the agent integration request
        workflow_str = json.dumps(workflow_json, indent=2)
        integration_message = TextMessage(
            content=f"""I have an N8N workflow that I want to integrate with a SolnAI {agent_type} agent.

Here is the current workflow JSON:
```json
{workflow_str}
```

I want to integrate the agent at this point in the workflow: {integration_point}

Please modify the workflow to integrate the SolnAI {agent_type} agent.
The integration should:
1. Send data from the workflow to the agent
2. Wait for the agent to process the data
3. Receive the results from the agent
4. Continue the workflow with the agent's results

Please provide the modified workflow JSON and explain how the integration works.""",
            source="user"
        )
        
        # Get agent integration response
        response = await self.n8n_assistant.on_messages(
            [integration_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the integrated workflow JSON from the response
        integrated_workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "original_workflow": workflow_json,
            "agent_type": agent_type,
            "integration_point": integration_point,
            "explanation": response.chat_message.content,
        }
        
        if integrated_workflow_json:
            result["integrated_workflow"] = integrated_workflow_json
            
            # If the workflow was successfully integrated, analyze its performance
            if self.performance_tracking and integrated_workflow_json:
                try:
                    performance_analysis = await self.analyze_workflow_performance(integrated_workflow_json)
                    result["performance_analysis"] = performance_analysis
                except Exception as e:
                    logger.error(f"Error analyzing integrated workflow performance: {e}")
            
            # If caching is enabled, cache the result
            if self.enable_caching:
                self.set_cached_result(cache_key, result)
        
        return result
    
    async def generate_workflow_template(self, template_type: str, customizations: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Generate a template N8N workflow for a specific use case.
        
        Args:
            template_type: Type of template to generate (data_processing, api_integration, email_automation, etc.)
            customizations: Dict of customizations to apply to the template
            
        Returns:
            Dict with the generated template workflow
        """
        logger.info(f"Generating {template_type} workflow template")
        
        # Check cache if caching is enabled
        if self.enable_caching:
            # Create a stable representation of the template parameters for caching
            customizations_str_for_cache = json.dumps(customizations, sort_keys=True) if customizations else ""
            cache_key = self.generate_cache_key("generate_workflow_template", template_type, customizations_str_for_cache)
            cached_result = self.get_cached_result(cache_key)
            if cached_result is not None:
                logger.info(f"Using cached template for {template_type}")
                return cached_result
        
        # Create customizations string
        customizations_str = ""
        if customizations:
            customizations_str = "with the following customizations:\n"
            for key, value in customizations.items():
                customizations_str += f"- {key}: {value}\n"
        
        # Create a message with the template generation request
        template_message = TextMessage(
            content=f"""I need to generate an N8N workflow template for {template_type} {customizations_str}

Please create a complete N8N workflow JSON that serves as a template for this use case.
The template should:
1. Follow N8N best practices
2. Include proper error handling
3. Be well-documented with comments
4. Be easily customizable

Please provide the template workflow JSON and a brief explanation of how it works and how it can be customized.""",
            source="user"
        )
        
        # Get template generation response
        response = await self.n8n_assistant.on_messages(
            [template_message],
            cancellation_token=CancellationToken(),
        )
        
        # Extract the template workflow JSON from the response
        template_workflow_json = self._extract_json(response.chat_message.content)
        
        result = {
            "template_type": template_type,
            "customizations": customizations,
            "explanation": response.chat_message.content,
        }
        
        if template_workflow_json:
            result["template_workflow"] = template_workflow_json
            
            # If the template was successfully generated, analyze its performance
            if self.performance_tracking and template_workflow_json:
                try:
                    performance_analysis = await self.analyze_workflow_performance(template_workflow_json)
                    result["performance_analysis"] = performance_analysis
                except Exception as e:
                    logger.error(f"Error analyzing template workflow performance: {e}")
            
            # If caching is enabled, cache the result
            if self.enable_caching:
                self.set_cached_result(cache_key, result)
        
        return result
    
    def clear_cache(self, pattern: Optional[str] = None) -> Dict[str, Any]:
        """
        Clear the cache, optionally filtering by a pattern.
        
        Args:
            pattern: Optional pattern to match cache keys against
            
        Returns:
            Dict with information about the cleared cache
        """
        if not self.enable_caching:
            return {"status": "error", "message": "Caching is not enabled"}
        
        cache_size_before = len(self.cache)
        keys_to_remove = []
        
        if pattern:
            # Remove only keys matching the pattern
            for key in self.cache.keys():
                if pattern in key:
                    keys_to_remove.append(key)
        else:
            # Remove all keys
            keys_to_remove = list(self.cache.keys())
        
        # Remove the keys
        for key in keys_to_remove:
            del self.cache[key]
        
        return {
            "status": "success",
            "cache_size_before": cache_size_before,
            "cache_size_after": len(self.cache),
            "keys_removed": len(keys_to_remove),
            "pattern": pattern
        }
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the current cache.
        
        Returns:
            Dict with cache statistics
        """
        if not self.enable_caching:
            return {"status": "error", "message": "Caching is not enabled"}
        
        # Group cache items by type
        cache_types = {}
        expired_count = 0
        current_time = time.time()
        
        for key, item in self.cache.items():
            # Check if the item has expired
            expiration_time = item["timestamp"] + item["ttl"]
            if current_time > expiration_time:
                expired_count += 1
                continue
            
            # Extract the cache type from the key (first part before the hash)
            cache_type = key.split("_")[0] if "_" in key else "unknown"
            if cache_type not in cache_types:
                cache_types[cache_type] = 0
            cache_types[cache_type] += 1
        
        return {
            "status": "success",
            "total_items": len(self.cache),
            "expired_items": expired_count,
            "active_items": len(self.cache) - expired_count,
            "cache_types": cache_types,
            "cache_size_bytes": sys.getsizeof(self.cache)
        }
    
    def export_optimization_suggestions(self) -> Dict[str, Any]:
        """
        Export all collected optimization suggestions.
        
        Returns:
            Dict with optimization suggestions
        """
        if not self.performance_tracking:
            return {"status": "error", "message": "Performance tracking is not enabled"}
        
        # Group suggestions by category
        categorized_suggestions = {
            "performance": [],
            "reliability": [],
            "security": [],
            "maintainability": [],
            "other": []
        }
        
        # Keywords for categorization
        category_keywords = {
            "performance": ["performance", "speed", "efficient", "fast", "slow", "optimize", "bottleneck", "cache", "parallel"],
            "reliability": ["reliability", "error", "exception", "fail", "crash", "robust", "handle", "recover"],
            "security": ["security", "secure", "auth", "encrypt", "protect", "vulnerability", "risk"],
            "maintainability": ["maintainability", "readable", "clean", "document", "comment", "structure"]
        }
        
        # Categorize each suggestion
        for suggestion in self.optimization_suggestions:
            suggestion_lower = suggestion.lower()
            assigned = False
            
            for category, keywords in category_keywords.items():
                if any(keyword in suggestion_lower for keyword in keywords):
                    categorized_suggestions[category].append(suggestion)
                    assigned = True
                    break
            
            if not assigned:
                categorized_suggestions["other"].append(suggestion)
        
        return {
            "status": "success",
            "total_suggestions": len(self.optimization_suggestions),
            "categorized_suggestions": categorized_suggestions,
            "all_suggestions": list(self.optimization_suggestions)
        }
    
    def export_performance_analysis(self, workflow_json: Dict[str, Any]) -> Dict[str, Any]:
        """
        Export a comprehensive performance analysis for a workflow.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            
        Returns:
            Dict with performance analysis
        """
        if not self.performance_tracking:
            return {"status": "error", "message": "Performance tracking is not enabled"}
        
        try:
            # Extract basic workflow metrics
            node_count = len(workflow_json.get("nodes", []))
            connection_count = len(workflow_json.get("connections", {}).get("main", []))
            
            # Categorize nodes by type
            node_types = {}
            api_nodes = []
            transform_nodes = []
            trigger_nodes = []
            error_handling_nodes = []
            
            for node in workflow_json.get("nodes", []):
                node_type = node.get("type", "")
                node_types[node_type] = node_types.get(node_type, 0) + 1
                
                # Identify API nodes (HTTP Request, API, etc.)
                if "http" in node_type.lower() or "api" in node_type.lower():
                    api_nodes.append(node)
                
                # Identify transformation nodes (Set, Function, etc.)
                if node_type in ["Set", "Function", "FunctionItem", "Move Binary Data"]:
                    transform_nodes.append(node)
                
                # Identify trigger nodes
                if "trigger" in node_type.lower():
                    trigger_nodes.append(node)
                
                # Identify error handling nodes
                if "error" in node_type.lower():
                    error_handling_nodes.append(node)
            
            # Calculate complexity metrics
            complexity_score = node_count * 0.5 + connection_count * 0.3 + len(api_nodes) * 1.0
            error_handling_ratio = len(error_handling_nodes) / node_count if node_count > 0 else 0
            
            # Determine performance risk areas
            performance_risks = []
            
            if len(api_nodes) > 5:
                performance_risks.append("High number of API calls may cause performance issues")
            
            if error_handling_ratio < 0.1:
                performance_risks.append("Low error handling coverage may cause reliability issues")
            
            if node_count > 20 and len(transform_nodes) / node_count > 0.5:
                performance_risks.append("High proportion of transformation nodes may indicate inefficient data processing")
            
            # Create the analysis result
            result = {
                "status": "success",
                "workflow_metrics": {
                    "node_count": node_count,
                    "connection_count": connection_count,
                    "node_types": node_types,
                    "api_node_count": len(api_nodes),
                    "transform_node_count": len(transform_nodes),
                    "trigger_node_count": len(trigger_nodes),
                    "error_handling_node_count": len(error_handling_nodes),
                },
                "complexity_analysis": {
                    "complexity_score": complexity_score,
                    "error_handling_ratio": error_handling_ratio,
                    "complexity_level": "High" if complexity_score > 20 else "Medium" if complexity_score > 10 else "Low"
                },
                "performance_risks": performance_risks,
                "optimization_suggestions": list(self.optimization_suggestions)
            }
            
            return result
        except Exception as e:
            logger.error(f"Error exporting performance analysis: {e}")
            return {"status": "error", "message": str(e)}
    
    async def benchmark_workflow(self, workflow_json: Dict[str, Any], iterations: int = 5, input_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Benchmark a workflow's performance by executing it multiple times and measuring performance metrics.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            iterations: Number of times to execute the workflow
            input_data: Optional input data for the workflow
            
        Returns:
            Dict with benchmark results
        """
        if not self.performance_tracking:
            return {"status": "error", "message": "Performance tracking is not enabled"}
        
        if not self.n8n_api_url or not self.n8n_api_key:
            return {"status": "error", "message": "N8N API connection not configured"}
        
        try:
            # First, import the workflow to N8N
            import_response = self._import_workflow(workflow_json)
            if not import_response or "id" not in import_response:
                return {"status": "error", "message": "Failed to import workflow for benchmarking"}
            
            workflow_id = import_response["id"]
            execution_times = []
            memory_usage = []
            success_count = 0
            error_count = 0
            errors = []
            
            logger.info(f"Benchmarking workflow {workflow_id} with {iterations} iterations")
            
            # Execute the workflow multiple times
            for i in range(iterations):
                start_time = time.time()
                
                try:
                    # Execute the workflow
                    execution_result = self._execute_workflow(workflow_id, input_data)
                    
                    # Record execution time
                    end_time = time.time()
                    execution_time = end_time - start_time
                    execution_times.append(execution_time)
                    
                    # Check if execution was successful
                    if execution_result and "finished" in execution_result and execution_result["finished"]:
                        success_count += 1
                        
                        # Try to extract memory usage if available
                        if "data" in execution_result and "memoryUsage" in execution_result["data"]:
                            memory_usage.append(execution_result["data"]["memoryUsage"])
                    else:
                        error_count += 1
                        errors.append(f"Execution {i+1} failed: {execution_result}")
                except Exception as e:
                    error_count += 1
                    errors.append(f"Execution {i+1} error: {str(e)}")
                
                # Add a small delay between executions
                await asyncio.sleep(0.5)
            
            # Calculate statistics
            avg_execution_time = statistics.mean(execution_times) if execution_times else 0
            median_execution_time = statistics.median(execution_times) if execution_times else 0
            min_execution_time = min(execution_times) if execution_times else 0
            max_execution_time = max(execution_times) if execution_times else 0
            std_dev_execution_time = statistics.stdev(execution_times) if len(execution_times) > 1 else 0
            
            avg_memory_usage = statistics.mean(memory_usage) if memory_usage else 0
            
            # Clean up - delete the imported workflow
            self._delete_workflow(workflow_id)
            
            # Prepare benchmark results
            benchmark_results = {
                "status": "success",
                "workflow_id": workflow_id,
                "iterations": iterations,
                "successful_executions": success_count,
                "failed_executions": error_count,
                "execution_times": {
                    "average": avg_execution_time,
                    "median": median_execution_time,
                    "min": min_execution_time,
                    "max": max_execution_time,
                    "standard_deviation": std_dev_execution_time,
                    "raw_times": execution_times
                },
                "memory_usage": {
                    "average": avg_memory_usage,
                    "raw_usage": memory_usage
                },
                "errors": errors,
                "timestamp": datetime.datetime.now().isoformat()
            }
            
            # Store benchmark results in performance metrics
            self.performance_metrics[workflow_id] = benchmark_results
            
            return benchmark_results
        except Exception as e:
            logger.error(f"Error benchmarking workflow: {e}")
            return {"status": "error", "message": str(e)}
    
    def _import_workflow(self, workflow_json: Dict[str, Any]) -> Dict[str, Any]:
        """
        Import a workflow into N8N.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            
        Returns:
            Dict with the imported workflow details
        """
        if not self.n8n_api_url or not self.n8n_api_key:
            logger.error("N8N API connection not configured")
            return {}
        
        try:
            headers = {
                "X-N8N-API-KEY": self.n8n_api_key,
                "Content-Type": "application/json"
            }
            
            response = requests.post(
                f"{self.n8n_api_url}/workflows",
                headers=headers,
                json=workflow_json
            )
            
            if response.status_code in (200, 201):
                return response.json()
            else:
                logger.error(f"Failed to import workflow: {response.status_code} - {response.text}")
                return {}
        except Exception as e:
            logger.error(f"Error importing workflow: {e}")
            return {}
    
    def _execute_workflow(self, workflow_id: str, input_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Execute a workflow in N8N.
        
        Args:
            workflow_id: ID of the workflow to execute
            input_data: Optional input data for the workflow
            
        Returns:
            Dict with the execution result
        """
        if not self.n8n_api_url or not self.n8n_api_key:
            logger.error("N8N API connection not configured")
            return {}
        
        try:
            headers = {
                "X-N8N-API-KEY": self.n8n_api_key,
                "Content-Type": "application/json"
            }
            
            payload = {}
            if input_data:
                payload["data"] = input_data
            
            response = requests.post(
                f"{self.n8n_api_url}/workflows/{workflow_id}/execute",
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                logger.error(f"Failed to execute workflow: {response.status_code} - {response.text}")
                return {}
        except Exception as e:
            logger.error(f"Error executing workflow: {e}")
            return {}
    
    def _delete_workflow(self, workflow_id: str) -> bool:
        """
        Delete a workflow from N8N.
        
        Args:
            workflow_id: ID of the workflow to delete
            
        Returns:
            Boolean indicating success or failure
        """
        if not self.n8n_api_url or not self.n8n_api_key:
            logger.error("N8N API connection not configured")
            return False
        
        try:
            headers = {
                "X-N8N-API-KEY": self.n8n_api_key
            }
            
            response = requests.delete(
                f"{self.n8n_api_url}/workflows/{workflow_id}",
                headers=headers
            )
            
            return response.status_code in (200, 204)
        except Exception as e:
            logger.error(f"Error deleting workflow: {e}")
            return False
    
    async def run_interactive_session(self) -> None:
        """
        Run an interactive N8N integration session.
        """
        logger.info("Starting interactive N8N integration session")
        
        # Create a console for interaction
        await Console(
            self.n8n_assistant.on_messages_stream(
                [TextMessage(content="I'm ready to help with N8N workflow integration. What would you like to do?", source="user")],
                cancellation_token=CancellationToken(),
            ),
            output_stats=True,
        )
    
    def _extract_optimization_suggestions(self, text: str) -> set[str]:
        """
        Extract optimization suggestions from the text response.
        
        Args:
            text: Text containing optimization suggestions
            
        Returns:
            Set of optimization suggestions
        """
        import re
        
        suggestions = set()
        
        # Look for numbered lists (1. Something)
        numbered_suggestions = re.findall(r'\d+\.\s*([^\n]+)', text)
        for suggestion in numbered_suggestions:
            suggestion = suggestion.strip()
            if suggestion and len(suggestion) > 10:  # Avoid very short phrases
                suggestions.add(suggestion)
        
        # Look for bullet points
        bullet_suggestions = re.findall(r'[•\-*]\s*([^\n]+)', text)
        for suggestion in bullet_suggestions:
            suggestion = suggestion.strip()
            if suggestion and len(suggestion) > 10:  # Avoid very short phrases
                suggestions.add(suggestion)
        
        # Look for suggestions in paragraphs containing keywords
        keywords = ["optimize", "improve", "efficiency", "performance", "bottleneck", "cache", "parallel"]
        paragraphs = text.split('\n\n')
        for paragraph in paragraphs:
            if any(keyword in paragraph.lower() for keyword in keywords):
                # Clean up the paragraph
                clean_paragraph = re.sub(r'\s+', ' ', paragraph).strip()
                if clean_paragraph and len(clean_paragraph) > 20:  # Longer to ensure it's a real suggestion
                    suggestions.add(clean_paragraph)
        
        return suggestions
    
    def get_cached_result(self, key: str) -> Optional[Any]:
        """
        Get a cached result if available and not expired.
        
        Args:
            key: Cache key
            
        Returns:
            Cached value or None if not found or expired
        """
        if not self.enable_caching:
            return None
        
        if key not in self.cache:
            return None
        
        cached_item = self.cache[key]
        expiration_time = cached_item["timestamp"] + cached_item["ttl"]
        
        # Check if the cached item has expired
        if time.time() > expiration_time:
            # Remove the expired item
            del self.cache[key]
            return None
        
        return cached_item["value"]
    
    def set_cached_result(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        """
        Cache a result for future use.
        
        Args:
            key: Cache key
            value: Value to cache
            ttl: Time-to-live in seconds (optional, defaults to self.cache_ttl)
        """
        if not self.enable_caching:
            return
        
        ttl = ttl if ttl is not None else self.cache_ttl
        
        self.cache[key] = {
            "value": value,
            "timestamp": time.time(),
            "ttl": ttl
        }
    
    def generate_cache_key(self, *args: Any, **kwargs: Any) -> str:
        """
        Generate a deterministic cache key from the inputs.
        
        Args:
            *args: Positional arguments to include in the key
            **kwargs: Keyword arguments to include in the key
            
        Returns:
            A string cache key
        """
        # Convert args and kwargs to a stable string representation
        key_parts = [str(arg) for arg in args]
        key_parts.extend(f"{k}={v}" for k, v in sorted(kwargs.items()))
        
        # Join all parts and create a hash
        key_string = "|".join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    async def run_with_cache(self, func: Callable, *args: Any, ttl: Optional[int] = None, **kwargs: Any) -> Any:
        """
        Run a function with caching support.
        
        Args:
            func: Function to run
            *args: Positional arguments for the function
            ttl: Time-to-live for the cached result (optional)
            **kwargs: Keyword arguments for the function
            
        Returns:
            Function result (from cache if available)
        """
        if not self.enable_caching:
            return await func(*args, **kwargs)
        
        # Generate a cache key for this function call
        cache_key = self.generate_cache_key(func.__name__, *args, **kwargs)
        
        # Check if we have a cached result
        cached_result = self.get_cached_result(cache_key)
        if cached_result is not None:
            logger.info(f"Cache hit for {func.__name__}")
            return cached_result
        
        # No cached result, run the function
        logger.info(f"Cache miss for {func.__name__}")
        result = await func(*args, **kwargs)
        
        # Cache the result
        self.set_cached_result(cache_key, result, ttl)
        
        return result
    
    def _extract_json(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Extract JSON from text.
        
        Args:
            text: Text containing JSON
            
        Returns:
            Extracted JSON as dict or None if not found
        """
        import re
        
        # Find JSON blocks in the text
        json_blocks = re.findall(r'```(?:json)?\s*([\s\S]*?)```', text)
        
        if not json_blocks:
            # Try to find JSON without code blocks
            json_blocks = re.findall(r'(\{\s*"[^"]+"\s*:[\s\S]*\})', text)
        
        for block in json_blocks:
            try:
                # Try to parse the JSON
                json_obj = json.loads(block.strip())
                return json_obj
            except json.JSONDecodeError:
                continue
        
        return None
    
    def _deploy_workflow(self, workflow_json: Dict[str, Any]) -> Dict[str, Any]:
        """
        Deploy a workflow to N8N.
        
        Args:
            workflow_json: JSON representation of the N8N workflow
            
        Returns:
            Dict with deployment results
        """
        if not self.n8n_api_url or not self.n8n_api_key:
            return {
                "success": False,
                "error": "N8N API URL or API key not configured"
            }
        
        try:
            # Set up the request headers
            headers = {
                "Content-Type": "application/json",
                "X-N8N-API-KEY": self.n8n_api_key
            }
            
            # Create the workflow
            create_url = f"{self.n8n_api_url}/workflows"
            response = requests.post(create_url, json=workflow_json, headers=headers)
            
            if response.status_code == 200:
                workflow_data = response.json()
                return {
                    "success": True,
                    "workflow_id": workflow_data.get("id"),
                    "workflow_name": workflow_data.get("name"),
                    "workflow_url": f"{self.n8n_api_url}/workflow/{workflow_data.get('id')}"
                }
            else:
                return {
                    "success": False,
                    "status_code": response.status_code,
                    "error": response.text
                }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }


# Example usage
if __name__ == "__main__":
    # Initialize the agent
    agent = N8NIntegrationAgent()
    
    # Example: Run an interactive session
    asyncio.run(agent.run_interactive_session())
</file>

<file path="README.md">
# N8N Integration Agent for SolnAI

## Overview

The N8N Integration Agent is an intelligent assistant built on AutoGen v0.4.7 that specializes in integrating SolnAI with N8N workflows. It leverages Claude 3.7 Sonnet to provide high-quality workflow automation capabilities within the SolnAI ecosystem.

## Features

- **Workflow Creation**: Creates N8N workflows based on natural language descriptions
- **Workflow Optimization**: Optimizes existing workflows for performance, reliability, and readability
- **Workflow Debugging**: Identifies and fixes issues in workflows that are producing errors
- **SolnAI Agent Integration**: Integrates SolnAI agents into N8N workflows
- **Template Generation**: Generates template workflows for common use cases
- **Interactive Sessions**: Supports interactive workflow development sessions

## Architecture

The N8N Integration Agent is built using the following components:

- **AutoGen v0.4.7**: Provides the multi-agent framework
- **Claude 3.7 Sonnet**: Powers the workflow understanding and generation
- **AssistantAgent**: Handles the core workflow development functionality
- **UserProxyAgent**: Manages user interactions and code execution
- **N8N API Integration**: Connects with the N8N API for workflow deployment

## Installation

### Prerequisites

- Python 3.8 or higher
- AutoGen v0.4.7
- Anthropic API key
- N8N instance with API access (optional, for deployment)

### Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/your-org/solnai-app.git
   cd solnai-app/SolnAI-agents/n8n-integration-agent
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Set up your environment variables:
   ```bash
   export ANTHROPIC_API_KEY="your_anthropic_api_key"
   export N8N_API_URL="https://your-n8n-instance/api/v1"
   export N8N_API_KEY="your_n8n_api_key"
   ```

## Usage

### Creating a Workflow

```python
import asyncio
from n8n_integration_agent import N8NIntegrationAgent

async def main():
    # Initialize the agent
    agent = N8NIntegrationAgent()
    
    # Create a workflow based on a description
    workflow_description = """
    Create a workflow that monitors a Gmail inbox for new emails with attachments,
    extracts the attachments, processes them using the SolnAI Research Agent,
    and sends a summary of the findings back to the sender.
    """
    
    result = await agent.create_workflow(workflow_description)
    
    # Print the workflow JSON
    if "workflow_json" in result:
        print(json.dumps(result["workflow_json"], indent=2))

# Run the example
asyncio.run(main())
```

### Optimizing a Workflow

```python
import asyncio
import json
from n8n_integration_agent import N8NIntegrationAgent

async def main():
    # Initialize the agent
    agent = N8NIntegrationAgent()
    
    # Load an existing workflow
    with open("path/to/workflow.json", "r") as f:
        workflow_json = json.load(f)
    
    # Optimize the workflow for performance
    result = await agent.optimize_workflow(
        workflow_json=workflow_json,
        optimization_goal="performance"
    )
    
    # Print the optimized workflow JSON
    if "optimized_workflow" in result:
        print(json.dumps(result["optimized_workflow"], indent=2))

# Run the example
asyncio.run(main())
```

### Debugging a Workflow

```python
import asyncio
import json
from n8n_integration_agent import N8NIntegrationAgent

async def main():
    # Initialize the agent
    agent = N8NIntegrationAgent()
    
    # Load a workflow with errors
    with open("path/to/workflow.json", "r") as f:
        workflow_json = json.load(f)
    
    # Debug the workflow
    error_message = "Error: Cannot read property 'data' of undefined"
    result = await agent.debug_workflow(
        workflow_json=workflow_json,
        error_message=error_message
    )
    
    # Print the fixed workflow JSON
    if "fixed_workflow" in result:
        print(json.dumps(result["fixed_workflow"], indent=2))

# Run the example
asyncio.run(main())
```

### Integrating a SolnAI Agent

```python
import asyncio
import json
from n8n_integration_agent import N8NIntegrationAgent

async def main():
    # Initialize the agent
    agent = N8NIntegrationAgent()
    
    # Load an existing workflow
    with open("path/to/workflow.json", "r") as f:
        workflow_json = json.load(f)
    
    # Integrate a Research Agent
    result = await agent.integrate_solnai_agent(
        workflow_json=workflow_json,
        agent_type="research",
        integration_point="After the HTTP Request node to analyze the API response"
    )
    
    # Print the integrated workflow JSON
    if "integrated_workflow" in result:
        print(json.dumps(result["integrated_workflow"], indent=2))

# Run the example
asyncio.run(main())
```

### Generating a Template Workflow

```python
import asyncio
from n8n_integration_agent import N8NIntegrationAgent

async def main():
    # Initialize the agent
    agent = N8NIntegrationAgent()
    
    # Generate a template workflow for data processing
    customizations = {
        "data_source": "CSV file",
        "processing_steps": ["Filter", "Transform", "Aggregate"],
        "output_destination": "Google Sheets"
    }
    
    result = await agent.generate_workflow_template(
        template_type="data_processing",
        customizations=customizations
    )
    
    # Print the template workflow JSON
    if "template_workflow" in result:
        print(json.dumps(result["template_workflow"], indent=2))

# Run the example
asyncio.run(main())
```

### Interactive Session

```python
import asyncio
from n8n_integration_agent import N8NIntegrationAgent

async def main():
    # Initialize the agent
    agent = N8NIntegrationAgent()
    
    # Start an interactive workflow development session
    await agent.run_interactive_session()

# Run the example
asyncio.run(main())
```

## API Reference

### `N8NIntegrationAgent`

The main class that provides N8N integration capabilities.

#### Methods

- `create_workflow(workflow_description: str) -> Dict[str, Any]`: Creates an N8N workflow based on a natural language description
- `optimize_workflow(workflow_json: Dict[str, Any], optimization_goal: str = "performance") -> Dict[str, Any]`: Optimizes an existing N8N workflow
- `debug_workflow(workflow_json: Dict[str, Any], error_message: str) -> Dict[str, Any]`: Debugs an N8N workflow that is producing errors
- `integrate_solnai_agent(workflow_json: Dict[str, Any], agent_type: str, integration_point: str) -> Dict[str, Any]`: Integrates a SolnAI agent into an N8N workflow
- `generate_workflow_template(template_type: str, customizations: Dict[str, Any] = None) -> Dict[str, Any]`: Generates a template N8N workflow for a specific use case
- `run_interactive_session() -> None`: Runs an interactive N8N integration session

## N8N Integration

The N8N Integration Agent can connect with N8N in the following ways:

1. **Workflow Creation**: Generate workflow JSON that can be imported into N8N
2. **Direct API Integration**: Deploy workflows directly to N8N using the N8N API
3. **SolnAI Agent Integration**: Connect SolnAI agents with N8N workflows
4. **Workflow Optimization**: Improve existing N8N workflows

## Integration with SolnAI

The N8N Integration Agent is designed to integrate seamlessly with other SolnAI agents:

- Can use the Research Agent to gather information for workflows
- Works with the Code Executor Agent for custom code execution in workflows
- Integrates with the Data Analysis Agent for data processing in workflows
- Supports the SolnAI message format for interoperability

## Limitations

- The current implementation requires an N8N instance with API access for deployment
- Complex workflows may require manual adjustments
- Performance depends on the quality of the Claude 3.7 Sonnet model and its API availability

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="requirements.txt">
autogen-agentchat==0.4.7
autogen-core==0.2.11
autogen-ext==0.2.11
anthropic>=0.18.0
requests>=2.31.0
python-dotenv>=1.0.0
</file>

</files>
