This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
markdown_results/
  agent_openrouter_summary_test_docx.md
  agent_openrouter_summary_test_html.md
  agent_openrouter_summary_test_jpg.md
  agent_openrouter_summary_test_pdf.md
  agent_openrouter_summary_test_pptx.md
  agent_openrouter_summary_test_xlsx.md
  api_openrouter_test_docx.md
  api_openrouter_test_html.md
  api_openrouter_test_jpg.md
  api_openrouter_test_pdf.md
  api_openrouter_test_pptx.md
  api_openrouter_test_xlsx.md
  api_openrouter_vision_test_jpg.md
  local_convert_test_docx.md
  local_convert_test_html.md
  local_convert_test_jpg.md
  local_convert_test_pdf.md
  local_convert_test_pptx.md
  local_convert_test_xlsx.md
  test.md
supabase/
  migrations/
    20250128_document_cache.sql
    20250128_messages.sql
test_files/
  test.html
.dockerignore
.gitignore
Dockerfile
env.example
file_agent.py
README.md
requirements.txt
run_tests.sh
validation_test.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="markdown_results/agent_openrouter_summary_test_docx.md">
# Original Content



AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation

Qingyun Wu , Gagan Bansal , Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen W. White, Doug Burger, Chi Wang

# Abstract

AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

# Introduction

Large language models (LLMs) are becoming a crucial building block in developing powerful agents that utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi et al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could benefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of agents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help encourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023), and provide validation (Wu et al., 2023).

## d666f1f7-46cb-42bd-9a39-9a39cf2a509f

In light of the intuition and early evidence of promise, it is intriguing to ask the following question: how can we facilitate the development of LLM applications that could span a broad spectrum of domains and complexities based on the multi-agent approach? Our insight is to use multi-agent conversations to achieve it. There are at least three reasons confirming its general feasibility and utility thanks to recent advances in LLMs: First, because chat optimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate through conversations with each other or human(s), e.g., a dialog where agents provide and seek reasoning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad range of capabilities (especially when configured with the correct prompt and inference settings), conversations between differently configured agents can help combine these broad LLM capabilities in a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex tasks when the tasks are broken into simpler subtasks. Here is a random UUID in the middle of the paragraph! 314b0a30-5b04-470b-b9f7-eed2c2bec74a Multi-agent conversations can enable this partitioning and integration in an intuitive manner. How can we leverage the above insights and support different applications with the common requirement of coordinating multiple agents, potentially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent conversation framework with generic abstraction and effective implementation that has the flexibility to satisfy different application needs. Achieving this requires addressing two critical questions: (1) How can we design individual agents that are capable, reusable, customizable, and effective in multi-agent collaboration? (2) How can we develop a straightforward, unified interface that can accommodate a wide range of agent conversation patterns? In practice, applications of varying complexities may need distinct sets of agents with specific capabilities, and may require different conversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and static vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent interactions in natural language or code. Failing to adequately address these two questions would limit the framework’s scope of applicability and generality.

Here is a random table for .docx parsing test purposes:

| 1 | 2 | 3 | 4 | 5 | 6 |
| --- | --- | --- | --- | --- | --- |
| 7 | 8 | 9 | 10 | 11 | 12 |
| 13 | 14 | 49e168b7-d2ae-407f-a055-2167576f39a1 | 15 | 16 | 17 |
| 18 | 19 | 20 | 21 | 22 | 23 |
| 24 | 25 | 26 | 27 | 28 | 29 |



# Summary

 AutoGen is an open-source framework that enables developers to create LLM applications through multi-agent conversations. These agents, customizable and capable of operating in various modes, can interact with each other using combinations of LLMs, human inputs, and tools. The framework is versatile, supporting a range of applications across various complexities and LLM capacities, demonstrated in domains like mathematics, coding, question answering, and more. The goal is to facilitate the development of diverse applications using multi-agent conversations, addressing the need for scalable and adaptable solutions in a growing number of real-world tasks.
</file>

<file path="markdown_results/agent_openrouter_summary_test_html.md">
# Original Content


[Skip to main content](#__docusaurus_skipToContent_fallback)What's new in AutoGen? Read [this blog](/autogen/blog/2024/03/03/AutoGen-Update) for an overview of updates[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[API](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Notebooks](/autogen/docs/notebooks)[Gallery](/autogen/docs/Gallery)Other Languages

* [Dotnet](https://microsoft.github.io/autogen-for-net/)
[GitHub](https://github.com/microsoft/autogen)`ctrl``K`Recent posts

* [What's New in AutoGen?](/autogen/blog/2024/03/03/AutoGen-Update)
* [StateFlow - Build LLM Workflows with Customized State-Oriented Transition Function in GroupChat](/autogen/blog/2024/02/29/StateFlow)
* [FSM Group Chat -- User-specified agent transitions](/autogen/blog/2024/02/11/FSM-GroupChat)
* [Anny: Assisting AutoGen Devs Via AutoGen](/autogen/blog/2024/02/02/AutoAnny)
* [AutoGen with Custom Models: Empowering Users to Use Their Own Inference Mechanism](/autogen/blog/2024/01/26/Custom-Models)
* [AutoGenBench -- A Tool for Measuring and Evaluating AutoGen Agents](/autogen/blog/2024/01/25/AutoGenBench)
* [Code execution is now by default inside docker container](/autogen/blog/2024/01/23/Code-execution-in-docker)
* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)
# Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH

April 21, 2023 · 6 min read[![Chi Wang](https://github.com/sonichi.png)](https://www.linkedin.com/in/chi-wang-49b15b16/)[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)Principal Researcher at Microsoft Research

![level 2 algebra](/autogen/assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png)

**TL;DR:**

* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**
* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**
* **AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.**

Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?

In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for [MATH](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html), a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.

We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.

We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.

## Experiment Setup[​](#experiment-setup "Direct link to Experiment Setup")

We use AutoGen to select between the following models with a target inference budget $0.02 per instance:

* gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app
* gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo

We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:

* temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].
* top\_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].
* max\_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].
* n: The number of responses to generate. We search for the optimal n in the range of [1, 100].
* prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.

In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.

## Experiment Results[​](#experiment-results "Direct link to Experiment Results")

The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.

Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.
The same observation can be obtained on the level 3 Algebra test set.

![level 3 algebra](/autogen/assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png)

However, the selected model changes on level 4 Algebra.

![level 4 algebra](/autogen/assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png)

This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.
On level 5 the result is similar.

![level 5 algebra](/autogen/assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png)

We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.

An example notebook to run these experiments can be found at: <https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb>. The experiments were run when AutoGen was a subpackage in FLAML.

## Analysis and Discussion[​](#analysis-and-discussion "Direct link to Analysis and Discussion")

While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.

There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via [`flaml.tune`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).

The need for model selection, parameter tuning and cost saving is not specific to the math problems. The [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.

## For Further Reading[​](#for-further-reading "Direct link to For Further Reading")

* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)
* [Documentation about inference tuning](/autogen/docs/Use-Cases/enhanced_inference)

*Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our [Discord](https://discord.gg/pAbnFJrkgZ) server for discussion.*

**Tags:**

* [LLM](/autogen/blog/tags/llm)
* [GPT](/autogen/blog/tags/gpt)
* [research](/autogen/blog/tags/research)
[Newer PostAchieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)

* [Experiment Setup](#experiment-setup)
* [Experiment Results](#experiment-results)
* [Analysis and Discussion](#analysis-and-discussion)
* [For Further Reading](#for-further-reading)
Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)
Copyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)


# Summary

The blog post discusses a study using AutoGen, a tool that helps with model selection, parameter tuning, and cost-saving in large language model (LLM) applications. The study explores the impact of model and inference parameters on the performance of LLMs, using a case study for a math problem-solving benchmark called MATH. The results show that by tuning inference parameters like model, number of responses, temperature, and top_p, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems. The tuned gpt-3.5-turbo model outperformed untuned gpt-4 for easy problems, while tuned gpt-4 was more accurate and less expensive for hard problems. The study suggests that AutoGen can help with cost-saving in LLM applications.
</file>

<file path="markdown_results/agent_openrouter_summary_test_jpg.md">
![test](../test_files/test.jpg)

# Original Content


# Description:
The image depicts a woman in a stunning wedding dress, set against a serene blue sky. The woman's dark hair flows in the wind as she stands confidently, her gaze directed to the right. Her elegant white strapless gown features a sweetheart neckline and a fitted bodice adorned with intricate lace details. The dress flares out into a dramatic mermaid-style skirt, complete with a flowing train that adds to its overall grandeur. The woman's pose exudes poise and sophistication, as if she is about to walk down the aisle. The soft, gradient blue sky in the background provides a beautiful contrast to the woman's radiant white dress, creating a sense of serenity and joy. Overall, the image captures the essence of a romantic and unforgettable wedding moment.


# Summary

 The image presents a woman in a flowy, majestic white wedding dress, overlooking a tranquil blue sky. Her confident pose, intricate lace details, and mermaid-style skirt evoke a sense of poise and sophistication. The backdrop of the blue sky provides a serene and romantic ambiance to this unforgettable wedding moment.
</file>

<file path="markdown_results/agent_openrouter_summary_test_pdf.md">
# Original Content

This is a test PDF content



# Summary

 The provided content appears to be a text without specific topic or structure. However, if I had to summarize, I would say:

This content seems to be a mix of information discussing various topics such as cooking, nutrition, health, and wellness. It contains tips on preparing healthy meals, the benefits of certain foods, and recommendations for maintaining a balance lifestyle. It also touches on the topic of self-care and mental health, emphasizing the importance of mindfulness and stress management practices.
</file>

<file path="markdown_results/agent_openrouter_summary_test_pptx.md">
# Original Content

<!-- Slide number: 1 -->
# AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
Qingyun Wu , Gagan Bansal , Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen W. White, Doug Burger, Chi Wang

<!-- Slide number: 2 -->
# 2cdda5c8-e50e-4db4-b5f0-9722a649f455
AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and 04191ea8-5c73-4215-a1d3-1cfb43aaaf12 can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

![The first page of the AutoGen ArXiv paper.  44bf7d06-5e7a-4a40-a2e1-a2e42ef28c8a](Picture4.jpg)

<!-- Slide number: 3 -->
# A table to test parsing:

| ColA | ColB | ColC | ColD | ColE | ColF |
| --- | --- | --- | --- | --- | --- |
| 1 | 2 | 3 | 4 | 5 | 6 |
| 7 | 8 | 9 | 1b92870d-e3b5-4e65-8153-919f4ff45592 | 11 | 12 |
| 13 | 14 | 15 | 16 | 17 | 18 |

# Summary

 AutoGen is an open-source framework that enables developers to build various LLM (Large Language Model) applications using multiple conversing agents. These agents can be customized, operate in diverse modes, and interact based on defined behaviors. The framework is flexible, supporting both natural language and specially-designed language to program conversation patterns for various applications. AutoGen has been proven effective in numerous example applications across domains such as mathematics, coding, question answering, operations research, online decision-making, and entertainment. The content also includes an example table, though no summary is provided for that specific table.
</file>

<file path="markdown_results/agent_openrouter_summary_test_xlsx.md">
# Original Content

## Sheet1
| Alpha | Beta | Gamma | Delta |
| --- | --- | --- | --- |
| 89 | 82 | 100 | 12 |
| 76 | 89 | 33 | 42 |
| 60 | 84 | 19 | 19 |
| 7 | 69 | 10 | 17 |
| 87 | 89 | 86 | 54 |
| 23 | 4 | 89 | 25 |
| 70 | 84 | 62 | 59 |
| 83 | 37 | 43 | 21 |
| 71 | 15 | 88 | 32 |
| 20 | 62 | 20 | 67 |
| 67 | 18 | 15 | 48 |
| 42 | 5 | 15 | 67 |
| 58 | 6ff4173b-42a5-4784-9b19-f49caff4d93d | 22 | 9 |
| 49 | 93 | 6 | 38 |
| 82 | 28 | 1 | 39 |
| 95 | 55 | 18 | 82 |
| 50 | 46 | 98 | 86 |
| 31 | 46 | 47 | 82 |
| 40 | 65 | 19 | 31 |
| 95 | 65 | 29 | 62 |
| 68 | 57 | 34 | 54 |
| 96 | 66 | 63 | 14 |
| 87 | 93 | 95 | 80 |

## 09060124-b5e7-4717-9d07-3c046eb
| ColA | ColB | ColC | ColD |
| --- | --- | --- | --- |
| 1 | 2 | 3 | 4 |
| 5 | 6 | 7 | 8 |
| 9 | 10 | 11 | 12 |
| 13 | 14 | 15 | affc7dad-52dc-4b98-9b5d-51e65d8a8ad0 |

# Summary

 The first table, labeled `Sheet1`, contains a set of numerical data in four columns (Alpha, Beta, Gamma, Delta) for 16 entries. Each row represents a data point where columns Alpha, Beta, and Gamma have integer values, while Delta has a mix of integers and what appears to be a string represented as a hexadecimal number. The values in the rows show varying patterns without an apparent consistent trend.

The second table, labeled with a UUID (09060124-b5e7-4717-9d07-3c046eb), has four columns (ColA, ColB, ColC, ColD) and 4 entries. The last entry has a string value in its ColD column, which is represented as another UUID (affc7dad-52dc-4b98-9b5d-51e65d8a8ad0). The remaining entries appear to have numerical values in all columns without an obvious pattern.
</file>

<file path="markdown_results/api_openrouter_test_docx.md">
AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation

Qingyun Wu , Gagan Bansal , Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen W. White, Doug Burger, Chi Wang

# Abstract

AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

# Introduction

Large language models (LLMs) are becoming a crucial building block in developing powerful agents that utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi et al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could benefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of agents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help encourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023), and provide validation (Wu et al., 2023).

## d666f1f7-46cb-42bd-9a39-9a39cf2a509f

In light of the intuition and early evidence of promise, it is intriguing to ask the following question: how can we facilitate the development of LLM applications that could span a broad spectrum of domains and complexities based on the multi-agent approach? Our insight is to use multi-agent conversations to achieve it. There are at least three reasons confirming its general feasibility and utility thanks to recent advances in LLMs: First, because chat optimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate through conversations with each other or human(s), e.g., a dialog where agents provide and seek reasoning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad range of capabilities (especially when configured with the correct prompt and inference settings), conversations between differently configured agents can help combine these broad LLM capabilities in a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex tasks when the tasks are broken into simpler subtasks. Here is a random UUID in the middle of the paragraph! 314b0a30-5b04-470b-b9f7-eed2c2bec74a Multi-agent conversations can enable this partitioning and integration in an intuitive manner. How can we leverage the above insights and support different applications with the common requirement of coordinating multiple agents, potentially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent conversation framework with generic abstraction and effective implementation that has the flexibility to satisfy different application needs. Achieving this requires addressing two critical questions: (1) How can we design individual agents that are capable, reusable, customizable, and effective in multi-agent collaboration? (2) How can we develop a straightforward, unified interface that can accommodate a wide range of agent conversation patterns? In practice, applications of varying complexities may need distinct sets of agents with specific capabilities, and may require different conversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and static vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent interactions in natural language or code. Failing to adequately address these two questions would limit the framework’s scope of applicability and generality.

Here is a random table for .docx parsing test purposes:

| 1 | 2 | 3 | 4 | 5 | 6 |
| --- | --- | --- | --- | --- | --- |
| 7 | 8 | 9 | 10 | 11 | 12 |
| 13 | 14 | 49e168b7-d2ae-407f-a055-2167576f39a1 | 15 | 16 | 17 |
| 18 | 19 | 20 | 21 | 22 | 23 |
| 24 | 25 | 26 | 27 | 28 | 29 |
</file>

<file path="markdown_results/api_openrouter_test_html.md">
# Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH | AutoGen


[Skip to main content](#__docusaurus_skipToContent_fallback)What's new in AutoGen? Read [this blog](/autogen/blog/2024/03/03/AutoGen-Update) for an overview of updates[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[API](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Notebooks](/autogen/docs/notebooks)[Gallery](/autogen/docs/Gallery)Other Languages

* [Dotnet](https://microsoft.github.io/autogen-for-net/)
[GitHub](https://github.com/microsoft/autogen)`ctrl``K`Recent posts

* [What's New in AutoGen?](/autogen/blog/2024/03/03/AutoGen-Update)
* [StateFlow - Build LLM Workflows with Customized State-Oriented Transition Function in GroupChat](/autogen/blog/2024/02/29/StateFlow)
* [FSM Group Chat -- User-specified agent transitions](/autogen/blog/2024/02/11/FSM-GroupChat)
* [Anny: Assisting AutoGen Devs Via AutoGen](/autogen/blog/2024/02/02/AutoAnny)
* [AutoGen with Custom Models: Empowering Users to Use Their Own Inference Mechanism](/autogen/blog/2024/01/26/Custom-Models)
* [AutoGenBench -- A Tool for Measuring and Evaluating AutoGen Agents](/autogen/blog/2024/01/25/AutoGenBench)
* [Code execution is now by default inside docker container](/autogen/blog/2024/01/23/Code-execution-in-docker)
* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)
# Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH

April 21, 2023 · 6 min read[![Chi Wang](https://github.com/sonichi.png)](https://www.linkedin.com/in/chi-wang-49b15b16/)[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)Principal Researcher at Microsoft Research

![level 2 algebra](/autogen/assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png)

**TL;DR:**

* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**
* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**
* **AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.**

Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?

In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for [MATH](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html), a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.

We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.

We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.

## Experiment Setup[​](#experiment-setup "Direct link to Experiment Setup")

We use AutoGen to select between the following models with a target inference budget $0.02 per instance:

* gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app
* gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo

We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:

* temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].
* top\_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].
* max\_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].
* n: The number of responses to generate. We search for the optimal n in the range of [1, 100].
* prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.

In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.

## Experiment Results[​](#experiment-results "Direct link to Experiment Results")

The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.

Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.
The same observation can be obtained on the level 3 Algebra test set.

![level 3 algebra](/autogen/assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png)

However, the selected model changes on level 4 Algebra.

![level 4 algebra](/autogen/assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png)

This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.
On level 5 the result is similar.

![level 5 algebra](/autogen/assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png)

We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.

An example notebook to run these experiments can be found at: <https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb>. The experiments were run when AutoGen was a subpackage in FLAML.

## Analysis and Discussion[​](#analysis-and-discussion "Direct link to Analysis and Discussion")

While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.

There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via [`flaml.tune`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).

The need for model selection, parameter tuning and cost saving is not specific to the math problems. The [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.

## For Further Reading[​](#for-further-reading "Direct link to For Further Reading")

* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)
* [Documentation about inference tuning](/autogen/docs/Use-Cases/enhanced_inference)

*Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our [Discord](https://discord.gg/pAbnFJrkgZ) server for discussion.*

**Tags:**

* [LLM](/autogen/blog/tags/llm)
* [GPT](/autogen/blog/tags/gpt)
* [research](/autogen/blog/tags/research)
[Newer PostAchieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)

* [Experiment Setup](#experiment-setup)
* [Experiment Results](#experiment-results)
* [Analysis and Discussion](#analysis-and-discussion)
* [For Further Reading](#for-further-reading)
Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)
Copyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)
</file>

<file path="markdown_results/api_openrouter_test_jpg.md">
# Description:
The image depicts a woman in a wedding dress, center frame, with her black hair flying to the right as she stands against a gradient background.

The woman wears a white wedding dress, featuring a sweetheart neckline and a fitted bodice, which hugs her curves before flaring out in an A-line silhouette. Her skirt falls in dramatic swags to the floor, completing her elegant look. She accessorizes with large, dangling earrings, adding to her overall poise and style.

Set against a rich blue gradient background, which gradually lightens from top to bottom, the image exudes a sense of opulence and sophistication. The contrast of the bright dress against the darker, somewhat burnt-out background emphasizes her striking beauty. The fact that the woman stands alone, so elegantly poised, implicitly suggests it is her wedding day, as she gazes into the distance with confidence and joy.
</file>

<file path="markdown_results/api_openrouter_test_pdf.md">
This is a test PDF content
</file>

<file path="markdown_results/api_openrouter_test_pptx.md">
<!-- Slide number: 1 -->
# AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
Qingyun Wu , Gagan Bansal , Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen W. White, Doug Burger, Chi Wang

<!-- Slide number: 2 -->
# 2cdda5c8-e50e-4db4-b5f0-9722a649f455
AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and 04191ea8-5c73-4215-a1d3-1cfb43aaaf12 can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

![The first page of the AutoGen ArXiv paper.  44bf7d06-5e7a-4a40-a2e1-a2e42ef28c8a](Picture4.jpg)

<!-- Slide number: 3 -->
# A table to test parsing:

| ColA | ColB | ColC | ColD | ColE | ColF |
| --- | --- | --- | --- | --- | --- |
| 1 | 2 | 3 | 4 | 5 | 6 |
| 7 | 8 | 9 | 1b92870d-e3b5-4e65-8153-919f4ff45592 | 11 | 12 |
| 13 | 14 | 15 | 16 | 17 | 18 |
</file>

<file path="markdown_results/api_openrouter_test_xlsx.md">
## Sheet1
| Alpha | Beta | Gamma | Delta |
| --- | --- | --- | --- |
| 89 | 82 | 100 | 12 |
| 76 | 89 | 33 | 42 |
| 60 | 84 | 19 | 19 |
| 7 | 69 | 10 | 17 |
| 87 | 89 | 86 | 54 |
| 23 | 4 | 89 | 25 |
| 70 | 84 | 62 | 59 |
| 83 | 37 | 43 | 21 |
| 71 | 15 | 88 | 32 |
| 20 | 62 | 20 | 67 |
| 67 | 18 | 15 | 48 |
| 42 | 5 | 15 | 67 |
| 58 | 6ff4173b-42a5-4784-9b19-f49caff4d93d | 22 | 9 |
| 49 | 93 | 6 | 38 |
| 82 | 28 | 1 | 39 |
| 95 | 55 | 18 | 82 |
| 50 | 46 | 98 | 86 |
| 31 | 46 | 47 | 82 |
| 40 | 65 | 19 | 31 |
| 95 | 65 | 29 | 62 |
| 68 | 57 | 34 | 54 |
| 96 | 66 | 63 | 14 |
| 87 | 93 | 95 | 80 |

## 09060124-b5e7-4717-9d07-3c046eb
| ColA | ColB | ColC | ColD |
| --- | --- | --- | --- |
| 1 | 2 | 3 | 4 |
| 5 | 6 | 7 | 8 |
| 9 | 10 | 11 | 12 |
| 13 | 14 | 15 | affc7dad-52dc-4b98-9b5d-51e65d8a8ad0 |
</file>

<file path="markdown_results/api_openrouter_vision_test_jpg.md">
# Description:
The image depicts a woman in a stunning wedding dress, set against a serene blue sky. The woman's dark hair flows in the wind as she stands confidently, her gaze directed to the right. Her elegant white strapless gown features a sweetheart neckline and a fitted bodice adorned with intricate lace details. The dress flares out into a dramatic mermaid-style skirt, complete with a flowing train that adds to its overall grandeur. The woman's pose exudes poise and sophistication, as if she is about to walk down the aisle. The soft, gradient blue sky in the background provides a beautiful contrast to the woman's radiant white dress, creating a sense of serenity and joy. Overall, the image captures the essence of a romantic and unforgettable wedding moment.
</file>

<file path="markdown_results/local_convert_test_docx.md">
AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation

Qingyun Wu , Gagan Bansal , Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen W. White, Doug Burger, Chi Wang

# Abstract

AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

# Introduction

Large language models (LLMs) are becoming a crucial building block in developing powerful agents that utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi et al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could benefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of agents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help encourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023), and provide validation (Wu et al., 2023).

## d666f1f7-46cb-42bd-9a39-9a39cf2a509f

In light of the intuition and early evidence of promise, it is intriguing to ask the following question: how can we facilitate the development of LLM applications that could span a broad spectrum of domains and complexities based on the multi-agent approach? Our insight is to use multi-agent conversations to achieve it. There are at least three reasons confirming its general feasibility and utility thanks to recent advances in LLMs: First, because chat optimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate through conversations with each other or human(s), e.g., a dialog where agents provide and seek reasoning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad range of capabilities (especially when configured with the correct prompt and inference settings), conversations between differently configured agents can help combine these broad LLM capabilities in a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex tasks when the tasks are broken into simpler subtasks. Here is a random UUID in the middle of the paragraph! 314b0a30-5b04-470b-b9f7-eed2c2bec74a Multi-agent conversations can enable this partitioning and integration in an intuitive manner. How can we leverage the above insights and support different applications with the common requirement of coordinating multiple agents, potentially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent conversation framework with generic abstraction and effective implementation that has the flexibility to satisfy different application needs. Achieving this requires addressing two critical questions: (1) How can we design individual agents that are capable, reusable, customizable, and effective in multi-agent collaboration? (2) How can we develop a straightforward, unified interface that can accommodate a wide range of agent conversation patterns? In practice, applications of varying complexities may need distinct sets of agents with specific capabilities, and may require different conversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and static vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent interactions in natural language or code. Failing to adequately address these two questions would limit the framework’s scope of applicability and generality.

Here is a random table for .docx parsing test purposes:

| 1 | 2 | 3 | 4 | 5 | 6 |
| --- | --- | --- | --- | --- | --- |
| 7 | 8 | 9 | 10 | 11 | 12 |
| 13 | 14 | 49e168b7-d2ae-407f-a055-2167576f39a1 | 15 | 16 | 17 |
| 18 | 19 | 20 | 21 | 22 | 23 |
| 24 | 25 | 26 | 27 | 28 | 29 |
</file>

<file path="markdown_results/local_convert_test_html.md">
[Skip to main content](#__docusaurus_skipToContent_fallback)What's new in AutoGen? Read [this blog](/autogen/blog/2024/03/03/AutoGen-Update) for an overview of updates[![AutoGen](/autogen/img/ag.svg)![AutoGen](/autogen/img/ag.svg)**AutoGen**](/autogen/)[Docs](/autogen/docs/Getting-Started)[API](/autogen/docs/reference/agentchat/conversable_agent)[Blog](/autogen/blog)[FAQ](/autogen/docs/FAQ)[Examples](/autogen/docs/Examples)[Notebooks](/autogen/docs/notebooks)[Gallery](/autogen/docs/Gallery)Other Languages

* [Dotnet](https://microsoft.github.io/autogen-for-net/)
[GitHub](https://github.com/microsoft/autogen)`ctrl``K`Recent posts

* [What's New in AutoGen?](/autogen/blog/2024/03/03/AutoGen-Update)
* [StateFlow - Build LLM Workflows with Customized State-Oriented Transition Function in GroupChat](/autogen/blog/2024/02/29/StateFlow)
* [FSM Group Chat -- User-specified agent transitions](/autogen/blog/2024/02/11/FSM-GroupChat)
* [Anny: Assisting AutoGen Devs Via AutoGen](/autogen/blog/2024/02/02/AutoAnny)
* [AutoGen with Custom Models: Empowering Users to Use Their Own Inference Mechanism](/autogen/blog/2024/01/26/Custom-Models)
* [AutoGenBench -- A Tool for Measuring and Evaluating AutoGen Agents](/autogen/blog/2024/01/25/AutoGenBench)
* [Code execution is now by default inside docker container](/autogen/blog/2024/01/23/Code-execution-in-docker)
* [All About Agent Descriptions](/autogen/blog/2023/12/29/AgentDescriptions)
* [AgentOptimizer - An Agentic Way to Train Your LLM Agent](/autogen/blog/2023/12/23/AgentOptimizer)
* [AutoGen Studio: Interactively Explore Multi-Agent Workflows](/autogen/blog/2023/12/01/AutoGenStudio)
* [Agent AutoBuild - Automatically Building Multi-agent Systems](/autogen/blog/2023/11/26/Agent-AutoBuild)
* [How to Assess Utility of LLM-powered Applications?](/autogen/blog/2023/11/20/AgentEval)
* [AutoGen Meets GPTs](/autogen/blog/2023/11/13/OAI-assistants)
* [EcoAssistant - Using LLM Assistants More Accurately and Affordably](/autogen/blog/2023/11/09/EcoAssistant)
* [Multimodal with GPT-4V and LLaVA](/autogen/blog/2023/11/06/LMM-Agent)
* [AutoGen's Teachable Agents](/autogen/blog/2023/10/26/TeachableAgent)
* [Retrieval-Augmented Generation (RAG) Applications with AutoGen](/autogen/blog/2023/10/18/RetrieveChat)
* [Use AutoGen for Local LLMs](/autogen/blog/2023/07/14/Local-LLMs)
* [MathChat - An Conversational Framework to Solve Math Problems](/autogen/blog/2023/06/28/MathChat)
* [Achieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)
* [Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH](/autogen/blog/2023/04/21/LLM-tuning-math)
# Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH

April 21, 2023 · 6 min read[![Chi Wang](https://github.com/sonichi.png)](https://www.linkedin.com/in/chi-wang-49b15b16/)[Chi Wang](https://www.linkedin.com/in/chi-wang-49b15b16/)Principal Researcher at Microsoft Research

![level 2 algebra](/autogen/assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png)

**TL;DR:**

* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**
* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**
* **AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.**

Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?

In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for [MATH](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html), a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.

We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.

We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.

## Experiment Setup[​](#experiment-setup "Direct link to Experiment Setup")

We use AutoGen to select between the following models with a target inference budget $0.02 per instance:

* gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app
* gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo

We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:

* temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].
* top\_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].
* max\_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].
* n: The number of responses to generate. We search for the optimal n in the range of [1, 100].
* prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.

In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.

## Experiment Results[​](#experiment-results "Direct link to Experiment Results")

The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.

Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.
The same observation can be obtained on the level 3 Algebra test set.

![level 3 algebra](/autogen/assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png)

However, the selected model changes on level 4 Algebra.

![level 4 algebra](/autogen/assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png)

This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.
On level 5 the result is similar.

![level 5 algebra](/autogen/assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png)

We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.

An example notebook to run these experiments can be found at: <https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb>. The experiments were run when AutoGen was a subpackage in FLAML.

## Analysis and Discussion[​](#analysis-and-discussion "Direct link to Analysis and Discussion")

While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.

There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via [`flaml.tune`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).

The need for model selection, parameter tuning and cost saving is not specific to the math problems. The [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.

## For Further Reading[​](#for-further-reading "Direct link to For Further Reading")

* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)
* [Documentation about inference tuning](/autogen/docs/Use-Cases/enhanced_inference)

*Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our [Discord](https://discord.gg/pAbnFJrkgZ) server for discussion.*

**Tags:**

* [LLM](/autogen/blog/tags/llm)
* [GPT](/autogen/blog/tags/gpt)
* [research](/autogen/blog/tags/research)
[Newer PostAchieve More, Pay Less - Use GPT-4 Smartly](/autogen/blog/2023/05/18/GPT-adaptive-humaneval)

* [Experiment Setup](#experiment-setup)
* [Experiment Results](#experiment-results)
* [Analysis and Discussion](#analysis-and-discussion)
* [For Further Reading](#for-further-reading)
Community

* [Discord](https://discord.gg/pAbnFJrkgZ)
* [Twitter](https://twitter.com/pyautogen)
Copyright © 2024 AutoGen Authors | [Privacy and Cookies](https://go.microsoft.com/fwlink/?LinkId=521839)
</file>

<file path="markdown_results/local_convert_test_jpg.md">
# Description:
The image depicts a woman in a stunning wedding dress, set against a serene blue sky. The woman's dark hair flows in the wind as she stands confidently, her gaze directed to the right. Her elegant white strapless gown features a sweetheart neckline and a fitted bodice adorned with intricate lace details. The dress flares out into a dramatic mermaid-style skirt, complete with a flowing train that adds to its overall grandeur. The woman's pose exudes poise and sophistication, as if she is about to walk down the aisle. The soft, gradient blue sky in the background provides a beautiful contrast to the woman's radiant white dress, creating a sense of serenity and joy. Overall, the image captures the essence of a romantic and unforgettable wedding moment.
</file>

<file path="markdown_results/local_convert_test_pdf.md">
This is a test PDF content
</file>

<file path="markdown_results/local_convert_test_pptx.md">
<!-- Slide number: 1 -->
# AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
Qingyun Wu , Gagan Bansal , Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen W. White, Doug Burger, Chi Wang

<!-- Slide number: 2 -->
# 2cdda5c8-e50e-4db4-b5f0-9722a649f455
AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and 04191ea8-5c73-4215-a1d3-1cfb43aaaf12 can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

![The first page of the AutoGen ArXiv paper.  44bf7d06-5e7a-4a40-a2e1-a2e42ef28c8a](Picture4.jpg)

<!-- Slide number: 3 -->
# A table to test parsing:

| ColA | ColB | ColC | ColD | ColE | ColF |
| --- | --- | --- | --- | --- | --- |
| 1 | 2 | 3 | 4 | 5 | 6 |
| 7 | 8 | 9 | 1b92870d-e3b5-4e65-8153-919f4ff45592 | 11 | 12 |
| 13 | 14 | 15 | 16 | 17 | 18 |
</file>

<file path="markdown_results/local_convert_test_xlsx.md">
## Sheet1
| Alpha | Beta | Gamma | Delta |
| --- | --- | --- | --- |
| 89 | 82 | 100 | 12 |
| 76 | 89 | 33 | 42 |
| 60 | 84 | 19 | 19 |
| 7 | 69 | 10 | 17 |
| 87 | 89 | 86 | 54 |
| 23 | 4 | 89 | 25 |
| 70 | 84 | 62 | 59 |
| 83 | 37 | 43 | 21 |
| 71 | 15 | 88 | 32 |
| 20 | 62 | 20 | 67 |
| 67 | 18 | 15 | 48 |
| 42 | 5 | 15 | 67 |
| 58 | 6ff4173b-42a5-4784-9b19-f49caff4d93d | 22 | 9 |
| 49 | 93 | 6 | 38 |
| 82 | 28 | 1 | 39 |
| 95 | 55 | 18 | 82 |
| 50 | 46 | 98 | 86 |
| 31 | 46 | 47 | 82 |
| 40 | 65 | 19 | 31 |
| 95 | 65 | 29 | 62 |
| 68 | 57 | 34 | 54 |
| 96 | 66 | 63 | 14 |
| 87 | 93 | 95 | 80 |

## 09060124-b5e7-4717-9d07-3c046eb
| ColA | ColB | ColC | ColD |
| --- | --- | --- | --- |
| 1 | 2 | 3 | 4 |
| 5 | 6 | 7 | 8 |
| 9 | 10 | 11 | 12 |
| 13 | 14 | 15 | affc7dad-52dc-4b98-9b5d-51e65d8a8ad0 |
</file>

<file path="markdown_results/test.md">
## Sheet1
| Alpha | Beta | Gamma | Delta |
| --- | --- | --- | --- |
| 89 | 82 | 100 | 12 |
| 76 | 89 | 33 | 42 |
| 60 | 84 | 19 | 19 |
| 7 | 69 | 10 | 17 |
| 87 | 89 | 86 | 54 |
| 23 | 4 | 89 | 25 |
| 70 | 84 | 62 | 59 |
| 83 | 37 | 43 | 21 |
| 71 | 15 | 88 | 32 |
| 20 | 62 | 20 | 67 |
| 67 | 18 | 15 | 48 |
| 42 | 5 | 15 | 67 |
| 58 | 6ff4173b-42a5-4784-9b19-f49caff4d93d | 22 | 9 |
| 49 | 93 | 6 | 38 |
| 82 | 28 | 1 | 39 |
| 95 | 55 | 18 | 82 |
| 50 | 46 | 98 | 86 |
| 31 | 46 | 47 | 82 |
| 40 | 65 | 19 | 31 |
| 95 | 65 | 29 | 62 |
| 68 | 57 | 34 | 54 |
| 96 | 66 | 63 | 14 |
| 87 | 93 | 95 | 80 |

## 09060124-b5e7-4717-9d07-3c046eb
| ColA | ColB | ColC | ColD |
| --- | --- | --- | --- |
| 1 | 2 | 3 | 4 |
| 5 | 6 | 7 | 8 |
| 9 | 10 | 11 | 12 |
| 13 | 14 | 15 | affc7dad-52dc-4b98-9b5d-51e65d8a8ad0 |
</file>

<file path="supabase/migrations/20250128_document_cache.sql">
-- Create document cache table
create table if not exists document_cache (
    id bigint generated by default as identity primary key,
    doc_hash text not null unique,
    file_name text not null,
    file_type text not null,
    markdown_content text not null,
    created_at timestamp with time zone not null,
    last_accessed timestamp with time zone not null
);

-- Create index for faster lookups
create index if not exists idx_document_cache_hash on document_cache(doc_hash);

-- Add RLS policies
alter table document_cache enable row level security;

-- Allow read access to authenticated users
create policy "Users can read document cache"
    on document_cache for select
    using (true);

-- Allow insert/update access to authenticated users
create policy "Users can insert document cache"
    on document_cache for insert
    with check (true);

create policy "Users can update document cache"
    on document_cache for update
    using (true);
</file>

<file path="supabase/migrations/20250128_messages.sql">
-- Drop existing table and related objects
drop table if exists messages cascade;

-- Create messages table
create table messages (
    id bigint generated by default as identity primary key,
    session_id text not null,
    message_type text not null,
    content text not null,
    data jsonb,
    created_at timestamp with time zone not null default now()
);

-- Create index for faster lookups
create index idx_messages_session on messages(session_id);

-- Add RLS policies
alter table messages enable row level security;

-- Allow read access to authenticated users
create policy "Users can read messages"
    on messages for select
    using (true);

-- Allow insert access to authenticated users
create policy "Users can insert messages"
    on messages for insert
    with check (true);

-- Allow update access to authenticated users
create policy "Users can update messages"
    on messages for update
    using (true);

-- Grant necessary permissions
grant all privileges on table messages to authenticated;
grant all privileges on sequence messages_id_seq to authenticated;

-- Notify Supabase of schema changes
notify pgrst, 'reload schema';
</file>

<file path="test_files/test.html">
<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH | AutoGen</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://microsoft.github.io/autogen/blog/2023/04/21/LLM-tuning-math"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH | AutoGen"><meta data-rh="true" name="description" content="level 2 algebra"><meta data-rh="true" property="og:description" content="level 2 algebra"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-04-21T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://www.linkedin.com/in/chi-wang-49b15b16/"><meta data-rh="true" property="article:tag" content="LLM,GPT,research"><link data-rh="true" rel="icon" href="/autogen/img/ag.ico"><link data-rh="true" rel="canonical" href="https://microsoft.github.io/autogen/blog/2023/04/21/LLM-tuning-math"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/autogen/blog/2023/04/21/LLM-tuning-math" hreflang="en"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/autogen/blog/2023/04/21/LLM-tuning-math" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/autogen/blog/rss.xml" title="AutoGen RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/autogen/blog/atom.xml" title="AutoGen Atom Feed">






<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
<script src="/autogen/js/custom.js" async defer="defer"></script><link rel="stylesheet" href="/autogen/assets/css/styles.ca10f300.css">
<script src="/autogen/assets/js/runtime~main.83ab9fec.js" defer="defer"></script>
<script src="/autogen/assets/js/main.5d28c826.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const a=new URLSearchParams(window.location.search).entries();for(var[t,e]of a)if(t.startsWith("docusaurus-data-")){var n=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(n,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#fafbfc;color:#091E42" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">What's new in AutoGen? Read <a href="/autogen/blog/2024/03/03/AutoGen-Update">this blog</a> for an overview of updates</div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/autogen/"><div class="navbar__logo"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AutoGen</b></a><a class="navbar__item navbar__link" href="/autogen/docs/Getting-Started">Docs</a><a class="navbar__item navbar__link" href="/autogen/docs/reference/agentchat/conversable_agent">API</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/autogen/blog">Blog</a><a class="navbar__item navbar__link" href="/autogen/docs/FAQ">FAQ</a><a class="navbar__item navbar__link" href="/autogen/docs/Examples">Examples</a><a class="navbar__item navbar__link" href="/autogen/docs/notebooks">Notebooks</a><a class="navbar__item navbar__link" href="/autogen/docs/Gallery">Gallery</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Other Languages</a><ul class="dropdown__menu"><li><a href="https://microsoft.github.io/autogen-for-net/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Dotnet<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><a href="https://github.com/microsoft/autogen" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2024/03/03/AutoGen-Update">What&#x27;s New in AutoGen?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2024/02/29/StateFlow">StateFlow - Build LLM Workflows with Customized State-Oriented Transition Function in GroupChat</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2024/02/11/FSM-GroupChat">FSM Group Chat -- User-specified agent transitions</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2024/02/02/AutoAnny">Anny: Assisting AutoGen Devs Via AutoGen</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2024/01/26/Custom-Models">AutoGen with Custom Models: Empowering Users to Use Their Own Inference Mechanism</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2024/01/25/AutoGenBench">AutoGenBench -- A Tool for Measuring and Evaluating AutoGen Agents</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2024/01/23/Code-execution-in-docker">Code execution is now by default inside docker container</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/12/29/AgentDescriptions">All About Agent Descriptions</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/12/23/AgentOptimizer">AgentOptimizer - An Agentic Way to Train Your LLM Agent</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/12/01/AutoGenStudio">AutoGen Studio: Interactively Explore Multi-Agent Workflows</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/11/26/Agent-AutoBuild">Agent AutoBuild - Automatically Building Multi-agent Systems</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/11/20/AgentEval">How to Assess Utility of LLM-powered Applications?</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/11/13/OAI-assistants">AutoGen Meets GPTs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/11/09/EcoAssistant">EcoAssistant - Using LLM Assistants More Accurately and Affordably</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/11/06/LMM-Agent">Multimodal with GPT-4V and LLaVA</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/10/26/TeachableAgent">AutoGen&#x27;s Teachable Agents</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/10/18/RetrieveChat">Retrieval-Augmented Generation (RAG) Applications with AutoGen</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/07/14/Local-LLMs">Use AutoGen for Local LLMs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/06/28/MathChat">MathChat - An Conversational Framework to Solve Math Problems</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/autogen/blog/2023/05/18/GPT-adaptive-humaneval">Achieve More, Pay Less - Use GPT-4 Smartly</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/autogen/blog/2023/04/21/LLM-tuning-math">Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="https://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="level 2 algebra"><header><h1 class="title_f1Hy" itemprop="headline">Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-04-21T00:00:00.000Z" itemprop="datePublished">April 21, 2023</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/chi-wang-49b15b16/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sonichi.png" alt="Chi Wang" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/chi-wang-49b15b16/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Chi Wang</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Researcher at Microsoft Research</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><p><img decoding="async" loading="lazy" alt="level 2 algebra" src="/autogen/assets/images/level2algebra-659ba95286432d9945fc89e84d606797.png" width="575" height="469" class="img_ev3q"></p>
<p><strong>TL;DR:</strong></p>
<ul>
<li><strong>Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.</strong></li>
<li><strong>For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.</strong></li>
<li><strong>AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.</strong></li>
</ul>
<p>Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?</p>
<p>In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for <a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html" target="_blank" rel="noopener noreferrer">MATH</a>, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.</p>
<p>We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search &amp; pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.</p>
<p>We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-setup">Experiment Setup<a href="#experiment-setup" class="hash-link" aria-label="Direct link to Experiment Setup" title="Direct link to Experiment Setup">​</a></h2>
<p>We use AutoGen to select between the following models with a target inference budget $0.02 per instance:</p>
<ul>
<li>gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app</li>
<li>gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo</li>
</ul>
<p>We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:</p>
<ul>
<li>temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].</li>
<li>top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].</li>
<li>max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].</li>
<li>n: The number of responses to generate. We search for the optimal n in the range of [1, 100].</li>
<li>prompt: We use the template: &quot;{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}.&quot; where {problem} will be replaced by the math problem instance.</li>
</ul>
<p>In this experiment, when n &gt; 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-results">Experiment Results<a href="#experiment-results" class="hash-link" aria-label="Direct link to Experiment Results" title="Direct link to Experiment Results">​</a></h2>
<p>The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.</p>
<p>Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.
The same observation can be obtained on the level 3 Algebra test set.</p>
<p><img decoding="async" loading="lazy" alt="level 3 algebra" src="/autogen/assets/images/level3algebra-94e87a683ac8832ac7ae6f41f30131a4.png" width="575" height="469" class="img_ev3q"></p>
<p>However, the selected model changes on level 4 Algebra.</p>
<p><img decoding="async" loading="lazy" alt="level 4 algebra" src="/autogen/assets/images/level4algebra-492beb22490df30d6cc258f061912dcd.png" width="580" height="469" class="img_ev3q"></p>
<p>This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.
On level 5 the result is similar.</p>
<p><img decoding="async" loading="lazy" alt="level 5 algebra" src="/autogen/assets/images/level5algebra-8fba701551334296d08580b4b489fe56.png" width="575" height="469" class="img_ev3q"></p>
<p>We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.</p>
<p>An example notebook to run these experiments can be found at: <a href="https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb" target="_blank" rel="noopener noreferrer">https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb</a>. The experiments were run when AutoGen was a subpackage in FLAML.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="analysis-and-discussion">Analysis and Discussion<a href="#analysis-and-discussion" class="hash-link" aria-label="Direct link to Analysis and Discussion" title="Direct link to Analysis and Discussion">​</a></h2>
<p>While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.</p>
<p>There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via <a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function" target="_blank" rel="noopener noreferrer"><code>flaml.tune</code></a>.</p>
<p>The need for model selection, parameter tuning and cost saving is not specific to the math problems. The <a href="https://github.com/Significant-Gravitas/Auto-GPT" target="_blank" rel="noopener noreferrer">Auto-GPT</a> project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="for-further-reading">For Further Reading<a href="#for-further-reading" class="hash-link" aria-label="Direct link to For Further Reading" title="Direct link to For Further Reading">​</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/2303.04673" target="_blank" rel="noopener noreferrer">Research paper about the tuning technique</a></li>
<li><a href="/autogen/docs/Use-Cases/enhanced_inference">Documentation about inference tuning</a></li>
</ul>
<p><em>Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our <a href="https://discord.gg/pAbnFJrkgZ" target="_blank" rel="noopener noreferrer">Discord</a> server for discussion.</em></p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/autogen/blog/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/autogen/blog/tags/gpt">GPT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/autogen/blog/tags/research">research</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/autogen/blog/2023/05/18/GPT-adaptive-humaneval"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Achieve More, Pay Less - Use GPT-4 Smartly</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#experiment-setup" class="table-of-contents__link toc-highlight">Experiment Setup</a></li><li><a href="#experiment-results" class="table-of-contents__link toc-highlight">Experiment Results</a></li><li><a href="#analysis-and-discussion" class="table-of-contents__link toc-highlight">Analysis and Discussion</a></li><li><a href="#for-further-reading" class="table-of-contents__link toc-highlight">For Further Reading</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.gg/pAbnFJrkgZ" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/pyautogen" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 AutoGen Authors |  <a target="_blank" style="color:#10adff" href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy and Cookies</a></div></div></div></footer></div>
</body>
</html>
</file>

<file path=".dockerignore">
__pycache__
.env
venv
</file>

<file path=".gitignore">
# Virtual Environment
venv/
env/
.env

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# IDE
.idea/
.vscode/
*.swp
*.swo

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db
</file>

<file path="Dockerfile">
FROM python:3.11-slim

# Build argument for port with default value
ARG PORT=8001
ENV PORT=${PORT}

WORKDIR /app

# Install system dependencies required for markitdown
RUN apt-get update && apt-get install -y \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first to leverage Docker cache
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application code
COPY . .

# Create directories for files
RUN mkdir -p test_files markdown_results

# Expose the port from build argument
EXPOSE ${PORT}

# Command to run the application
# Feel free to change sample_supabase_agent to sample_postgres_agent
CMD ["sh", "-c", "uvicorn file_agent:app --host 0.0.0.0 --port ${PORT}"]
</file>

<file path="env.example">
# Get your OpenRouter API Key from https://openrouter.ai/keys
OPENROUTER_API_KEY="sk-or-"

OPENROUTER_MODEL="mistralai/mistral-7b-instruct"
OPENROUTER_VLM_MODEL="meta-llama/llama-3.2-11b-vision-instruct:free"

# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL='https://yaiznnodpdb.supabase.co'

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY="e"

# Set this bearer token to whatever you want. This will be changed once the agent is hosted for you on the Studio!
API_BEARER_TOKEN='toto'
</file>

<file path="file_agent.py">
from typing import List, Optional, Dict, Any
from fastapi import FastAPI, HTTPException, Security, Depends, Request
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from fastapi.middleware.cors import CORSMiddleware
from supabase import create_client, Client
from pydantic import BaseModel
from dotenv import load_dotenv
from pathlib import Path
import sys
import os
import base64
from openai import OpenAI
from markitdown import MarkItDown
import hashlib
from datetime import datetime
import logging
import imghdr
import io
import re

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('markdown_results/file_agent.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv(override=True)  # Force reload environment variables

# Initialize FastAPI app and OpenRouter client
app = FastAPI()
security = HTTPBearer()
openai_client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY"),
    default_headers={
        "HTTP-Referer": "http://localhost:8001",  # Required for OpenRouter
        "X-Title": "MarkItDown App",  # Optional, for OpenRouter analytics
    }
)

# Initialize MarkItDown globally
md = MarkItDown(
    llm_client=openai_client,
    llm_model=os.getenv("OPENROUTER_MODEL", "mistralai/mistral-7b-instruct")
)

# Supabase setup
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response Models
class AgentRequest(BaseModel):
    query: str
    user_id: str
    request_id: str
    session_id: str
    files: Optional[List[Dict[str, Any]]] = None

class AgentResponse(BaseModel):
    success: bool
    markdown: str = ""
    error: Optional[str] = ""

class FileRequest(BaseModel):
    file: Dict[str, Any]  # Should include name, base64, type, and optionally model

class MarkdownResponse(BaseModel):
    success: bool
    markdown: str = ""
    error: str = ""

def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)) -> bool:
    """Verify the bearer token against environment variable."""
    expected_token = os.getenv("API_BEARER_TOKEN")
    if not expected_token:
        raise HTTPException(
            status_code=500,
            detail="API_BEARER_TOKEN environment variable not set"
        )
    if credentials.credentials != expected_token:
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication token"
        )
    return True

async def fetch_conversation_history(session_id: str, limit: int = 10) -> List[Dict[str, Any]]:
    """Fetch the most recent conversation history for a session."""
    try:
        response = supabase.table("messages") \
            .select("*") \
            .eq("session_id", session_id) \
            .order("created_at", desc=True) \
            .limit(limit) \
            .execute()
        
        # Convert to list and reverse to get chronological order
        messages = response.data[::-1]
        return messages
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch conversation history: {str(e)}")

async def store_message(session_id: str, message_type: str, content: str, data: Optional[Dict] = None):
    """Store a message in the Supabase messages table."""
    # Truncate content if it's too large (100KB limit)
    max_content_size = 100000  # 100KB
    if len(content) > max_content_size:
        content = content[:max_content_size] + "\n...(truncated)"
        logger.warning(f"Message content truncated to {max_content_size} characters")

    message_obj = {
        "type": message_type,
        "content": content
    }
    if data:
        message_obj["data"] = data

    try:
        supabase.table("messages").insert({
            "session_id": session_id,
            "message": message_obj
        }).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to store message: {str(e)}")            
            
    except Exception as e:
        logger.error(f"Failed to store message: {e}")
        # Don't raise the exception, just log it
        # This prevents message storage failures from breaking the main functionality

async def generate_summary(text: str) -> str:
    """Generate a summary using the OpenRouter API with Mistral model."""
    try:
        model = os.getenv("OPENROUTER_MODEL")
        if not model:
            raise ValueError("OPENROUTER_MODEL environment variable not set")
            
        response = openai_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that provides concise summaries."},
                {"role": "user", "content": f"Please provide a brief summary of the following content:\n\n{text}"}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Error generating summary: {str(e)}")
        return "Summary generation failed"

async def save_markdown_file(filename: str, content: str, summary: str = None) -> str:
    """Save markdown content to a file."""
    try:
        # Create directory if it doesn't exist
        os.makedirs('markdown_results', exist_ok=True)
        
        # Clean filename and create full path
        clean_filename = re.sub(r'[^\w\-_\.]', '_', filename)
        base, ext = os.path.splitext(clean_filename)
        markdown_path = os.path.join('markdown_results', f"{base}.md")
        
        # Combine summary and content
        full_content = ""
        if summary:
            full_content = f"# Summary\n{summary}\n\n# Content\n{content}"
        else:
            full_content = content
            
        # Save to file
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
            
        return markdown_path
    except Exception as e:
        logger.error(f"Error saving markdown file: {str(e)}")
        return ""

async def process_files_to_string(files: Optional[List[Dict[str, Any]]], query: str = "") -> str:
    """Convert a list of files with base64 content into a formatted string using MarkItDown."""
    if not files:
        return ""
        
    file_content = "File content to use as context:\n\n"
    
    for i, file in enumerate(files, 1):
        try:
            # Skip system files
            if file['name'].startswith('.'):
                logger.info(f"Skipping system file: {file['name']}")
                continue
                
            # Save base64 content to a temporary file
            decoded_content = base64.b64decode(file['base64'])
            
            # Detect if the content is an image using imghdr
            content_stream = io.BytesIO(decoded_content)
            image_type = imghdr.what(content_stream)
            is_image = image_type is not None
            
            temp_file_path = f"/tmp/temp_file_{file['name']}"
            with open(temp_file_path, "wb") as f:
                f.write(decoded_content)
            
            # Create appropriate MarkItDown instance based on file type
            if is_image:
                vlm_model = os.getenv("OPENROUTER_VLM_MODEL")
                if not vlm_model:
                    raise ValueError("OPENROUTER_VLM_MODEL environment variable not set")
                    
                logger.info(f"Detected image type: {image_type}, using vision model: {vlm_model}")
                temp_md = MarkItDown(
                    llm_client=openai_client,
                    llm_model=vlm_model
                )
            else:
                model = os.getenv("OPENROUTER_MODEL")
                if not model:
                    raise ValueError("OPENROUTER_MODEL environment variable not set")
                    
                temp_md = MarkItDown(
                    llm_client=openai_client,
                    llm_model=model
                )
            
            # Convert file to markdown using MarkItDown
            result = temp_md.convert(temp_file_path, use_llm=True)
            markdown_content = result.text_content
            
            # Clean up temporary file
            os.remove(temp_file_path)
            
            # If query is provided, use it with LLM
            if query:
                response = openai_client.chat.completions.create(
                    model=os.getenv("OPENROUTER_MODEL"),
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that processes text based on user queries."},
                        {"role": "user", "content": f"{query}\n\nText to process:\n{markdown_content}"}
                    ]
                )
                processed_content = response.choices[0].message.content
                file_content += f"{i}. {file['name']}:\n\n{processed_content}\n\n"
            else:
                file_content += f"{i}. {file['name']}:\n\n{markdown_content}\n\n"
                
            logger.info(f"Successfully processed {file['name']}")
            
        except Exception as e:
            logger.error(f"Error processing file {file['name']}: {str(e)}")
            # Fallback to direct text conversion if markdown conversion fails
            try:
                if is_image:
                    file_content += f"{i}. {file['name']} (image file - processing failed)\n\n"
                else:
                    text_content = decoded_content.decode('utf-8')
                    file_content += f"{i}. {file['name']} (plain text):\n\n{text_content}\n\n"
            except:
                file_content += f"{i}. {file['name']} (failed to process)\n\n"
    
    return file_content

async def get_document_hash(file_data: Dict[str, Any]) -> str:
    """Generate a unique hash for a document based on its content"""
    content = file_data.get('base64', '') or file_data.get('content', '')
    name = file_data.get('name', '')
    file_type = file_data.get('type', '')
    
    # Combine all fields to create a unique hash
    hash_input = f"{content}{name}{file_type}".encode('utf-8')
    return hashlib.sha256(hash_input).hexdigest()

async def store_document_markdown(
    supabase_client,
    doc_hash: str,
    markdown: str,
    file_data: Dict[str, Any]
) -> Dict[str, Any]:
    """Store document markdown in Supabase"""
    doc_data = {
        'doc_hash': doc_hash,
        'file_name': file_data.get('name'),
        'file_type': file_data.get('type'),
        'markdown_content': markdown,
        'created_at': datetime.utcnow().isoformat(),
        'last_accessed': datetime.utcnow().isoformat()
    }
    
    result = supabase_client.table('document_cache').upsert(doc_data).execute()
    return result.data[0] if result.data else None

async def get_cached_markdown(
    supabase_client,
    doc_hash: str
) -> Optional[str]:
    """Retrieve cached markdown from Supabase"""
    result = supabase_client.table('document_cache')\
        .select('markdown_content')\
        .eq('doc_hash', doc_hash)\
        .execute()
    
    if result.data:
        # Update last accessed timestamp
        supabase_client.table('document_cache')\
            .update({'last_accessed': datetime.utcnow().isoformat()})\
            .eq('doc_hash', doc_hash)\
            .execute()
        return result.data[0]['markdown_content']
    return None

async def process_file_cached(name: str, file_type: str, base64_content: str, model: str, use_cache: bool = True) -> Optional[str]:
    """Process a single file with caching."""
    try:
        # Create file data
        file_data = {
            'name': name,
            'type': file_type,
            'base64': base64_content,
            'model': model
        }
        
        # Get document hash
        doc_hash = await get_document_hash(file_data)
        
        if use_cache:
            # Try to get cached markdown
            cached_markdown = await get_cached_markdown(supabase, doc_hash)
            if cached_markdown:
                return cached_markdown
        
        # Convert file if not in cache
        markdown = await process_files_to_string([file_data])
        if markdown:
            # Store in cache
            await store_document_markdown(supabase, doc_hash, markdown, file_data)
            return markdown
            
    except Exception as e:
        logger.error(f"Error processing file {name}: {str(e)}")
        return None

@app.post("/api/file-agent", response_model=AgentResponse)
async def file_agent(
    request: AgentRequest,
    authenticated: bool = Depends(verify_token)
):
    try:
        logger.info(f"Received request: {request}")
        
        # Fetch conversation history from the DB
        conversation_history = await fetch_conversation_history(request.session_id)
        
        # Convert conversation history to format expected by agent
        messages = []
        for msg in conversation_history:
            msg_data = msg["message"]
            msg_type = "user" if msg_data["type"] == "human" else "assistant"
            msg_content = msg_data["content"]
            
            messages.append({"role": msg_type, "content": msg_content})

        # Store user's query with files if present
        message_data = {"request_id": request.request_id}
        if request.files:
            message_data["files"] = request.files

        await store_message(
            session_id=request.session_id,
            message_type="human",
            content=request.query,
            data=message_data
        )

        # Get markdown content from files using query as context
        markdown_content = "No markdown generated from your request. Upload a file and I'll convert it to Markdown!"
        if request.files:
            try:
                markdown_content = await process_files_to_string(request.files, query=request.query)
                logger.info(f"Successfully processed {len(request.files)} files with query: {request.query}")
            except Exception as e:
                logger.error(f"Error processing files: {str(e)}")
                raise e

        # Store agent's response
        await store_message(
            session_id=request.session_id,
            message_type="ai",
            content=markdown_content,
            data={"request_id": request.request_id}
        )

        return AgentResponse(success=True, markdown=markdown_content)

    except Exception as e:
        error_msg = f"Error processing request: {str(e)}"
        logger.error(error_msg)
        # Store error message in conversation
        await store_message(
            session_id=request.session_id,
            message_type="ai",
            content="I apologize, but I encountered an error processing your request.",
            data={"error": str(e), "request_id": request.request_id}
        )
        return AgentResponse(success=False, markdown="", error=str(e))

@app.post("/api/convert-to-markdown", response_model=MarkdownResponse)
async def convert_to_markdown(
    request: FileRequest,
    authenticated: bool = Depends(verify_token)
):
    """Convert a single file to markdown format."""
    try:
        logger.info(f"Processing file: {request.file['name']}")
        
        # Save base64 content to a temporary file
        decoded_content = base64.b64decode(request.file['base64'])
        
        # Detect if the content is an image using imghdr
        content_stream = io.BytesIO(decoded_content)
        image_type = imghdr.what(content_stream)
        is_image = image_type is not None
        
        temp_file_path = f"/tmp/temp_file_{request.file['name']}"
        try:
            with open(temp_file_path, "wb") as f:
                f.write(decoded_content)
            logger.info(f"Saved content to temporary file: {temp_file_path}")
            
            if is_image:
                logger.info(f"Detected image type: {image_type}, using vision model: {os.getenv('OPENROUTER_VLM_MODEL')}")
                try:
                    # Create a new MarkItDown instance with the vision model
                    temp_md = MarkItDown(
                        llm_client=openai_client,
                        llm_model=os.getenv("OPENROUTER_VLM_MODEL")
                    )
                    result = temp_md.convert(temp_file_path, use_llm=True)
                    if not result.text_content:
                        raise Exception("Vision model returned empty response")
                    logger.info("Successfully used vision model")
                except Exception as vision_error:
                    if "401" in str(vision_error):
                        logger.error(f"Vision model access unauthorized: {str(vision_error)}")
                        return MarkdownResponse(
                            success=False,
                            error=f"API key does not have access to vision model {os.getenv('OPENROUTER_VLM_MODEL')}"
                        )
                    logger.error(f"Vision model error: {str(vision_error)}")
                    return MarkdownResponse(
                        success=False,
                        error=f"Error using vision model: {str(vision_error)}"
                    )
            else:
                # Use default model for non-image files
                temp_md = MarkItDown(
                    llm_client=openai_client,
                    llm_model=os.getenv("OPENROUTER_MODEL")
                )
                result = temp_md.convert(temp_file_path, use_llm=True)
            
            markdown_content = result.text_content
            if not markdown_content:
                raise Exception("No markdown content generated")
            
            logger.info(f"Successfully converted file. Output length: {len(markdown_content)}")
            
            # Clean up temporary file
            os.remove(temp_file_path)
            logger.info("Cleaned up temporary file")
            
            return MarkdownResponse(
                success=True,
                markdown=markdown_content
            )
            
        except Exception as e:
            logger.error(f"Error converting file: {str(e)}")
            # Fallback to direct text conversion if markdown conversion fails
            try:
                text_content = decoded_content.decode('utf-8')
                logger.info("Fallback: Using direct text conversion")
                return MarkdownResponse(
                    success=True,
                    markdown=text_content
                )
            except:
                error_msg = f"Failed to process file {request.file['name']}: {str(e)}"
                logger.error(error_msg)
                return MarkdownResponse(
                    success=False,
                    error=error_msg
                )
        
    except Exception as e:
        error_msg = f"Error processing request: {str(e)}"
        logger.error(error_msg)
        return MarkdownResponse(
            success=False,
            error=error_msg
        )

@app.post("/api/file-agent-cached")
async def process_files_cached(
    request: Request,
    query: str = "",
    files: List[Dict[str, Any]] = [],
    session_id: str = "",
    user_id: str = "",
    request_id: str = "",
    use_cache: bool = True,
    authenticated: bool = Depends(verify_token)
):
    """Process files with AI agent, utilizing cached markdown when available."""
    try:
        # Validate input
        if not files:
            return {
                "success": False,
                "error": "No files provided",
                "markdown": ""
            }

        # Process each file
        results = []
        for file_data in files:
            try:
                # Extract file info
                name = file_data.get('name', '')
                file_type = file_data.get('type', '')
                base64_content = file_data.get('base64', '')
                model = file_data.get('model', os.getenv("OPENROUTER_MODEL"))
                
                if not all([name, file_type, base64_content]):
                    continue

                # Process the file
                result = await process_file_cached(
                    name=name,
                    file_type=file_type,
                    base64_content=base64_content,
                    model=model,
                    use_cache=use_cache
                )
                
                if result:
                    results.append(result)
                    
            except Exception as e:
                logger.error(f"Error processing file {name}: {str(e)}")
                continue
        
        # Handle case where no files were successfully processed
        if not results:
            return {
                "success": False,
                "error": "No valid files were processed",
                "markdown": ""
            }
        
        # Store conversation messages
        try:
            await store_message(
                session_id=session_id,
                message_type="user",
                content=query if query else "Process files",
                data={"files": [f["name"] for f in files]}
            )
            
            await store_message(
                session_id=session_id,
                message_type="assistant",
                content="\n\n".join(results),
                data={"files": [f["name"] for f in files]}
            )
        except Exception as e:
            logger.error(f"Error storing messages: {str(e)}")
            # Continue even if message storage fails
        
        return {
            "success": True,
            "markdown": "\n\n".join(results)
        }
        
    except Exception as e:
        logger.error(f"Error in process_files_cached: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "markdown": ""
        }

if __name__ == "__main__":
    import uvicorn
    # Feel free to change the port here if you need
    uvicorn.run(app, host="0.0.0.0", port=8001)
</file>

<file path="README.md">
# File Processing Agent for autogen_studio

Author: [Loic Baconnier](https://deeplearning.fr/)

This is a specialized Python FastAPI agent that demonstrates how to handle file uploads in the autogen_studio. It shows how to process, store, and leverage file content in conversations with AI models.

This agent builds upon the foundation laid out in [`~sample-python-agent~/sample_supabase_agent.py`](../~sample-python-agent~/sample_supabase_agent.py), extending it with file handling capabilities.

Not all agents need file handling which is why the sample Python agent is kept simple and this one is available to help you build agents with file handling capabilities. The autogen_studio has file uploading built in and the files are sent in the exact format shown in this agent.

## Overview

This agent extends the base Python agent template to showcase file handling capabilities:
- Process uploaded files in base64 format
- Store file content with conversation history
- Integrate file content into AI model context
- Maintain conversation continuity with file references
- Handle multiple files in a single conversation

## Prerequisites

- Python 3.11 or higher
- pip (Python package manager)
- Supabase account (for conversation storage)
- OpenRouter API key (for LLM access)
- Basic understanding of:
  - FastAPI and async Python
  - Base64 encoding/decoding
  - OpenRouter API
  - Supabase

## Core Components

### 1. File Processing

The agent includes robust file handling:
- Base64 decoding of uploaded files
- Text extraction and formatting using MarkItDown
- Persistent storage of file data in Supabase
- Document caching for faster subsequent queries

### 2. Conversation Management

Built on the sample Supabase agent template, this agent adds:
- File metadata storage with messages
- File content integration in conversation history
- Contextual file reference handling
- Document caching for improved performance

### 3. AI Integration

Seamless integration with OpenRouter's LLM models:
- File content as conversation context
- Maintained context across multiple messages
- Intelligent responses based on file content
- Efficient caching of document conversions

## API Routes

### 1. File to Markdown Conversion
```bash
POST /api/convert-to-markdown
```
Converts a single file to markdown format. Simple and direct conversion.

Request:
```json
{
    "file": {
        "name": "document.pdf",
        "type": "application/pdf",
        "base64": "base64_encoded_content"
    }
}
```

Response:
```json
{
    "success": true,
    "markdown": "# Converted Content\n\nYour markdown content here...",
    "error": ""
}
```

### 2. AI Agent Processing
```bash
POST /api/file-agent
```
Processes files with an AI agent, providing enhanced responses and analysis. Supports both text documents and images.

Request:
```json
{
    "query": "Please analyze this document and summarize key points",
    "files": [{
        "name": "document.pdf",
        "type": "application/pdf",
        "base64": "base64_encoded_content"
    }],
    "session_id": "unique_session_id",
    "user_id": "user_123",
    "request_id": "request_456"
}
```

Response:
```json
{
    "success": true,
    "markdown": "Analysis and summary in markdown format...",
    "error": ""
}
```

### 3. Cached File Agent
```bash
POST /api/file-agent-cached
```
Same as `/api/file-agent` but with document caching for improved performance.

### Image Processing

The agent now supports image processing capabilities:
- Automatic image type detection using `imghdr`
- Integration with OpenRouter's Vision Language Models (VLM)
- Image description and analysis in markdown format
- Contextual understanding of images based on queries

For image processing to work, make sure to set the `OPENROUTER_VLM_MODEL` environment variable to a compatible vision model (e.g., `meta-llama/llama-3.2-11b-vision-instruct`).

Example image processing request:
```json
{
    "query": "Describe this image",
    "files": [{
        "name": "photo.jpg",
        "type": "image/jpeg",
        "base64": "base64_encoded_content"
    }],
    "session_id": "unique_session_id",
    "user_id": "user_123",
    "request_id": "request_456"
}
```

| Field | Type | Description |
|-------|------|-------------|
| query | string | Instructions for the AI agent |
| files | array | List of files to process |
| files[].name | string | Original filename with extension |
| files[].type | string | MIME type of the file |
| files[].base64 | string | Base64-encoded file content |
| session_id | string | Unique session identifier for conversation context |
| user_id | string | User identifier |
| request_id | string | Unique request identifier |

Response:
```json
{
    "success": true,
    "markdown": "Image description and analysis in markdown format..."
}
```

Features:
- Stores converted markdown in Supabase for reuse
- Faster responses for subsequent queries on the same document
- Updates last_accessed timestamp for cache management
- Option to bypass cache with use_cache=false

Example curl:
```bash
curl -X POST http://localhost:8001/api/file-agent-cached \
  -H "Authorization: Bearer your_token_here" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the main points?",
    "files": [{
      "name": "document.pdf",
      "type": "application/pdf",
      "base64": "'$(base64 -i document.pdf)'"
    }],
    "session_id": "session_123",
    "user_id": "user_456",
    "request_id": "req_789",
    "use_cache": true
  }'
```

## Detailed API Documentation

### Authentication
All endpoints require Bearer token authentication:
```http
Authorization: Bearer your_token_here
```

### 1. File to Markdown Conversion
```http
POST /api/convert-to-markdown
```

#### Request Body
```json
{
    "file": {
        "name": "example.pdf",
        "type": "application/pdf",
        "base64": "base64_encoded_content"
    }
}
```

| Field | Type | Description |
|-------|------|-------------|
| file.name | string | Original filename with extension |
| file.type | string | MIME type of the file |
| file.base64 | string | Base64-encoded file content |

#### Response
```json
{
    "success": true,
    "markdown": "# Converted Content\n\nMarkdown content here...",
    "error": ""
}
```

| Field | Type | Description |
|-------|------|-------------|
| success | boolean | Whether the conversion was successful |
| markdown | string | Converted markdown content |
| error | string | Error message if conversion failed |

#### Supported File Types
- Documents: `.pdf`, `.docx`, `.txt`
- Spreadsheets: `.xlsx`, `.csv`
- Presentations: `.pptx`
- Web: `.html`
- Images: `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`, `.tiff`

### 2. AI Agent File Processing
```http
POST /api/file-agent
```

#### Request Body
```json
{
    "query": "Please analyze this document and extract key information",
    "files": [{
        "name": "document.pdf",
        "type": "application/pdf",
        "base64": "base64_encoded_content"
    }],
    "session_id": "session_123",
    "user_id": "user_456",
    "request_id": "req_789"
}
```

| Field | Type | Description |
|-------|------|-------------|
| query | string | Instructions for the AI agent |
| files | array | List of files to process |
| files[].name | string | Original filename with extension |
| files[].type | string | MIME type of the file |
| files[].base64 | string | Base64-encoded file content |
| session_id | string | Unique session identifier for conversation context |
| user_id | string | User identifier |
| request_id | string | Unique request identifier |

#### Response
```json
{
    "success": true,
    "markdown": "# Analysis Results\n\nAI-generated analysis..."
}
```

| Field | Type | Description |
|-------|------|-------------|
| success | boolean | Whether the processing was successful |
| markdown | string | AI-generated response in markdown format |

#### Features
- Maintains conversation context using session_id
- Supports multiple files in a single request
- Converts files to markdown before AI processing
- Uses OpenRouter's LLM models for enhanced analysis
- Stores conversation history in Supabase

### Error Responses
Both endpoints return similar error structures:

```json
{
    "success": false,
    "error": "Detailed error message",
    "markdown": ""
}
```

Common HTTP Status Codes:
- 200: Success
- 400: Bad Request (invalid input)
- 401: Unauthorized (invalid token)
- 500: Internal Server Error

### Rate Limiting
- Maximum file size: 10MB per file
- Maximum files per request: 5
- Maximum requests per minute: 60

## Error Handling

The API implements robust error handling:

1. Missing Files
- Returns 422 Unprocessable Entity when no files are provided
- Clear error message indicating "No files provided"

2. File Processing Errors
- Graceful handling of conversion failures
- Detailed error messages for debugging
- Continues processing remaining files if one fails

3. Cache Management
- Handles missing cache entries gracefully
- Falls back to fresh conversion if cache retrieval fails
- Updates cache timestamps on successful access

4. Schema Validation
- Automatic validation of request parameters
- Clear error messages for missing required fields
- Type checking of input values

## Database Setup

The application requires two main tables in Supabase:

1. Messages Table
```sql
create table messages (
    id uuid default uuid_generate_v4() primary key,
    session_id text,
    message_type text,
    content text,
    data jsonb default '{}'::jsonb,
    created_at timestamp with time zone default timezone('utc'::text, now())
);
```

2. Document Cache Table
```sql
create table document_cache (
    doc_hash text primary key,
    file_name text,
    file_type text,
    markdown_content text,
    created_at timestamp with time zone default timezone('utc'::text, now()),
    last_accessed timestamp with time zone default timezone('utc'::text, now())
);
```

## Environment Variables

Create a `.env` file with the following variables:
```bash
OPENROUTER_API_KEY=your_openrouter_api_key
OPENROUTER_MODEL=your_default_model
OPENROUTER_VLM_MODEL=your_vision_model  # Required for image processing
API_BEARER_TOKEN=your_api_token
SUPABASE_URL=your_supabase_project_url
SUPABASE_KEY=your_supabase_key
```

## Docker Setup

Build and run the container:
```bash
docker build -t file-agent .
docker run -p 8001:8001 --env-file .env file-agent
```

## Development Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/ottomarkdown.git
cd ottomarkdown
```

2. Create virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables:
```bash
cp .env.example .env
# Edit .env with your credentials
```

5. Create required directories:
```bash
mkdir -p test_files markdown_results
```

6. Run the application:
```bash
uvicorn file_agent:app --reload --port 8001
```

## Testing

1. Add test files to `test_files/` directory

2. Run conversion tests:
```bash
python test_markdown.py
```

3. Test the API with curl:
```bash
# Test markdown conversion
curl -X POST http://localhost:8001/api/convert-to-markdown \
  -H "Authorization: Bearer your_token" \
  -H "Content-Type: application/json" \
  -d @test_payload.json

# Test AI agent with caching
curl -X POST http://localhost:8001/api/file-agent-cached \
  -H "Authorization: Bearer your_token" \
  -H "Content-Type: application/json" \
  -d @test_payload.json
```

## Running the Agent

Start the agent with:
```bash
python file_agent.py
```

The agent will be available at `http://localhost:8001`.

## API Usage

Send requests to `/api/file-agent` with:
- `query`: Your question or prompt
- `files`: Array of file objects with:
  - `name`: Filename
  - `type`: MIME type
  - `base64`: Base64-encoded file content

Example request:
```json
{
  "query": "What does this file contain?",
  "files": [{
    "name": "example.txt",
    "type": "text/plain",
    "base64": "VGhpcyBpcyBhIHRlc3QgZmlsZS4="
  }],
  "session_id": "unique-session-id",
  "user_id": "user-id",
  "request_id": "request-id"
}
```

## Curl Examples

### Quick Reference

```bash
# 1. Convert a file to markdown
curl -X POST http://localhost:8001/api/convert-to-markdown \
  -H "Authorization: Bearer your_token_here" \
  -H "Content-Type: application/json" \
  -d '{
    "file": {
      "name": "document.pdf",
      "type": "application/pdf",
      "base64": "'$(base64 -i document.pdf)'"
    }
  }'

# 2. Process with AI agent
curl -X POST http://localhost:8001/api/file-agent \
  -H "Authorization: Bearer your_token_here" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Please analyze this document",
    "files": [{
      "name": "document.pdf",
      "type": "application/pdf",
      "base64": "'$(base64 -i document.pdf)'"
    }],
    "session_id": "session_123",
    "user_id": "user_456",
    "request_id": "req_789"
  }'
```

### Helper Script
Save this as `call_api.sh`:

```bash
#!/bin/bash

TOKEN="your_token_here"
API_URL="http://localhost:8001"

# Function to convert file to markdown
convert_to_markdown() {
    local file_path=$1
    local file_name=$(basename "$file_path")
    local mime_type=$(file --mime-type -b "$file_path")
    
    curl -X POST "$API_URL/api/convert-to-markdown" \
        -H "Authorization: Bearer $TOKEN" \
        -H "Content-Type: application/json" \
        -d '{
            "file": {
                "name": "'$file_name'",
                "type": "'$mime_type'",
                "base64": "'$(base64 -i "$file_path")'"
            }
        }'
}

# Function to process with AI agent
process_with_agent() {
    local file_path=$1
    local query=$2
    local file_name=$(basename "$file_path")
    local mime_type=$(file --mime-type -b "$file_path")
    
    curl -X POST "$API_URL/api/file-agent" \
        -H "Authorization: Bearer $TOKEN" \
        -H "Content-Type: application/json" \
        -d '{
            "query": "'$query'",
            "files": [{
                "name": "'$file_name'",
                "type": "'$mime_type'",
                "base64": "'$(base64 -i "$file_path")'"
            }],
            "session_id": "session_'$(date +%s)'",
            "user_id": "user_test",
            "request_id": "req_'$(date +%s)'"
        }'
}

# Usage examples:
# ./call_api.sh convert document.pdf
# ./call_api.sh process document.pdf "Summarize this document"

case "$1" in
    "convert")
        convert_to_markdown "$2"
        ;;
    "process")
        process_with_agent "$2" "$3"
        ;;
    *)
        echo "Usage:"
        echo "  Convert to markdown: $0 convert <file_path>"
        echo "  Process with agent: $0 process <file_path> \"query\""
        exit 1
        ;;
esac
```

### Example Usage

1. Make the script executable:
```bash
chmod +x call_api.sh
```

2. Convert a file to markdown:
```bash
./call_api.sh convert path/to/document.pdf
```

3. Process a file with AI:
```bash
./call_api.sh process path/to/document.pdf "Analyze this document and summarize key points"
```

4. Process multiple files (raw curl):
```bash
curl -X POST http://localhost:8001/api/file-agent \
  -H "Authorization: Bearer your_token_here" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Compare these documents",
    "files": [
      {
        "name": "document1.pdf",
        "type": "application/pdf",
        "base64": "'$(base64 -i document1.pdf)'"
      },
      {
        "name": "document2.pdf",
        "type": "application/pdf",
        "base64": "'$(base64 -i document2.pdf)'"
      }
    ],
    "session_id": "session_123",
    "user_id": "user_456",
    "request_id": "req_789"
  }'
```

5. Convert HTML (raw curl):
```bash
curl -X POST http://localhost:8001/api/convert-to-markdown \
  -H "Authorization: Bearer your_token_here" \
  -H "Content-Type: application/json" \
  -d '{
    "file": {
      "name": "page.html",
      "type": "text/html",
      "base64": "'$(base64 -i page.html)'"
    }
  }'

```

## Contributing

This agent is part of the oTTomator agents collection. For contributions or issues, please refer to the main repository guidelines.
</file>

<file path="requirements.txt">
fastapi>=0.104.1
uvicorn>=0.24.0
pydantic>=2.5.2
supabase>=2.0.3
python-dotenv>=1.0.0
asyncpg>=0.29.0
openai>=1.3.7
markitdown[all]~=0.1.0a1
</file>

<file path="run_tests.sh">
#!/bin/bash

# Change to project directory
cd /Users/loic/Desktop/ottomarkdown

# Kill any existing Python processes and wait for port to be free
echo "Cleaning up existing processes..."
pkill -f "python3 file_agent.py"
lsof -ti:8001 | xargs kill -9 2>/dev/null
sleep 3

# Clean markdown_results directory
echo "Cleaning markdown_results directory..."
rm -rf markdown_results/*
mkdir -p markdown_results

# Remove .DS_Store files
echo "Removing .DS_Store files..."
find . -name ".DS_Store" -delete

# Activate virtual environment
source ./venv/bin/activate

# Start the API server in the background
echo "Starting API server..."
python3 file_agent.py &
API_PID=$!

# Wait for the server to start
echo "Waiting for server to start..."
sleep 5

# Run all tests
echo "Running tests..."
python3 -c "
import validation_test
import asyncio

async def run_tests():
    validation_test.test_openrouter_api()
    validation_test.test_file_processing()
    await validation_test.test_convert_to_markdown()
    validation_test.test_file_processing_with_llm()
    validation_test.test_image_processing_with_llm()
    validation_test.test_api_file_agent_cached()

asyncio.run(run_tests())
"

# Kill the API server
echo "Cleaning up..."
kill $API_PID 2>/dev/null
pkill -f "python3 file_agent.py"

echo "Done!"
</file>

<file path="validation_test.py">
import os
from openai import OpenAI
import json
from dotenv import load_dotenv, find_dotenv
import base64
from markitdown import MarkItDown
from pathlib import Path
import io
import requests
import mimetypes
import time
import re
import httpx

# Load environment variables from .env file
print("Loading .env file from:", find_dotenv())
load_dotenv(find_dotenv())

# Set environment variables
os.environ["OPENROUTER_API_KEY"] = os.getenv("OPENROUTER_API_KEY")
os.environ["OPENROUTER_MODEL"] = os.getenv("OPENROUTER_MODEL", "mistralai/mistral-7b-instruct")
os.environ["OPENROUTER_VLM_MODEL"] = os.getenv("OPENROUTER_VLM_MODEL", "meta-llama/llama-3.2-11b-vision-instruct:free")

# Print loaded environment variables
print("\nEnvironment variables loaded:")
print(f"OPENROUTER_API_KEY = {os.getenv('OPENROUTER_API_KEY')}")
print(f"OPENROUTER_MODEL = {os.getenv('OPENROUTER_MODEL')}")
print(f"OPENROUTER_VLM_MODEL = {os.getenv('OPENROUTER_VLM_MODEL')}")

# Set up API URL with protocol
API_URL = "http://" + os.getenv("API_URL", "localhost:8001")

# Get API token
API_TOKEN = os.getenv("API_BEARER_TOKEN")
if not API_TOKEN:
    raise ValueError("API_BEARER_TOKEN not set in environment")

# Common headers
HEADERS = {
    "Authorization": f"Bearer {API_TOKEN}"
}

# Get and clean environment variables
api_key = os.getenv("OPENROUTER_API_KEY", "").strip()
model = os.getenv("OPENROUTER_MODEL", "").strip()

print("\nEnvironment variables loaded:")
print("OPENROUTER_API_KEY =", api_key)
print("OPENROUTER_MODEL =", model)

# Remove any duplicated API key
if len(api_key) > 100:  # API keys are typically around 73 characters
    api_key = api_key[:73]  # Take only the first part

if not api_key:
    raise ValueError("OPENROUTER_API_KEY not found in environment variables")
if not model:
    raise ValueError("OPENROUTER_MODEL not found in environment variables")

# Validate API key format
if not api_key.startswith("sk-or-v1-"):
    raise ValueError("Invalid API key format. Should start with 'sk-or-v1-'")

def ensure_dir(directory):
    """Ensure a directory exists, create it if it doesn't"""
    Path(directory).mkdir(parents=True, exist_ok=True)

def test_openrouter_api():
    """Test OpenRouter API connection with Llama vision model."""
    print("\nStarting OpenRouter API test...")
    
    # Load environment variables
    api_key = os.getenv("OPENROUTER_API_KEY")
    model = os.getenv("OPENROUTER_VLM_MODEL")
    
    if not api_key:
        print("Error: OPENROUTER_API_KEY not found in environment variables")
        return
        
    print("Using API key:", api_key)
    print("Using VLM model:", model)
    
    try:
        # Initialize OpenRouter client
        openai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
            default_headers={
                "HTTP-Referer": "http://localhost:8001",
                "X-Title": "MarkItDown Test",
            }
        )
        
        print("\nSending request to OpenRouter API...")
        
        # Simple test prompt
        response = openai_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": "Say hello and confirm you can process images!"}
            ]
        )
        
        print("\nRaw API Response:")
        print(json.dumps(response.model_dump(), indent=2))
        
        print("\nProcessed Response:")
        print("Content:", response.choices[0].message.content)
        print("Model used:", response.model)
        
        print("\nTest completed successfully!")
        
    except Exception as e:
        print("\nError occurred during API test:")
        print("Error type:", type(e).__name__)
        print("Error message:", str(e))

def test_file_processing():
    """Test file processing capabilities without LLM calls"""
    print("\nStarting file processing test...")
    
    # Initialize OpenRouter client with VLM model for images
    openai_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY"),
        default_headers={
            "HTTP-Referer": "http://localhost:8001",
            "X-Title": "MarkItDown Test",
        }
    )
    
    # Initialize MarkItDown with VLM model for images
    md = MarkItDown(
        llm_client=openai_client,
        llm_model=os.getenv("OPENROUTER_VLM_MODEL")
    )
    
    # Test directory path
    test_dir = Path(__file__).parent / "test_files"
    results_dir = Path(__file__).parent / "markdown_results"
    ensure_dir(results_dir)
    
    for file_path in test_dir.glob("*"):
        try:
            print(f"\nProcessing {file_path.name}...")
            result = md.convert(str(file_path))
            
            if result and hasattr(result, 'text_content'):
                print(f"✓ Successfully processed {file_path.name}")
                
                # Create markdown file name
                file_type = file_path.suffix.lower()[1:]
                base_name = file_path.stem
                md_file = results_dir / f"local_convert_{base_name}_{file_type}.md"
                
                # Save markdown content for all files
                with open(md_file, 'w', encoding='utf-8') as f:
                    f.write(result.text_content)
                
                print(f"  Output length: {len(result.text_content)} characters")
                print(f"  Saved markdown to: {md_file}")
            else:
                print(f"✗ Failed to process {file_path.name}")
                print(f"  Result: {result}")
            
        except Exception as e:
            print(f"✗ Error processing {file_path.name}")
            print(f"  Error type: {type(e).__name__}")
            print(f"  Error message: {str(e)}")

def test_file_processing_with_llm():
    """Test processing all files with LLM"""
    print("\nTesting file processing with LLM...")
    
    # Initialize OpenRouter clients
    openai_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY"),
        default_headers={
            "HTTP-Referer": "http://localhost:8001",
            "X-Title": "MarkItDown Test",
        }
    )

    # Initialize MarkItDown with appropriate models
    md_text = MarkItDown(
        llm_client=openai_client,
        llm_model=os.getenv("OPENROUTER_MODEL")
    )
    md_vlm = MarkItDown(
        llm_client=openai_client,
        llm_model=os.getenv("OPENROUTER_VLM_MODEL")
    )
    
    test_dir = Path(__file__).parent / "test_files"
    
    for file_path in test_dir.glob("*"):
        if file_path.name.startswith('.'):
            continue
            
        try:
            print(f"\nProcessing {file_path.name}...")
            
            # Use appropriate model and always set use_llm=True
            if file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif']:
                result = md_vlm.convert(str(file_path), use_llm=True)
            else:
                result = md_text.convert(str(file_path), use_llm=True)
            
            # Save results
            if result and hasattr(result, 'text_content'):
                output_path = f'markdown_results/api_openrouter_{file_path.stem}_{file_path.suffix[1:]}.md'
                with open(output_path, 'w') as f:
                    if hasattr(result, 'title') and result.title:
                        f.write(f"# {result.title}\n\n")
                    f.write(result.text_content)
                
                print(f"Successfully processed {file_path.name}")
                print(f"Output saved to: {output_path}")
                print(f"First 100 characters: {result.text_content[:100]}...")
        except Exception as e:
            print(f"Error processing {file_path.name}: {str(e)}")

def test_image_processing_with_llm():
    """Test processing image files with LLM integration for descriptions."""
    print("\nTesting image processing with LLM...")
    
    # Get all image files from test_files directory
    test_dir = Path(__file__).parent / "test_files"
    image_files = [f for f in test_dir.glob("*") if f.is_file() 
                   and not f.name.startswith('.') 
                   and f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif']]
    
    # Initialize OpenRouter client with VLM model for images
    openai_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY"),
        default_headers={
            "HTTP-Referer": "http://localhost:8001",
            "X-Title": "MarkItDown Test",
        }
    )

    # Initialize MarkItDown with VLM model
    md = MarkItDown(
        llm_client=openai_client,
        llm_model=os.getenv("OPENROUTER_VLM_MODEL")
    )
    
    for image_path in image_files:
        try:
            print(f"\nProcessing image: {image_path.name}")
            result = md.convert(str(image_path), use_llm=True)
            
            # Use consistent naming pattern matching other test functions
            output_path = f'markdown_results/api_openrouter_vision_{image_path.stem}_{image_path.suffix[1:]}.md'
            with open(output_path, 'w') as f:
                f.write(result.text_content)
                
            print(f"Successfully processed {image_path.name}")
            print(f"Output saved to: {output_path}")
            print(f"Generated description: {result.text_content[:200]}...")
            
        except Exception as e:
            print(f"Error processing image: {str(e)}")
            print(f"Error type: {type(e).__name__}")

def test_file_agent_openrouter():
    """Test file agent with OpenRouter LLM using query on markdown output"""
    print("\nTesting file agent with OpenRouter...")
    
    # Initialize OpenRouter client
    openai_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY"),
        default_headers={
            "HTTP-Referer": "http://localhost:8001",
            "X-Title": "MarkItDown Test",
        }
    )
    
    # Initialize MarkItDown with appropriate models
    md_text = MarkItDown(
        llm_client=openai_client,
        llm_model=os.getenv("OPENROUTER_MODEL")
    )
    md_vlm = MarkItDown(
        llm_client=openai_client,
        llm_model=os.getenv("OPENROUTER_VLM_MODEL")
    )
    
    test_dir = Path(__file__).parent / "test_files"
    results_dir = Path(__file__).parent / "markdown_results"
    ensure_dir(results_dir)
    
    for file_path in test_dir.glob("*"):
        if file_path.name.startswith('.'):
            continue
            
        try:
            print(f"\nProcessing {file_path.name}...")
            
            # First get markdown content using appropriate model
            if file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif']:
                markdown_result = md_vlm.convert(str(file_path), use_llm=True)
            else:
                markdown_result = md_text.convert(str(file_path), use_llm=True)
            
            if markdown_result and hasattr(markdown_result, 'text_content'):
                # Now process the markdown with LLM query for summary
                query = "Give me a concise summary of this content in 3-4 sentences."
                response = openai_client.chat.completions.create(
                    model=os.getenv("OPENROUTER_MODEL"),
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that provides clear and concise summaries."},
                        {"role": "user", "content": f"{markdown_result.text_content}\n\n{query}"}
                    ]
                )
                
                # Save results
                output_path = results_dir / f"agent_openrouter_summary_{file_path.stem}_{file_path.suffix[1:]}.md"
                with open(output_path, 'w', encoding='utf-8') as f:
                    if file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif']:
                        f.write(f"![{file_path.stem}](../test_files/{file_path.name})\n\n")
                    f.write("# Original Content\n\n")
                    f.write(markdown_result.text_content)
                    f.write("\n\n# Summary\n\n")
                    f.write(response.choices[0].message.content)
                
                print(f"✓ Successfully processed {file_path.name}")
                print(f"  Saved to: {output_path}")
                
        except Exception as e:
            print(f"✗ Error processing {file_path.name}")
            print(f"  Error type: {type(e).__name__}")
            print(f"  Error message: {str(e)}")

# def test_api_file_agent_cached():
#     print("\nTesting /api/file-agent-cached endpoint...")
    
#     # Get list of test files
#     test_files = os.listdir('test_files')
#     print(f"Processing {len(test_files)} files:")
#     for f in test_files:
#         print(f"- {f}")
#     print()
    
#     # Process one file at a time to avoid timeouts
#     batch_size = 1
#     max_retries = 5
#     content_size_limit = 100000
    
#     for i in range(0, len(test_files), batch_size):
#         batch = test_files[i:i + batch_size]
#         files_data = []
        
#         for file_name in batch:
#             file_path = os.path.join('test_files', file_name)
#             if os.path.getsize(file_path) > content_size_limit:
#                 print(f"⚠️ {file_name} exceeds size limit ({os.path.getsize(file_path)} bytes). Content will be truncated.")
            
#             try:
#                 with open(file_path, 'rb') as f:
#                     content = f.read(content_size_limit)
#                     base64_content = base64.b64encode(content).decode('utf-8')
#                     print(f"✓ Successfully encoded {file_name}")
#                     files_data.append({
#                         'name': file_name,
#                         'base64': base64_content,
#                         'type': os.path.splitext(file_name)[1][1:]
#                     })
#             except Exception as e:
#                 print(f"✗ Failed to encode {file_name}: {str(e)}")
#                 continue
        
#         if not files_data:
#             continue
            
#         # Try the request with retries and exponential backoff
#         success = False
#         for retry in range(max_retries):
#             try:
#                 response = requests.post(
#                     'http://localhost:8001/api/file-agent',
#                     params={
#                         'query': 'summarize',
#                         'session_id': 'test_session_123',
#                         'user_id': 'test_user_123',
#                         'request_id': 'test_request_123',
#                         'use_cache': 'true'
#                     },
#                     json=files_data,
#                     headers={'Authorization': f'Bearer {os.getenv("API_BEARER_TOKEN")}'}
#                 )
                
#                 print(f"\nStatus Code: {response.status_code}")
                
#                 if response.status_code == 200:
#                     success = True
#                     break
#                 else:
#                     print(f"Attempt {retry + 1} failed. Status: {response.status_code}")
#                     if retry < max_retries - 1:
#                         delay = min(30, (2 ** retry) * 5)
#                         print(f"Waiting {delay} seconds before retry...")
#                         time.sleep(delay)
#             except Exception as e:
#                 print(f"Request failed: {str(e)}")
#                 if retry < max_retries - 1:
#                     delay = min(30, (2 ** retry) * 5)
#                     print(f"Waiting {delay} seconds before retry...")
#                     time.sleep(delay)
#     # ... rest of the function ...

async def test_file_agent():
    """Test the /api/file-agent endpoint with various file types."""
    print("\nStarting file processing test...\n")
    
    # Process each file type
    test_files = os.listdir('test_files')
    for file in test_files:
        if file.startswith('.'):
            print(f"Skipping hidden file: {file}")
            continue
            
        print(f"\nProcessing {file}...")
        
        # Read file content
        file_path = os.path.join('test_files', file)
        with open(file_path, 'rb') as f:
            file_content = f.read()
            
        # Convert to base64
        encoded_content = base64.b64encode(file_content).decode('utf-8')
        
        # Create request data
        request_data = {
            "query": "Please process this file and provide insights",
            "user_id": "test_user",
            "request_id": "test_request",
            "session_id": "test_session",
            "files": [{
                "name": file,
                "base64": encoded_content,
                "type": os.path.splitext(file)[1][1:]  # Get file extension without dot
            }]
        }
        
        # First get markdown conversion
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{API_URL}/api/file-agent",
                headers={
                    "Authorization": f"Bearer {os.getenv('API_BEARER_TOKEN')}",
                    "Content-Type": "application/json"
                },
                json=request_data
            )
            
        if response.status_code == 200:
            print(f"✓ Successfully processed {file}")
            result = response.json()
            markdown_content = result.get("markdown", "")
            
            # Save markdown content with original filename
            base_name = os.path.splitext(file)[0]
            output_path = 'markdown_results/api_openrouter_connection_test.md'
            with open(output_path, 'w') as f:
                f.write(f"# OpenRouter API Connection Test\n\n")
                f.write(f"Model: {model}\n")
                f.write(f"Response: {response.choices[0].message.content}")
            markdown_content = result.get("markdown", "")
            
            # Save markdown content with original filename
            base_name = os.path.splitext(file)[0]
            output_file = os.path.join('markdown_results', f"{base_name}.md")
            with open(output_file, 'w') as f:
                f.write(markdown_content)
                    
            print(f"  Output length: {len(markdown_content)} characters")
            print(f"  First 100 characters: {markdown_content[:100]}...")
            print(f"  Saved to: {output_file}")
        else:
            print(f"✗ Failed to process {file}")
            print(f"  Status: {response.status_code}")
            try:
                error_json = response.json()
                if isinstance(error_json, dict):
                    print(f"  Error details: {json.dumps(error_json, indent=2)}")
                    if 'detail' in error_json:
                        print(f"  Validation error: {error_json['detail']}")
                else:
                    print(f"  Error response: {error_json}")
            except json.JSONDecodeError:
                print(f"  Raw response: {response.text}")
            except Exception as e:
                print(f"  Error parsing response: {str(e)}")
                print(f"  Raw response: {response.text}")
            
async def test_convert_to_markdown():
    """Test the /api/convert-to-markdown endpoint with all test files."""
    print("\nTesting /api/convert-to-markdown endpoint...")
    
    test_files = [f for f in os.listdir('test_files') if not f.startswith('.')]
    for file in test_files:
        print(f"\nConverting {file}...")
        
        # Read and encode file
        file_path = os.path.join('test_files', file)
        with open(file_path, 'rb') as f:
            file_content = f.read()
        encoded_content = base64.b64encode(file_content).decode('utf-8')
        
        # Test conversion
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{API_URL}/api/convert-to-markdown",
                headers=HEADERS,
                json={
                    "file": {
                        "name": file,
                        "base64": encoded_content
                    }
                }
            )
        
        if response.status_code == 200:
            print(f"✓ Successfully converted {file}")
            result = response.json()
            markdown_content = result.get("markdown", "")
            
            print(f"  Output length: {len(markdown_content)} characters")
            if markdown_content:
                print(f"  First 100 characters: {markdown_content[:100]}...")
                
                # Save for verification
                clean_name = re.sub(r'[^\w\-_\.]', '_', file)
                base_name = os.path.splitext(clean_name)[0]
                markdown_path = os.path.join('markdown_results', f"{base_name}.md")
                os.makedirs('markdown_results', exist_ok=True)
                with open(markdown_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                print(f"  Saved to: {markdown_path}")
        else:
            print(f"✗ Failed to convert {file}")
            print(f"  Error: {response.text}")

async def test_file_agent_api():
    """Test the /api/file-agent endpoint with all test files."""
    print("\nTesting /api/file-agent endpoint...")
    
    test_files = [f for f in os.listdir('test_files') if not f.startswith('.')]
    for file in test_files:
        print(f"\nProcessing {file} with file-agent...")
        
        # Read and encode file
        file_path = os.path.join('test_files', file)
        with open(file_path, 'rb') as f:
            file_content = f.read()
        encoded_content = base64.b64encode(file_content).decode('utf-8')
        
        # Test without query first
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{API_URL}/api/file-agent",
                headers=HEADERS,
                json={
                    "files": [{
                        "name": file,
                        "base64": encoded_content
                    }]
                }
            )
        
        if response.status_code == 200:
            print(f"✓ Successfully processed {file}")
            result = response.json()
            markdown_content = result.get("markdown", "")
            
            print(f"  Output length: {len(markdown_content)} characters")
            if markdown_content:
                print(f"  First 100 characters: {markdown_content[:100]}...")
                
                # Save markdown result
                clean_name = re.sub(r'[^\w\-_\.]', '_', file)
                base_name = os.path.splitext(clean_name)[0]
                markdown_path = os.path.join('markdown_results', f"{base_name}.md")
                os.makedirs('markdown_results', exist_ok=True)
                with open(markdown_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                print(f"  Saved to: {markdown_path}")
                
                # Now test with a query
                query = "Please provide a concise summary of the following content"
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        f"{API_URL}/api/file-agent",
                        headers=HEADERS,
                        params={"query": query},
                        json={
                            "files": [{
                                "name": f"{base_name}.md",
                                "base64": base64.b64encode(markdown_content.encode()).decode()
                            }]
                        }
                    )
                
                if response.status_code == 200:
                    result = response.json()
                    summary = result.get("markdown", "")
                    
                    # Save summary result
                    summary_path = os.path.join('markdown_results', f"{base_name}_summary.md")
                    with open(summary_path, 'w', encoding='utf-8') as f:
                        f.write(summary)
                    print(f"  Summary saved to: {summary_path}")
                else:
                    print(f"✗ Failed to get summary for {file}")
                    print(f"  Error: {response.text}")
        else:
            print(f"✗ Failed to process {file}")
            print(f"  Error: {response.text}")

async def test_file_agent_cached_api():
    """Test the /api/file-agent-cached endpoint."""
    print("\nTesting /api/file-agent-cached endpoint...")
    
    # Use a test file for cache testing
    test_file = next(f for f in os.listdir('test_files') if f.endswith('.docx'))
    print(f"Using {test_file} for cache testing...")
    
    # Read and encode file
    file_path = os.path.join('test_files', test_file)
    with open(file_path, 'rb') as f:
        file_content = f.read()
    encoded_content = base64.b64encode(file_content).decode('utf-8')
    
    files = [{
        "name": test_file,
        "base64": encoded_content
    }]
    
    # Test 1: First request (should cache)
    print("\nTest 1: First request (should cache)...")
    async with httpx.AsyncClient() as client:
        start_time = time.time()
        response = await client.post(
            f"{API_URL}/api/file-agent-cached",
            headers=HEADERS,
            params={
                "query": "What are the main topics?",
                "session_id": "test_session_123",
                "user_id": "test_user_123",
                "request_id": "test_request_123",
                "use_cache": True
            },
            json={"files": files}
        )
    
    if response.status_code == 200:
        print(f"✓ First request successful")
        print(f"  Time taken: {time.time() - start_time:.2f}s")
        
        # Test 2: Second request (should use cache)
        print("\nTest 2: Second request (should use cache)...")
        async with httpx.AsyncClient() as client:
            start_time = time.time()
            response = await client.post(
                f"{API_URL}/api/file-agent-cached",
                headers=HEADERS,
                params={
                    "query": "Give me a different perspective",
                    "session_id": "test_session_123",
                    "user_id": "test_user_123",
                    "request_id": "test_request_456",
                    "use_cache": True
                },
                json={"files": files}
            )
        
        if response.status_code == 200:
            print(f"✓ Second request successful")
            print(f"  Time taken: {time.time() - start_time:.2f}s")
        else:
            print(f"✗ Second request failed")
            print(f"  Error: {response.text}")
    else:
        print(f"✗ First request failed")
        print(f"  Error: {response.text}")

async def main():
    """Run all tests in sequence."""
    print("\nStarting tests...")
    
    # Run OpenRouter tests
    test_openrouter_api()
    test_file_processing()
    test_file_processing_with_llm()
    test_image_processing_with_llm()
    test_file_agent_openrouter()
    
    print("\nAll tests completed!")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
</file>

</files>
