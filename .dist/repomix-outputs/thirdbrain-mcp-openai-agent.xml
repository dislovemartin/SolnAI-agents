This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
docs/
  mcp_readme.md
  MCPClient_design_diagrams.puml
  MCPClient_sequence_diagram.mmd
  sequence_diagram.md
mcp-tool-description-overrides/
  mcp-pandoc__convert-contents
  mcp-pandoc__convert-contents2
  mcp-pandoc.md
  sequentialthinking2
tests/
  curl.sh
weather-server-python/
  src/
    weather/
      __init__.py
      server.py
wip/
  mcp_openai_client.example.py
  mcp_proxy_pydantic_agent_client.example.py
  mcp-proxy-pydantic-agent.example.pyproject.toml
.dockerignore
.env.example
Dockerfile
exceptions.py
LICENSE
LiveAgentStudio_README.md
mcp_client.py
mcp_config_examples.json
mcp_config.json
README.md
requirements.txt
thirdbrain_mcp_openai_agent.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="docs/mcp_readme.md">
## Sample to show to integrate MCP (Model Context Protocol) servers with Pydantic.AI


Parts of this example uses content from : https://github.com/modelcontextprotocol/quickstart-resources.git - Esp. the weather 'server' code

Code uses two different LLMs just for demonstration. The Proxy Agent uses gpt-4o and the tool uses sonnet. 
So, export OPENAI_API_KEY as well as ANTHROPIC_API_KEY - OR - modify the code to suit your models

The pyproject.toml assumes you are using 'uv' package manager

### Steps to run
1. Clone this repo
1. uv sync
3. cd mc-client
2. uv run client.py

Now, try interacting with some questions like:

> What is the time in NY when it is 7:30pm in Bangalore?

> What is the Weather currently in Chicago?

(and quit when done)
</file>

<file path="docs/MCPClient_design_diagrams.puml">
@startuml
!include <C4/C4_Container>

Person(user, "User", "Interacts with the system")
Container_Boundary(app, "MCP Client Application") {
    Container(mcpClient, "MCPClient", "Python Class", "Manages MCP server connections and tool interactions")
    Container(mcpServer, "MCP Server", "External Service", "Provides tools and services")
    Container(configFile, "mcp_config.json", "Configuration File", "Stores server configurations")
}

Rel(user, mcpClient, "Sends commands and queries")
Rel(mcpClient, mcpServer, "Connects to and interacts with")
Rel(mcpClient, configFile, "Reads and writes configurations")
@enduml
</file>

<file path="docs/MCPClient_sequence_diagram.mmd">
sequenceDiagram
    participant User
    participant MCPClient
    participant MCPServer
    participant Tool

    User->>MCPClient: Send command/query
    MCPClient->>MCPServer: Connect to server
    MCPServer-->>MCPClient: Acknowledge connection
    MCPClient->>MCPServer: Request tool list
    MCPServer-->>MCPClient: Provide tool list
    MCPClient->>Tool: Call tool with arguments
    Tool-->>MCPClient: Return tool result
    MCPClient-->>User: Provide response

flowchart TD
    A[Start] --> B{Is config file present?}
    B -- Yes --> C[Load configuration]
    B -- No --> D[Raise ConfigurationError]
    C --> E{Is server enabled?}
    E -- Yes --> F[Connect to server]
    E -- No --> G[Skip server]
    F --> H[Add tools to available list]
    G --> H
    H --> I[End]
</file>

<file path="docs/sequence_diagram.md">
# Tool Calling Sequence Diagram

This document provides a sequence diagram that illustrates the process of tool calling in the system, highlighting the role of the chatbot in preparing the tool call.

### Updated Sequence Diagram for Tool Calling Sequence

```mermaid
sequenceDiagram
    participant User
    participant Chatbot
    participant MCPClient
    participant MCPServer
    participant Tool

    User->>Chatbot: Send query
    Chatbot->>MCPClient: Process query
    MCPClient->>MCPServer: Connect to server
    MCPServer-->>MCPClient: Acknowledge connection
    MCPClient->>MCPServer: Request tool list
    MCPServer-->>MCPClient: Provide tool list
    Chatbot->>Tool: Prepare tool call with arguments
    Tool-->>Chatbot: Return tool result
    Chatbot-->>User: Provide response
```

### Explanation

- **User Interaction:** The user sends a query to the chatbot.
- **Chatbot Processing:** The chatbot processes the query and interacts with the `MCPClient` to manage server connections and tool interactions.
- **MCPClient and MCPServer Interaction:** The `MCPClient` connects to the MCP server, requests the tool list, and receives the available tools.
- **Tool Preparation:** The chatbot prepares the tool call with the necessary arguments and invokes the tool.
- **Tool Execution:** The tool executes the requested operation and returns the result to the chatbot.
- **Response to User:** The chatbot provides the final response to the user, incorporating the tool's result.
</file>

<file path="mcp-tool-description-overrides/mcp-pandoc__convert-contents">
Converts content between different formats. 
- Basic formats: txt, html, markdown
- Advanced formats (REQUIRE complete file paths): pdf, docx, rst, latex, epub

1. File Paths - EXPLICIT REQUIREMENTS:
   * When asked to save or convert to a file, you MUST provide:
     - Complete directory path
     - Filename
     - File extension
   * Example request: 'Write a story and save as PDF'
   * You MUST specify: '/path/to/story.pdf' or 'C:\Documents\story.pdf'
   * The tool will NOT automatically generate filenames or extensions

2. File Location After Conversion:
   * After successful conversion, the tool will display the exact path where the file is saved
   * Look for message: 'Content successfully converted and saved to: [file_path]'
   * You can find your converted file at the specified location
   * For better control, always provide explicit output file paths

Note: After conversion, always check the success message for the exact file location.
</file>

<file path="mcp-tool-description-overrides/mcp-pandoc__convert-contents2">
Converts content between different formats. Transforms input content from any supported format into the specified output format.
Supported formats:
- Basic formats: txt, html, markdown
- Advanced formats (REQUIRE complete file paths): pdf, docx, rst, latex, epub

CORRECT Usage Examples:
1. 'Convert this text to HTML' (basic conversion)
   - Tool will show converted content

2. 'Save this text as PDF at /documents/story.pdf'
   - Correct: specifies path + filename + extension
   - Tool will show: 'Content successfully converted and saved to: /documents/story.pdf'

INCORRECT Usage:
1. 'Save this as PDF in /documents/'
   -Missing filename and extension
2. 'Convert to PDF'
   - Missing complete file path

When requesting conversion, ALWAYS specify:
1. The content or input file
2. The desired output format
3. For advanced formats: complete output path + filename + extension
Example: 'Convert this markdown to PDF and save as /path/to/output.pdf'

Always check the success message for the exact file location.
</file>

<file path="mcp-tool-description-overrides/mcp-pandoc.md">
Converts content between different formats. Transforms input content from any supported format into the specified output format.

🚨 CRITICAL REQUIREMENTS - PLEASE READ:
1. PDF Conversion:
   * You MUST install TeX Live BEFORE attempting PDF conversion:
   * Ubuntu/Debian: `sudo apt-get install texlive-xetex`
   * macOS: `brew install texlive`
   * Windows: Install MiKTeX or TeX Live from https://miktex.org/ or https://tug.org/texlive/
   * PDF conversion will FAIL without this installation

2. File Paths - EXPLICIT REQUIREMENTS:
   * When asked to save or convert to a file, you MUST provide:
     - Complete directory path
     - Filename
     - File extension
   * Example request: 'Write a story and save as PDF'
   * You MUST specify: '/path/to/story.pdf' or 'C:\Documents\story.pdf'
   * The tool will NOT automatically generate filenames or extensions

3. File Location After Conversion:
   * After successful conversion, the tool will display the exact path where the file is saved
   * Look for message: 'Content successfully converted and saved to: [file_path]'
   * You can find your converted file at the specified location
   * If no path is specified, files may be saved in system temp directory (/tmp/ on Unix systems)
   * For better control, always provide explicit output file paths

Supported formats:
- Basic formats: txt, html, markdown
- Advanced formats (REQUIRE complete file paths): pdf, docx, rst, latex, epub

✅ CORRECT Usage Examples:
1. 'Convert this text to HTML' (basic conversion)
   - Tool will show converted content

2. 'Save this text as PDF at /documents/story.pdf'
   - Correct: specifies path + filename + extension
   - Tool will show: 'Content successfully converted and saved to: /documents/story.pdf'

❌ INCORRECT Usage Examples:
1. 'Save this as PDF in /documents/'
   - Missing filename and extension
2. 'Convert to PDF'
   - Missing complete file path

When requesting conversion, ALWAYS specify:
1. The content or input file
2. The desired output format
3. For advanced formats: complete output path + filename + extension
Example: 'Convert this markdown to PDF and save as /path/to/output.pdf'

Note: After conversion, always check the success message for the exact file location.
</file>

<file path="mcp-tool-description-overrides/sequentialthinking2">
Tool for dynamic and reflective problem-solving through a structured thinking process.

Features
Break down complex problems into manageable steps
Revise and refine thoughts as understanding deepens
Branch into alternative paths of reasoning
Adjust the total number of thoughts dynamically
Generate and verify solution hypotheses

Usage
The Sequential Thinking tool is designed for:

Breaking down complex problems into steps
Planning and design with room for revision
Analysis that might need course correction
Problems where the full scope might not be clear initially
Tasks that need to maintain context over multiple steps
Situations where irrelevant information needs to be filtered out
</file>

<file path="tests/curl.sh">
bearer=$1
query=$2
sess=$3
req=$4
port=8001
user=cb

curl -X POST localhost:$port/api/thirdbrain-mcp-openai-agent \
-H "Authorization: Bearer '$bearer'" \
-H "Content-Type: application/json" \
-d '{
  "query": "'$query'",
  "user_id": "'$user'",
  "request_id": "R123_'$req'",
  "session_id": "'$sess'",
  "files": []
}'
</file>

<file path="weather-server-python/src/weather/__init__.py">
from . import server
import asyncio

def main():
    """Main entry point for the package."""
    asyncio.run(server.main())

# Optionally expose other important items at package level
__all__ = ['main', 'server']
</file>

<file path="weather-server-python/src/weather/server.py">
from typing import Any

import httpx
from mcp.server.models import InitializationOptions
import mcp.types as types
from mcp.server import NotificationOptions, Server
import mcp.server.stdio
import asyncio
NWS_API_BASE = "https://api.weather.gov"
USER_AGENT = "weather-app/1.0"

server = Server("weather")
@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """
    List available tools.
    Each tool specifies its arguments using JSON Schema validation.
    """
    return [
        types.Tool(
            name="get-alerts",
            description="Get weather alerts for a state",
            inputSchema={
                "type": "object",
                "properties": {
                    "state": {
                        "type": "string",
                        "description": "Two-letter state code (e.g. CA, NY)",
                    },
                },
                "required": ["state"],
            },
        ),
        types.Tool(
            name="get-forecast",
            description="Get weather forecast for a location",
            inputSchema={
                "type": "object",
                "properties": {
                    "latitude": {
                        "type": "number",
                        "description": "Latitude of the location",
                    },
                    "longitude": {
                        "type": "number",
                        "description": "Longitude of the location",
                    },
                },
                "required": ["latitude", "longitude"],
            },
        ),
    ]

async def make_nws_request(client: httpx.AsyncClient, url: str) -> dict[str, Any] | None:
    """Make a request to the NWS API with proper error handling."""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/geo+json"
    }

    try:
        response = await client.get(url, headers=headers, timeout=30.0)
        response.raise_for_status()
        return response.json()
    except Exception:
        return None

def format_alert(feature: dict) -> str:
    """Format an alert feature into a concise string."""
    props = feature["properties"]
    return (
        f"Event: {props.get('event', 'Unknown')}\n"
        f"Area: {props.get('areaDesc', 'Unknown')}\n"
        f"Severity: {props.get('severity', 'Unknown')}\n"
        f"Status: {props.get('status', 'Unknown')}\n"
        f"Headline: {props.get('headline', 'No headline')}\n"
        "---"
    )
@server.call_tool()
async def handle_call_tool(
    name: str, arguments: dict | None
) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:
    """
    Handle tool execution requests.
    Tools can fetch weather data and notify clients of changes.
    """
    if not arguments:
        raise ValueError("Missing arguments")
  
    if name == "get-alerts":
        state = arguments.get("state")
        if not state:
            raise ValueError("Missing state parameter")

        # Convert state to uppercase to ensure consistent format
        state = state.upper()
        if len(state) != 2:
            raise ValueError("State must be a two-letter code (e.g. CA, NY)")

        async with httpx.AsyncClient() as client:
            alerts_url = f"{NWS_API_BASE}/alerts?area={state}"
            alerts_data = await make_nws_request(client, alerts_url)

            if not alerts_data:
                return [types.TextContent(type="text", text="Failed to retrieve alerts data")]

            features = alerts_data.get("features", [])
            if not features:
                return [types.TextContent(type="text", text=f"No active alerts for {state}")]

            # Format each alert into a concise string
            formatted_alerts = [format_alert(feature) for feature in features]
            alerts_text = f"Active alerts for {state}:\n\n" + "\n".join(formatted_alerts)

            return [
                types.TextContent(
                    type="text",
                    text=alerts_text
                )
            ]
    elif name == "get-forecast":
        try:
            latitude = float(arguments.get("latitude"))
            longitude = float(arguments.get("longitude"))
        except (TypeError, ValueError):
            return [types.TextContent(
                type="text",
                text="Invalid coordinates. Please provide valid numbers for latitude and longitude."
            )]
            
        # Basic coordinate validation
        if not (-90 <= latitude <= 90) or not (-180 <= longitude <= 180):
            return [types.TextContent(
                type="text",
                text="Invalid coordinates. Latitude must be between -90 and 90, longitude between -180 and 180."
            )]

        async with httpx.AsyncClient() as client:
            # First get the grid point
            lat_str = f"{latitude}"
            lon_str = f"{longitude}"
            points_url = f"{NWS_API_BASE}/points/{lat_str},{lon_str}"
            points_data = await make_nws_request(client, points_url)

            if not points_data:
                return [types.TextContent(type="text", text=f"Failed to retrieve grid point data for coordinates: {latitude}, {longitude}. This location may not be supported by the NWS API (only US locations are supported).")]

            # Extract forecast URL from the response
            properties = points_data.get("properties", {})
            forecast_url = properties.get("forecast")
            
            if not forecast_url:
                return [types.TextContent(type="text", text="Failed to get forecast URL from grid point data")]

            # Get the forecast
            forecast_data = await make_nws_request(client, forecast_url)
            
            if not forecast_data:
                return [types.TextContent(type="text", text="Failed to retrieve forecast data")]

            # Format the forecast periods
            periods = forecast_data.get("properties", {}).get("periods", [])
            if not periods:
                return [types.TextContent(type="text", text="No forecast periods available")]

            # Format each period into a concise string
            formatted_forecast = []
            for period in periods:
                forecast_text = (
                    f"{period.get('name', 'Unknown')}:\n"
                    f"Temperature: {period.get('temperature', 'Unknown')}°{period.get('temperatureUnit', 'F')}\n"
                    f"Wind: {period.get('windSpeed', 'Unknown')} {period.get('windDirection', '')}\n"
                    f"{period.get('shortForecast', 'No forecast available')}\n"
                    "---"
                )
                formatted_forecast.append(forecast_text)

            forecast_text = f"Forecast for {latitude}, {longitude}:\n\n" + "\n".join(formatted_forecast)

            return [types.TextContent(
                type="text",
                text=forecast_text
            )]
    else:
        raise ValueError(f"Unknown tool: {name}")
async def main():
    # Run the server using stdin/stdout streams
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="weather",
                server_version="0.1.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="wip/mcp_openai_client.example.py">
import json
from huggingface_hub import get_token
from openai import AsyncOpenAI
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from typing import Any, List
import asyncio
 
MODEL_ID = "meta-llama/Llama-3.3-70B-Instruct"
 
# System prompt that guides the LLM's behavior and capabilities
# This helps the model understand its role and available tools
SYSTEM_PROMPT = """You are a helpful assistant capable of accessing external functions and engaging in casual chat. Use the responses from these function calls to provide accurate and informative answers. The answers should be natural and hide the fact that you are using tools to access real-time information. Guide the user about available tools and their capabilities. Always utilize tools to access real-time information when required. Engage in a friendly manner to enhance the chat experience.
 
# Tools
 
{tools}
 
# Notes 
 
- Ensure responses are based on the latest information available from function calls.
- Maintain an engaging, supportive, and friendly tone throughout the dialogue.
- Always highlight the potential of available tools to assist users comprehensively."""
 
 
# Initialize OpenAI client with HuggingFace inference API
# This allows us to use Llama models through HuggingFace's API
client = AsyncOpenAI(
    base_url="https://api-inference.huggingface.co/v1/",
    api_key=get_token(),
)
 
 
class MCPClient:
    """
    A client class for interacting with the MCP (Model Control Protocol) server.
    This class manages the connection and communication with the SQLite database through MCP.
    """
 
    def __init__(self, server_params: StdioServerParameters):
        """Initialize the MCP client with server parameters"""
        self.server_params = server_params
        self.session = None
        self._client = None
 
    async def __aenter__(self):
        """Async context manager entry"""
        await self.connect()
        return self
 
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.__aexit__(exc_type, exc_val, exc_tb)
        if self._client:
            await self._client.__aexit__(exc_type, exc_val, exc_tb)
 
    async def connect(self):
        """Establishes connection to MCP server"""
        self._client = stdio_client(self.server_params)
        self.read, self.write = await self._client.__aenter__()
        session = ClientSession(self.read, self.write)
        self.session = await session.__aenter__()
        await self.session.initialize()
 
    async def get_available_tools(self) -> List[Any]:
        """
        Retrieve a list of available tools from the MCP server.
        """
        if not self.session:
            raise RuntimeError("Not connected to MCP server")
 
        tools = await self.session.list_tools()
        _, tools_list = tools
        _, tools_list = tools_list
        return tools_list
 
    def call_tool(self, tool_name: str) -> Any:
        """
        Create a callable function for a specific tool.
        This allows us to execute database operations through the MCP server.
 
        Args:
            tool_name: The name of the tool to create a callable for
 
        Returns:
            A callable async function that executes the specified tool
        """
        if not self.session:
            raise RuntimeError("Not connected to MCP server")
 
        async def callable(*args, **kwargs):
            response = await self.session.call_tool(tool_name, arguments=kwargs)
            return response.content[0].text
 
        return callable
 
 
async def agent_loop(query: str, tools: dict, messages: List[dict] = None):
    """
    Main interaction loop that processes user queries using the LLM and available tools.
 
    This function:
    1. Sends the user query to the LLM with context about available tools
    2. Processes the LLM's response, including any tool calls
    3. Returns the final response to the user
 
    Args:
        query: User's input question or command
        tools: Dictionary of available database tools and their schemas
        messages: List of messages to pass to the LLM, defaults to None
    """
    messages = (
        [
            {
                "role": "system",
                "content": SYSTEM_PROMPT.format(
                    tools="\n- ".join(
                        [
                            f"{t['name']}: {t['schema']['function']['description']}"
                            for t in tools.values()
                        ]
                    )
                ),  # Creates System prompt based on available MCP server tools
            },
        ]
        if messages is None
        else messages  # reuse existing messages if provided
    )
    # add user query to the messages list
    messages.append({"role": "user", "content": query})
 
    # Query LLM with the system prompt, user query, and available tools
    first_response = await client.chat.completions.create(
        model=MODEL_ID,
        messages=messages,
        tools=([t["schema"] for t in tools.values()] if len(tools) > 0 else None),
        max_tokens=4096,
        temperature=0,
    )
    # detect how the LLM call was completed:
    # tool_calls: if the LLM used a tool
    # stop: If the LLM generated a general response, e.g. "Hello, how can I help you today?"
    stop_reason = (
        "tool_calls"
        if first_response.choices[0].message.tool_calls is not None
        else first_response.choices[0].finish_reason
    )
 
    if stop_reason == "tool_calls":
        # Extract tool use details from response
        for tool_call in first_response.choices[0].message.tool_calls:
            arguments = (
                json.loads(tool_call.function.arguments)
                if isinstance(tool_call.function.arguments, str)
                else tool_call.function.arguments
            )
            # Call the tool with the arguments using our callable initialized in the tools dict
            tool_result = await tools[tool_call.function.name]["callable"](**arguments)
            # Add the tool result to the messages list
            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": tool_call.function.name,
                    "content": json.dumps(tool_result),
                }
            )
 
        # Query LLM with the user query and the tool results
        new_response = await client.chat.completions.create(
            model=MODEL_ID,
            messages=messages,
        )
 
    elif stop_reason == "stop":
        # If the LLM stopped on its own, use the first response
        new_response = first_response
 
    else:
        raise ValueError(f"Unknown stop reason: {stop_reason}")
 
    # Add the LLM response to the messages list
    messages.append(
        {"role": "assistant", "content": new_response.choices[0].message.content}
    )
 
    # Return the LLM response and messages
    return new_response.choices[0].message.content, messages
 
 
async def main():
    """
    Main function that sets up the MCP server, initializes tools, and runs the interactive loop.
    The server is run in a Docker container to ensure isolation and consistency.
    """
    # Configure Docker-based MCP server for SQLite
    server_params = StdioServerParameters(
        command="docker",
        args=[
            "run",
            "--rm",  # Remove container after exit
            "-i",  # Interactive mode
            "-v",  # Mount volume
            "mcp-test:/mcp",  # Map local volume to container path
            "mcp/sqlite",  # Use SQLite MCP image
            "--db-path",
            "/mcp/test.db",  # Database file path inside container
        ],
        env=None,
    )
 
    # Start MCP client and create interactive session
    async with MCPClient(server_params) as mcp_client:
        # Get available database tools and prepare them for the LLM
        mcp_tools = await mcp_client.get_available_tools()
        # Convert MCP tools into a format the LLM can understand and use
        tools = {
            tool.name: {
                "name": tool.name,
                "callable": mcp_client.call_tool(
                    tool.name
                ),  # returns a callable function for the rpc call
                "schema": {
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": tool.inputSchema,
                    },
                },
            }
            for tool in mcp_tools
            if tool.name
            != "list_tables"  # Excludes list_tables tool as it has an incorrect schema
        }
 
        # Start interactive prompt loop for user queries
        messages = None
        while True:
            try:
                # Get user input and check for exit commands
                user_input = input("\nEnter your prompt (or 'quit' to exit): ")
                if user_input.lower() in ["quit", "exit", "q"]:
                    break
 
                # Process the prompt and run agent loop
                response, messages = await agent_loop(user_input, tools, messages)
                print("\nResponse:", response)
                # print("\nMessages:", messages)
            except KeyboardInterrupt:
                print("\nExiting...")
                break
            except Exception as e:
                print(f"\nError occurred: {e}")
 
 
if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="wip/mcp_proxy_pydantic_agent_client.example.py">
import asyncio
import json
from typing import Optional, Dict, List, Union, Any
from contextlib import AsyncExitStack
from colorama import init, Fore, Style

from mcp import ClientSession, StdioServerParameters
init(autoreset=True)  # Initialize colorama with autoreset=True
from pydantic import BaseModel
from pydantic_ai import Agent, RunContext
from pydantic_ai.tools import Tool, ToolDefinition
from mcp.client.stdio import stdio_client

from anthropic import Anthropic
from dotenv import load_dotenv

load_dotenv()  # load environment variables from .env

class MCPClient:
    def __init__(self):
        # Initialize sessions and agents dictionaries
        self.sessions: Dict[str, ClientSession] = {}  # Dictionary to store {server_name: session}
        self.agents: Dict[str, Agent] = {}  # Dictionary to store {server_name: agent}
        self.exit_stack = AsyncExitStack()
        self.anthropic = Anthropic()
        self.available_tools = []  # List to store all available tools across servers
        self.dynamic_tools: List[Tool] = []  # List to store dynamic pydantic tools

    async def connect_to_server(self):
        """Connect to an MCP server using config.json settings"""
        print("\nLoading config.json...")
        with open('config.json') as f:
            config = json.load(f)
        
        print("\nAvailable servers in config:", list(config['mcpServers'].keys()))
        print("\nFull config content:", json.dumps(config, indent=2))
        
        # Connect to all servers in config
        for server_name, server_config in config['mcpServers'].items():
            print(f"\nAttempting to load {server_name} server config...")
            print("Server config found:", json.dumps(server_config, indent=2))
            
            server_params = StdioServerParameters(
                command=server_config['command'],
                args=server_config['args'],
                env=None
            )
            print("\nCreated server parameters:", server_params)
            
            stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
            stdio, write = stdio_transport
            session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))
            
            await session.initialize()
            
            # Store session with server name as key
            self.sessions[server_name] = session
            
            # Create and store an Agent for this server
            server_agent: Agent = Agent(
                'openai:gpt-4',
                system_prompt=(
                    f"You are an AI assistant that helps interact with the {server_name} server. "
                    "You will use the available tools to process requests and provide responses."
                )
            )
            self.agents[server_name] = server_agent
            
            # List available tools for this server
            response = await session.list_tools()
            server_tools = [{
                "name": f"{server_name}__{tool.name}",
                "description": tool.description,
                "input_schema": tool.inputSchema
            } for tool in response.tools]
            
            # Add server's tools to overall available tools
            self.available_tools.extend(server_tools)

            # Create corresponding dynamic pydantic tools
            for tool in response.tools:
                async def prepare_tool(
                    ctx: RunContext[str], 
                    tool_def: ToolDefinition,
                    tool_name: str = tool.name,
                    server: str = server_name
                ) -> Union[ToolDefinition, None]:
                    # Customize tool definition based on server context
                    tool_def.name = f"{server}__{tool_name}"
                    tool_def.description = f"Tool from {server} server: {tool.description}"
                    return tool_def

                async def tool_func(ctx: RunContext[Any], str_arg) -> str:
                    agent_response = await server_agent.run_sync(str_arg)
                    print(f"\nServer agent response: {agent_response}")
                    return f"Tool {tool.name} called with {str_arg}. Agent response: {agent_response}"

                dynamic_tool = Tool(
                    tool_func,
                    prepare=prepare_tool,
                    name=f"{server_name}__{tool.name}",
                    description=tool.description
                )
                self.dynamic_tools.append(dynamic_tool)
                print(f"\nAdded dynamic tool: {dynamic_tool.name}")
                print(f"Description: {dynamic_tool.description}")
                print(f"Function: {dynamic_tool.function}")
                print(f"Prepare function: {dynamic_tool.prepare}")
            
            print(f"\nConnected to server {server_name} with tools:", 
                  [tool["name"] for tool in server_tools])

    async def process_query(self, query: str) -> str:
        """Process a query using Claude and available tools"""
        messages = [
            {
               # Create a function that matches the tool's schema and uses server_agent
                 "role": "user",
                "content": query
            }
        ]

        # Initial Claude API call with all available tools
        response = self.anthropic.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            messages=messages,
            tools=self.available_tools
        )

        # Process response and handle tool calls
        tool_results = []
        final_text = []

        for content in response.content:
            if content.type == 'text':
                final_text.append(content.text)
            elif content.type == 'tool_use':
                # Parse server name and tool name from the full tool name
                full_tool_name = content.name
                server_name, tool_name = full_tool_name.split('__', 1)
                tool_args = content.input
                
                # Get the appropriate session and execute tool call
                if server_name not in self.sessions:
                    raise ValueError(f"Unknown server: {server_name}")
                    
                result = await self.sessions[server_name].call_tool(tool_name, tool_args)
                tool_results.append({"call": tool_name, "result": result})
                final_text.append(f"[Calling tool {tool_name} with args {tool_args}]")

                # Continue conversation with tool results
                if hasattr(content, 'text') and content.text:
                    messages.append({
                      "role": "assistant",
                      "content": content.text
                    })
                messages.append({
                    "role": "user", 
                    "content": result.content
                })

                # Get next response from Claude
                response = self.anthropic.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1000,
                    messages=messages,
                )

                final_text.append(f"{Style.BRIGHT}{Fore.CYAN}{response.content[0].text}")

        return "\n".join(final_text)

    async def chat_loop(self):
        """Run an interactive chat loop"""
        print(f"{Fore.WHITE}\nMCP Client Started!")
        print(f"{Fore.WHITE}Type your queries or 'quit' to exit.")
        
        while True:
            try:
                query = input(f"\n{Fore.RED}Query: {Fore.LIGHTGREEN_EX}").strip()
                
                if query.lower() in ['quit', 'exit', 'bye', 'goodbye']:
                    print("\nGoodbye!")
                    break
                    
                response = await self.process_query(query)
                print(f"\n{Fore.YELLOW}{response}")
                    
            except Exception as e:
                print(f"\n{Fore.RED}Error: {str(e)}")
    
    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()

async def main():
    client = MCPClient()
    try:
        await client.connect_to_server()
        await client.chat_loop()
    finally:
        await client.cleanup()

if __name__ == "__main__":
    import sys
    asyncio.run(main())
</file>

<file path="wip/mcp-proxy-pydantic-agent.example.pyproject.toml">
[project]
name = "mcp-proxy-pydantic-agent"
version = "0.1.0"
description = "Sample to show to integrate MCP (Model Context Protocl) servers with Pydantic.AI"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "anthropic>=0.42.0",
    "colorama>=0.4.6",
    "httpx>=0.28.1",
    "mcp>=1.1.1",
    "mcp-server-time>=0.6.2",
    "pydantic-ai>=0.0.12",
    "python-dotenv>=1.0.1",
]
</file>

<file path=".dockerignore">
__pycache__
.env
venv
</file>

<file path=".env.example">
# For the Supabase version (sample_supabase_agent.py), set your Supabase URL and Service Key.
# Get your SUPABASE_URL from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
SUPABASE_URL=

# Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -
# https://supabase.com/dashboard/project/<your project ID>/settings/api
# On this page it is called the service_role secret.
SUPABASE_SERVICE_KEY=

# Set this bearer token to whatever you want. This will be changed once the agent is hosted for you on the Studio!
API_BEARER_TOKEN=

#### OPENAI ################################################
# Get your Open AI API Key by following these instructions -
# https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key
# You only need this environment variable set if you are using GPT (and not Ollama)
OPENAI_MODEL=gpt-4o-mini
#OPENAI_API_KEY=
OPENAI_URL=https://api.openai.com/v1/

### DEEPSEEK ############################################
# Get your DeepSeek API Key by following these instructions -
# You only need this environment variable set if you are using DeepSeek (and not Ollama or OpenAI)
# do not USE #LLM_MODEL=deepseek-reasoner AS THIS IS NOT SUPPORTED
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_API_KEY=
DEEPSEEK_URL=https://api.deepseek.com/

### OLLAMA ############################################
OLLAMA_MODEL=nezahatkorkmaz/deepseek-v3
OLLAMA_API_KEY=ollama
OLLAMA_URL=https://localhost:11434/v1

###########################################################
# Set this to the openai compatible  model with tool support you want to use.
# Options are:
# - openai
# - deepseek
# - ollama - TODO not tested
SELECTED=DEEPSEEK
</file>

<file path="Dockerfile">
FROM ottomator/base-python:latest

# Build argument for port with default value
ARG PORT=8001
ENV PORT=${PORT}

WORKDIR /app

# Copy requirements first to leverage Docker cache
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application code
COPY . .

# Expose the port from build argument
EXPOSE ${PORT}

# Command to run the application
CMD ["sh", "-c", "uvicorn thirdbrain_mcp_openai_agent:app --host 0.0.0.0 --port ${PORT}"]
</file>

<file path="exceptions.py">
class BaseMCPError(Exception):
    """Base class for all MCP-related errors."""
    pass

class ConfigurationError(BaseMCPError):
    """Raised when there is a configuration-related error."""
    pass

class ConnectionError(BaseMCPError):
    """Raised when there is a connection-related error."""
    pass

class ToolError(BaseMCPError):
    """Raised when there is an error related to tool operations."""
    pass

class DatabaseConnectionError(BaseMCPError):
    """Raised when there is a database connection-related error."""
    pass
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Carine Bruyndoncx

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="LiveAgentStudio_README.md">
# Base Sample Python Agent

Author: [Cole Medin](https://www.youtube.com/@ColeMedin)

This is a sample Python FastAPI agent that demonstrates the minimal required components to build an agent for the autogen_studio. It serves as a template and reference implementation for creating new Python-based agents.

This documentation is long but only because it gives you EVERYTHING you need to build a Python based agent for the autogen_studio! If you spend 5 minutes reading this carefully, you'll be ready to go.

## Overview;;

This agent provides a foundation for building AI-powered agents that can:
- Process natural language queries
- Maintain conversation history
- Integrate with external AI models
- Store and retrieve conversation data
- Handle authentication and security

**Supabase Version** (`sample_supabase_agent.py`): Uses Supabase client for database operations

## Prerequisites

- Python 3.11 or higher
- pip (Python package manager)
- Supabase account
- Basic understanding of:
  - FastAPI and async Python
  - RESTful APIs
  - Pydantic models
  - Environment variables

## Core Components

### 1. FastAPI Application (`sample_supabase_agent.py`)

The main application is built using FastAPI, providing:

- **Authentication**
  - Bearer token validation via environment variables
  - Secure endpoint protection
  - Customizable security middleware

- **Request Handling**
  - Async endpoint processing
  - Structured request validation
  - Error handling and HTTP status codes

- **Database Integration**
  - Supabase connection management
  - Message storage and retrieval
  - Session-based conversation tracking

### 2. Data Models

#### Request Model
```python
class AgentRequest(BaseModel):
    query: str        # The user's input text
    user_id: str      # Unique identifier for the user
    request_id: str   # Unique identifier for this request
    session_id: str   # Current conversation session ID
```

#### Response Model
```python
class AgentResponse(BaseModel):
    success: bool     # Indicates if the request was processed successfully
```

### 3. Database Schema

The agent uses Supabase tables with the following structure:

#### Messages Table
```sql
messages (
    id: uuid primary key
    created_at: timestamp with time zone
    session_id: text
    message: jsonb {
        type: string       # 'human' or 'assistant'
        content: string    # The message content
        data: jsonb       # Optional additional data
    }
)
```

## Setup

   # Copy example environment file
   cp .env.example .env

   # Edit .env with your credentials
   nano .env  # or use your preferred editor

2. **Configure Environment Variables**

   > ⚠️ **Important**: For Docker, do not wrap environment variable values in quotes, even if they contain special characters. Docker will handle the values correctly without quotes.

   #### Supabase Configuration
   Required environment variables in `.env` file (do not use quotes around values):
   ```plaintext
   SUPABASE_URL=your-project-url
   SUPABASE_SERVICE_KEY=your-service-key
   API_BEARER_TOKEN=your-token-here
   ```

3. **Create Database Tables**
   You'll need to create the following tables:
   ```sql
   -- Enable the pgcrypto extension for UUID generation
   CREATE EXTENSION IF NOT EXISTS pgcrypto;

   CREATE TABLE messages (
       id uuid DEFAULT gen_random_uuid() PRIMARY KEY,
       created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
       session_id TEXT NOT NULL,
       message JSONB NOT NULL
   );

   CREATE INDEX idx_messages_session_id ON messages(session_id);
   CREATE INDEX idx_messages_created_at ON messages(created_at);
   ```

   > Note: If you're using Supabase, the `pgcrypto` extension is already enabled by default.

## Installation Methods

### Docker Installation (Recommended)

1. Build the base image first (make sure Docker is running on your machine):
```bash
cd ../base_python_docker
docker build -t ottomator/base-python:latest .
cd ../~sample-python-agent~
```

2. Build the agent image (you can swap between Supabase and PostgreSQL versions in the `Dockerfile`):
```bash
docker build -t sample-python-agent .
```

3. Run the container:
```bash
docker run -d --name sample-python-agent -p 8001:8001 --env-file .env sample-python-agent
```

The agent will be available at `http://localhost:8001`.

### Local Installation (Docker Alternative)

1. Create and activate virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

2. Run the agent:

```bash
uvicorn sample_supabase_agent:app --host 0.0.0.0 --port 8001
```

## Configuration

The agent uses environment variables for configuration. 
You can set these variables in a `.env` file or using your operating system's environment variable settings.

## Making Your First Request

Test your agent using curl or any HTTP client:

### Supabase Version
```bash
curl -X POST http://localhost:8001/api/sample-supabase-agent \
  -H "Authorization: Bearer your-token-here" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Hello, agent!",
    "user_id": "test-user",
    "request_id": "test-request-1",
    "session_id": "test-session-1"
  }'
```

## Building Your Own Agent

1. **Fork the Template**
   - Create a copy of this sample agent
   - Rename files and update paths as needed

2. **Customize the Agent Logic**
   - Locate the TODO section in `sample_supabase_agent.py`
   - Add your custom agent logic:
     ```python
     # Example: Add AI model integration
     async def get_ai_response(query: str, history: List[Dict]) -> str:
         # Add your AI model logic here
         # e.g., OpenAI, Anthropic, or custom models
         return "AI response"

     @app.post("/api/your-agent", response_model=AgentResponse)
     async def your_agent(request: AgentRequest):
         # Get conversation history
         history = await fetch_conversation_history(request.session_id)
         
         # Process with your AI model
         response = await get_ai_response(request.query, history)
         
         # Store the response
         await store_message(
             session_id=request.session_id,
             message_type="assistant",
             content=response
         )
         
         return AgentResponse(success=True)
     ```

3. **Add Dependencies**
   - Update `requirements.txt` with new packages
   - Document any external services or APIs

4. **Implement Error Handling**
   ```python
   try:
       result = await your_operation()
   except YourCustomError as e:
       raise HTTPException(
           status_code=400,
           detail=f"Operation failed: {str(e)}"
       )
   ```

## Troubleshooting - Common issues and solutions:

1. **Authentication Errors**
   - Verify bearer token in environment
   - Check Authorization header format
   - Ensure token matches exactly

2. **Database Connection Issues**
   - For Supabase:
     - Verify Supabase credentials
     - Validate table permissions

3. **Performance Problems**
   - Check database query performance
   - Consider caching frequently accessed data
</file>

<file path="mcp_client.py">
import os
import asyncio
import json
import logging
import pprint
from exceptions import ConfigurationError, ConnectionError, ToolError

from dotenv import load_dotenv
from dataclasses import dataclass
from typing import Optional, Union, Any, Dict, List
from contextlib import AsyncExitStack
from colorama import init, Fore, Style
init(autoreset=True)  # Initialize colorama with autoreset=True

from pydantic import BaseModel
from pydantic_ai import Agent, RunContext 
from pydantic_ai.tools import Tool, ToolDefinition

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from httpx import AsyncClient
from supabase import Client
from openai import AsyncOpenAI
from pydantic_ai.models.openai import OpenAIModel

logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def initialize_client_and_model() -> tuple[AsyncOpenAI, OpenAIModel, str]:
    """
    Load environment variables, resolve provider-specific configuration,
    and return the client and model.

    Returns:
        tuple[AsyncOpenAI, OpenAIModel, str]: A tuple containing the client, model, and language model.

    Raises:
        ConfigurationError: If any required environment variable is missing.
    """

    load_dotenv()  
    selected = os.getenv("SELECTED")

    required_env_vars = {var: os.getenv(f"{selected}_{var}") for var in ["URL", "API_KEY", "MODEL"]}
    
    missing_vars = [var for var, value in required_env_vars.items() if not value]
    if missing_vars:
        raise ConfigurationError(f"Missing environment variables: {', '.join(missing_vars)}")
    
    base_url, api_key, language_model = required_env_vars["URL"], required_env_vars["API_KEY"], required_env_vars["MODEL"]

    client = AsyncOpenAI( 
        base_url=base_url,
        api_key=api_key)
    
    model = OpenAIModel(
        language_model,
        base_url=base_url,
        api_key=api_key)
    
    return client, model, language_model
    
try:
    client, model, language_model = initialize_client_and_model()
    logger.info("Client and model initialized successfully.")
except ConfigurationError as e:
    # Explain what happens with the raise here, what is executed next ?
    logger.error(f"Configuration error: {e}")
    raise
    
except Exception as e:
    logger.exception("Unexpected error during client and model initialization")
    raise

# System prompt that guides the LLM's behavior and capabilities
# This helps the model understand its role and available tools
SYSTEM_PROMPT = """You are a helpful assistant capable of accessing external functions and engaging in casual chat. Use the responses from these function calls to provide accurate and informative answers. The answers should be natural and hide the fact that you are using tools to access real-time information. Guide the user about available tools and their capabilities. Always utilize tools to access real-time information when required. Engage in a friendly manner to enhance the chat experience.
 
# Tools
 
{tools}
 
# Notes 
 
- Ensure responses are based on the latest information available from function calls.
- Maintain an engaging, supportive, and friendly tone throughout the dialogue.
- Always highlight the potential of available tools to assist users comprehensively."""
 
@dataclass
class Deps:
    client: AsyncClient
    supabase: Client
    session_id: str

class MCPClient:
    """
    A client class for interacting with the MCP (Model Control Protocol) server.
    This class manages the connection and communication with the tools through MCP.
    """
    def __init__(self):
        # Initialize sessions and agents dictionaries
        self.sessions: Dict[str, ClientSession] = {}  # Dictionary to store {server_name: session}
        self.agents: Dict[str, Agent] = {}  # Dictionary to store {server_name: agent}
        self.exit_stack = AsyncExitStack()
        self.available_tools = []
        self.tools = {}
        self.connected = False
        self.config_file = 'mcp_config.json'
        self.dynamic_tools: List[Tool] = []  # List to store dynamic pydantic tools

    async def connect_to_server(self) -> None:
        """
        Connect to the MCP server using the configuration file.
        
        Raises:
            ConfigurationError: If the configuration file is missing or invalid.
            ConnectionError: If unable to connect to the MCP server.
        """
        if self.connected:
            logging.info("Already connected to servers.")
            return

        logger.info(f"Loading configuration from {self.config_file}.")
        try:
            with open(self.config_file) as f:
                config = json.load(f)
        except FileNotFoundError:
            raise ConfigurationError(f"{self.config_file} file not found.")
        except json.JSONDecodeError:
            raise ConfigurationError(f"{self.config_file} is not a valid JSON file.")
        
        logger.debug("Available servers in config: %s", list(config['mcpServers'].keys()))
        
        # Connect only to enabled servers in config
        for server_name, server_config in config['mcpServers'].items():
            logger.info(f"Processing server configuration for {server_name}.")
            logger.debug(f"Server configuration details: %s", json.dumps(server_config, indent=2))
            if server_config.get("enable", False):
                logger.debug(f"Attempting to load {server_name} server config.")
                
                server_params = StdioServerParameters(
                    command=server_config['command'],
                    args=server_config['args'],
                    env=server_config.get('env'),
                )
                logger.info("Created server parameters: command=%s, args=%s, env=%s",
                              server_params.command, server_params.args, server_params.env)
               
                try:
                    # Create and store session with server name as key
                    stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
                    stdio, write = stdio_transport
                    session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))
                    await session.initialize()
                    self.sessions[server_name] = session
                    
                    # Create and store an Agent for this server
                    server_agent: Agent = Agent(
                        model,
                        system_prompt=(
                            f"You are an AI assistant that helps interact with the {server_name} server. "
                            "You will use the available tools to process requests and provide responses."
                            "Make sure to always give feedback to the user after you have called the tool, especially when the tool does not generate any message itself."
                        )
                    )
                    self.agents[server_name] = server_agent
                    
                    # List available tools for this server
                    response = await session.list_tools()
                    server_tools = [{
                        "name": f"{server_name}__{tool.name}",
                        "description": tool.description,
                        "input_schema": tool.inputSchema
                    } for tool in response.tools]
                except Exception as e:
                    raise ConnectionError(f"Failed to connect to MCP server {server_name}: {str(e)}")
                
                # Add server's tools to overall available tools
                self.available_tools.extend(server_tools)

                # Create corresponding dynamic pydantic tools
                # if pydantic-ai provides fix for OpenAI this can be used
                # now no dynalic tools are used
                for tool in response.tools:
                    
                    # Long descriptions beyond 1023 are not supported with OpenAI,
                    # so replacing with a local file description optimized for use if it exists.
                    file_name = f"./mcp-tool-description-overrides/{server_name}__{tool.name}"

                    if os.path.exists(file_name):
                        try:
                            with open(file_name, 'r') as f:
                                file_content = f.read()
                            tool.description = file_content
                        except Exception as e:
                            logging.error(f"An error occurred while reading the file: {e}")
                            raise
                        finally: 
                            f.close
                    else:
                        logger.debug(f"File '{file_name}' not found. Using default description.")

                    # Create corresponding dynamic pydantic tools
                    dynamic_tool = self.create_dynamic_tool(tool, server_name, server_agent)
                    self.tools[tool.name] = {
                        "name": tool.name,
                        "callable": self.call_tool(f"{server_name}__{tool.name}"),
                        "schema": {
                            "type": "function",
                            "function": {
                                "name": tool.name,
                                "description": tool.description,
                                "parameters": tool.inputSchema,
                            },
                        },
                    }
                    logger.debug(f"Added tool: {tool.name}")
                
                logger.info(f"Connected to server {server_name} with tools: {', '.join(tool['name'] for tool in server_tools)}")

                self.connected = True
            else:
                logging.info(f"Server {server_name} is disabled. Skipping connection.")
        logging.info("Done connecting to servers.")

    async def add_mcp_configuration(self, query: str) -> Optional[str]:
        """
        Add a new MCP server configuration if the query starts with 'mcpServer'.
        The query should be in the format:
        {"server_name": {"command": "command", "args": ["arg1", "arg2"], "env": null}}

        Args:
            query (str): The configuration query in JSON format.

        Returns:
            Optional[str]: Success message or error message if the operation fails.
        """
        braces_warning = ""
        try:
            config_str = query  # Define config_str from the query
            # Check for mismatched curly braces and attempt to fix
            open_braces = config_str.count('{')
            close_braces = config_str.count('}')
            if open_braces > close_braces:
                config_str += '}' * (open_braces - close_braces)
                braces_warning = "Added missing closing brace(s) to the configuration."
            elif close_braces > open_braces:
                config_str = '{' * (close_braces - open_braces) + config_str
                braces_warning = "Added missing opening brace(s) to the configuration."
            config_str = query
            new_config = json.loads(config_str)
            logging.debug("New configuration to add:", json.dumps(new_config, indent=2))

            # Validate the new configuration
            if not isinstance(new_config, dict):
                return "Error: Configuration must be a JSON object."

            # The server name is the key in the new_config dictionary
            server_name = next(iter(new_config), None)
            if not server_name:
                return "Error: Server name is required as the key in the configuration."

            server_config = new_config[server_name]

            # Validate the server configuration
            if not isinstance(server_config, dict):
                return f"Error: Configuration for server '{server_name}' must be a JSON object."

            if "command" not in server_config or "args" not in server_config:
                return f"Error: 'command' and 'args' are required for server '{server_name}'."

            # Load the existing config
            try:
                with open(self.config_file, "r") as f:
                    config = json.load(f)
            except FileNotFoundError:
                return f"Error: {self.config_file} file not found."
            except json.JSONDecodeError:
                return f"Error: {self.config_file} is not a valid JSON file."

            # Check if the server name already exists
            if server_name in config.get("mcpServers", {}):
                return f"Error: Server '{server_name}' already exists in the configuration."

            # Add the new server configuration with default enabled status
            if "mcpServers" not in config:
                config["mcpServers"] = {}

            config["mcpServers"][server_name] = {
                "command": server_config["command"],
                "args": server_config["args"],
                "env": server_config.get("env"),  # Optional field
                "enable": server_config.get("enable", True)  # Default to True if not specified
            }

            # Save the updated config back to the file
            with open(self.config_file, "w") as f:
                json.dump(config, f, indent=2)

            # Connect to the new server
            await self.connect_to_server_with_config(server_name, config["mcpServers"][server_name])

            return f"Successfully added and connected to server '{server_name}'."

        except json.JSONDecodeError:
            return "Error: Invalid JSON format in the query."
        except Exception as e:
            raise f"Error adding MCP configuration: {str(e)}"


    async def drop_mcp_server(self, server_name: str) -> str:
        """
        Remove an MCP server from the configuration and disconnect it.

        Args:
            server_name (str): The name of the server to remove.

        Returns:
            str: Success message or error message if the operation fails.
        """
        try:
            with open(self.config_file, "r") as f:
                config = json.load(f)

            if server_name not in config.get("mcpServers", {}):
                return f"Error: Server '{server_name}' does not exist in the configuration."

            # Remove the server from the configuration
            del config["mcpServers"][server_name]

            # Save the updated config back to the file
            with open(self.config_file, "w") as f:
                json.dump(config, f, indent=2)

            # Disconnect the server if it is connected
            if server_name in self.sessions:
                del self.sessions[server_name]
                del self.agents[server_name]

            return f"Successfully removed and disconnected server '{server_name}'."

        except FileNotFoundError:
            return f"Error: {self.config_file} file not found."
        except json.JSONDecodeError:
            return f"Error: {self.config_file} is not a valid JSON file."
        except Exception as e:
            return f"Error removing MCP server: {str(e)}"
         
    async def connect_to_server_with_config(self, server_name: str, server_config: dict) -> None:
        """
        Connect to a server using the provided configuration.

        Args:
            server_name (str): The name of the server.
            server_config (dict): The server configuration dictionary.
        """
        server_params = StdioServerParameters(
            command=server_config['command'],
            args=server_config['args'],
            env=None
        )
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        stdio, write = stdio_transport
        session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))
        await session.initialize()
        self.sessions[server_name] = session

        response = await session.list_tools()
        server_tools = [{
            "name": f"{server_name}__{tool.name}",
            "description": tool.description,
            "input_schema": tool.inputSchema
        } for tool in response.tools]

        self.available_tools.extend(server_tools)
        return None

    async def list_mcp_servers(self) -> str:
        """
        List all MCP servers in the configuration.

        Returns:
            str: A formatted string listing enabled and disabled servers.
        """
        try:
            with open(self.config_file, "r") as f:
                config = json.load(f)

            enabled_servers = [
                server_name for server_name, server_config in config.get("mcpServers", {}).items()
                if server_config.get("enable", True)
            ]
            disabled_servers = [
                server_name for server_name, server_config in config.get("mcpServers", {}).items()
                if not server_config.get("enable", True)
            ]
            suggestion = "\n - Use /functions &lt;server_name&gt; to list functions provided by a specific server."
            newline = '\n'
            return f"**Enabled servers:**{newline}1. {newline}1. ".join(enabled_servers) + f"{newline}{newline}Disabled servers: {', '.join(disabled_servers)}{newline}Next command suggestion: {suggestion}"
        except FileNotFoundError:
            return f"Error: {self.config_file} file not found."
        except json.JSONDecodeError:
            return f"Error: {self.config_file} is not a valid JSON file."

    async def list_server_functions(self, server_name: str) -> str:
        """
        List all functions provided by a specific MCP server.

        Args:
            server_name (str): The name of the server.

        Returns:
            str: A formatted string listing the functions or an error message.
        """
        if server_name not in self.sessions:
            return f"Error: Server '{server_name}' is not connected."
        try:
            response = await self.sessions[server_name].list_tools()
            functions = []
            for tool in response.tools:
                parameters = tool.inputSchema.get('properties', {})
                functions.append({
                    "function name": tool.name,
                    "parameters": parameters
                })
            return f"Functions for server '{server_name}':\n" + json_to_markdown(functions)

        except Exception as e:
            return f"Error listing functions for server '{server_name}': {str(e)}"

    async def cleanup(self) -> None:
        """
        Clean up resources by closing sessions and clearing tool lists.
        """
        logging.debug("Cleaning up resources...")
        await self.exit_stack.aclose()
        self.sessions.clear()
        self.available_tools.clear()
        self.connected = False
        logging.info("Cleanup completed.")

    async def toggle_server_status(self, server_names: List[str], enable: bool) -> str:
        """
        Enable or disable specific MCP servers.

        Args:
            server_names (List[str]): List of server names to toggle.
            enable (bool): True to enable, False to disable.

        Returns:
            str: A message indicating the result of the operation.
        """
        try:
            with open(self.config_file, "r") as f:
                config = json.load(f)

            results = []
            for server_name in server_names:
                if server_name not in config.get("mcpServers", {}):
                    results.append(f"Error: Server '{server_name}' does not exist in the configuration.")
                    continue

                # Update the enabled status
                config["mcpServers"][server_name]["enable"] = enable
                status = "enabled" if enable else "disabled"
                results.append(f"Successfully {status} server '{server_name}'.")

            # Save the updated config back to the file
            with open(self.config_file, "w") as f:
                json.dump(config, f, indent=2)

            return "\n".join(results)

        except FileNotFoundError:
            return "Error: mcp_config.json file not found."
        except json.JSONDecodeError:
            return "Error: mcp_config.json is not a valid JSON file."
        except Exception as e:
            return f"Error toggling server status: {str(e)}"
        
    async def cleanup(self):
        """Clean up resources."""
        logging.debug("Cleaning up resources...")
        await self.exit_stack.aclose()
        self.sessions.clear()
        self.available_tools.clear()
        self.connected = False
        logging.info("Cleanup completed.")
    
    async def get_available_tools(self) -> List[Any]:
        """
        Retrieve a list of available tools from the MCP server.
        Simplify the schema for each tool to make it compatible with the OpenAI API.

        Returns:
            List[Any]: A list of available tools with simplified schemas.
        """
        if not self.sessions:
            raise RuntimeError("Not connected to MCP server")
    
        def simplify_schema(schema):
            """
            Simplifies a JSON schema by removing unsupported constructs like 'allOf', 'oneOf', etc.,
            and preserving the core structure and properties. Needed for pandoc to work with the LLM.

            Args:
                schema (dict): The original JSON schema.

            Returns:
                dict: A simplified JSON schema.
            """
            # Create a new schema with only the basic structure
            simplified_schema = {
                "type": "object",
                "properties": schema.get("properties", {}),
                "required": schema.get("required", []),
                "additionalProperties": schema.get("additionalProperties", False)
            }

            # Remove unsupported constructs like 'allOf', 'oneOf', 'anyOf', 'not', 'enum' at the top level
            for key in ["allOf", "oneOf", "anyOf", "not", "enum"]:
                if key in simplified_schema:
                    del simplified_schema[key]

            return simplified_schema

        return  {
            tool['name']: {
                "name": tool['name'],
                "callable": self.call_tool(
                    tool['name']
                ),  # returns a callable function for the rpc call
                "schema": {
                    "type": "function",
                    "function": {
                        "name": tool['name'],
                        "description": tool['description'][:1023],
                        "parameters": simplify_schema(tool['input_schema'])
                    },
                },
            }
            for tool in self.available_tools
            if tool['name']
            != "xxx"  # Excludes xxx tool as it has an incorrect schema
        }
        
    def call_tool(self, server__tool_name: str) -> Any:
        """
        Create a callable function for a specific tool.
        This allows us to execute functions through the MCP server.

        Args:
            server__tool_name (str): The name of the tool to create a callable for.

        Returns:
            Any: A callable async function that executes the specified tool.
        """
        server_name, tool_name = server__tool_name.split("__")  

        if not server_name in self.sessions:
            raise RuntimeError("Not connected to MCP server")
 
        async def callable(*args, **kwargs):
            try:
                response = await asyncio.wait_for(
                    self.sessions[server_name].call_tool(tool_name, arguments=kwargs),
                    timeout=10.0  # Set a timeout
                )
                return response.content[0].text if response.content else None
            except asyncio.TimeoutError:
                # pandoc docker will not return timely respons
                logging.debug("Timeout while calling MCP server")
                return None
            except Exception as e:
                #ignore for now, many mcp servers not production ready
                logging.error(f"Error calling MCP server: {e}")
                return None
 
        return callable
    
    def create_dynamic_tool(self, tool, server_name: str, server_agent: Agent) -> Tool:
        """
        Create a dynamic tool for a given server and tool.

        Args:
            tool: The tool object.
            server_name (str): The name of the server.
            server_agent (Agent): The agent associated with the server.

        Returns:
            Tool: A dynamic tool object.
        """
        async def prepare_tool(
            ctx: RunContext[str], 
            tool_def: ToolDefinition,
            tool_name: str = tool.name,
            server: str = server_name
        ) -> Union[ToolDefinition, None]:
            # Customize tool definition based on server context
            tool_def.name = f"{server}__{tool_name}"
            tool_def.description = f"Tool from {server} server: {tool.description}"
            logging.info(tool_def.description)
            return tool_def

        async def tool_func(ctx: RunContext[Any], str_arg) -> str:
            agent_response = await server_agent.run_sync(str_arg)
            logging.debug(f"Server agent response: {agent_response}")
            logging.info(f"Tool {tool.name} called with {str_arg}. Agent response: {agent_response}")
            return f"Tool {tool.name} called with {str_arg}. Agent response: {agent_response}"

        return Tool(
            tool_func,
            prepare=prepare_tool,
            name=f"{server_name}__{tool.name}",
            description=tool.description
        )
    async def handle_slash_commands(self, query: str) -> str:
        """
        Handle slash commands for adding MCP servers and listing available functions.

        Args:
            query (str): The command query.

        Returns:
            str: The result of the command execution.
        """
        try:
            command, *args = query.split()
            if command == "/addMcpServer":
                result = await self.add_mcp_configuration(" ".join(args))
            elif command == "/list":
                result = await self.list_mcp_servers()
            elif command == "/enable" and args:
                result = await self.toggle_server_status(args, True)  # Pass list of server names
            elif command == "/disable" and args:
                result = await self.toggle_server_status(args, False)  # Pass list of server names
            elif command == "/functions" and args:
                result = await self.list_server_functions(args[0])
            elif command == "/dropMcpServer" and args:
                result = await self.drop_mcp_server(args[0])
            else:
                result = "Error: Invalid command or missing arguments."
        except Exception as e:
            logging.error(f"Error handling slash commands: {e}")
            raise

        return result
    
async def agent_loop(query: str, tools: dict, messages: List[dict] = None, deps: Deps = None):
    """
    Main interaction loop that processes user queries using the LLM and available tools.
 
    This function:
    1. Sends the user query to the LLM with context about available tools
    2. Processes the LLM's response, including any tool calls
    3. Returns the final response to the user
 
    Args:
        query: User's input question or command
        tools: Dictionary of available tools and their schemas
        messages: List of messages to pass to the LLM, defaults to None
    """
 
    messages = (
        [
            {
                "role": "system",
                "content": SYSTEM_PROMPT.format(
                    tools="\n- ".join(
                        [
                            f"{t['name']}: {t['schema']['function']['description']}"
                            for t in tools.values()
                        ]
                    )
                ),  # Creates System prompt based on available MCP server tools
            },
        ]
        if messages is None
        else messages  # reuse existing messages if provided
    )
    # add user query to the messages list
    messages.append({"role": "user", "content": query})
    pprint.pprint(messages)

    # Query LLM with the system prompt, user query, and available tools
    first_response = await client.chat.completions.create(
        model=language_model,
        messages=messages,
        tools=([t["schema"] for t in tools.values()] if len(tools) > 0 else None),
        max_tokens=4096,
        temperature=0,
    )
    # detect how the LLM call was completed:
    # tool_calls: if the LLM used a tool
    # stop: If the LLM generated a general response, e.g. "Hello, how can I help you today?"
    stop_reason = (
        "tool_calls"
        if first_response.choices[0].message.tool_calls is not None
        else first_response.choices[0].finish_reason
    )
 
    if stop_reason == "tool_calls":
        # Extract tool use details from response
        for tool_call in first_response.choices[0].message.tool_calls:
            arguments = (
                json.loads(tool_call.function.arguments)
                if isinstance(tool_call.function.arguments, str)
                else tool_call.function.arguments
            )
            # Call the tool with the arguments using our callable initialized in the tools dict
            logging.debug(tool_call.function.name)
            tool_result = await tools[tool_call.function.name]["callable"](**arguments)
            if tool_result is None:
                tool_result = f"{tool_call.function.name}"
            #logging.debug("tool result begin")
            #pprint.pprint(tool_result)
            #logging.debug("tool result end")

            # Add tool call to messages with an id
            messages.append({
                "role": "assistant",
                "content": None,
                "tool_calls": [{
                    "id": tool_call.id,
                    "type": "function",
                    "function": {
                        "name": tool_call.function.name,
                        "arguments": json.dumps(arguments)
                    }
                }]
            })
            
            # Add the tool result to the messages list
            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": tool_call.function.name,
                    "content": json.dumps(tool_result),
                }
            )
            pprint.pprint(messages)

        # Query LLM with the user query and the tool results
        new_response = await client.chat.completions.create(
            model=language_model,
            messages=messages,
        )
 
    elif stop_reason == "stop":
        # If the LLM stopped on its own, use the first response
        new_response = first_response
    else:
        raise ValueError(f"Unknown stop reason: {stop_reason}")
    
    # Add the LLM response to the messages list
    messages.append(
        {"role": "assistant", "content": new_response.choices[0].message.content}
    )

    # Return the LLM response and messages
    return new_response.choices[0].message.content, messages

def json_to_markdown(data, indent=0):
    markdown = ""
    prefix = "  " * indent  # Indentation for nested structures

    if isinstance(data, dict):
        for key, value in data.items():
            markdown += f"{prefix}- **{key}**: "
            if isinstance(value, (dict, list)):
                markdown += "\n" + json_to_markdown(value, indent + 1)
            else:
                markdown += f"{value}\n"
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, (dict, list)):
                markdown += f"{prefix}- \n{json_to_markdown(item, indent + 1)}"
            else:
                markdown += f"{prefix}- {item}\n"
    else:
        markdown += f"{prefix}- {data}\n"

    return markdown

async def main():
    """
    Main function that sets up the MCP server, initializes tools, and runs the interactive loop.
    """
    mcp_client = MCPClient()
    await mcp_client.connect_to_server()

    tools = await mcp_client.get_available_tools()
    
    # Start interactive prompt loop for user queries
    messages = None
    while True:
        try:
            # Get user input and check for exit commands
            user_input = input("\nEnter your prompt (or 'quit' to exit): ")
            if user_input.lower() in ["quit", "exit", "q"]:
                break
            if user_input.startswith("/"):
                response = await mcp_client.handle_slash_commands(user_input)
            else:
                # Process the prompt and run agent loop
                response, messages = await agent_loop(user_input, tools, messages)
            logging.debug("Response:", response)
            # logging.debug("Messages:", messages)
        except KeyboardInterrupt:
            logging.debug("Exiting...")
            break
        except Exception as e:
            logging.error(f"Error occurred: {e}")
 
    await mcp_client.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="mcp_config_examples.json">
{
  "mcpServers": {
    "timezone": {
      "command": "python",
      "args": [
        "-m",
        "mcp_server_time",
        "--local-timezone=America/New_York"
      ],
      "enable": true
    },
    "weather": {
      "command": "python",
      "args": [
        "./weather-server-python/src/weather/server.py"
      ],
      "enable": true
    },
    "sequentialthinking": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "-i",
        "mcp/sequentialthinking"
      ],
      "enable": true
    },
    "memory": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "mcp/memory"
      ],
      "env": null,
      "enable": true
    },
    "fetch": {
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "mcp/fetch"
      ],
      "env": null,
      "enable": true
    },
    "mcp-pandoc": {
      "command": "uvx",
      "args": [
        "mcp-pandoc"
      ],
      "env": null,
      "enable": true
    }
  }
}
</file>

<file path="mcp_config.json">
{
  "mcpServers": {
    "weather": {
      "command": "python",
      "args": [
        "./weather-server-python/src/weather/server.py"
      ],
      "enable": true
    }
  }
}
</file>

<file path="README.md">
# ThirdBrAIn MCP OpenAI agent

Author: [ThirdBrAIn.tech](https://github.com/cbruyndoncx)

This agent created for the oTTomator hackathon makes the LiveAgentStudio work in similar ways to Claude Desktop but for OpenAI compatible models.

This agent makes mcp tools accessible to openai compatible llms (chat completions).
This enables the use of MCP servers across a much wider range of models such as:
- evidently OpenAI such as GPT-4o-mini, GPT-4o
- deepseek-chat (currently v3) , at this point deepseek-r1 does not support function calling.
- ollama with deepseek variants (not tested yet).

> MCP makes it possible to centralize resources (RAG), prompts and tools and use the same code across platforms. 
> It is an extreme DRY concept for distributed agentic systems that hides the implementation details. 
> For more information see https://modelcontextprotocol.org

## Demo
### Planning for skiweekend
The agent has built-in python code for weather forecast service for US. 
I asked where to go skiing around Denver next weekend and did some follow-up calls to get exact weather conditions and a comparison between ski resorts demonstrating iterative tool calling until the weather forecast for each ski resort is obtained and recommendations made.
![Demo Skiweekend (animated gif)](docs/Skiweekend.gif)

## Installation
If you are unfamiliar with the hackathon agents, follow the detailed installation instructions provided for the hackathon.
[LiveAgentStudio_README.md]

### Specifics for the MCP OpenAI agent
This agent has uses or has been tested with:
- Python 3.12
- pip
- Supabase
- OpenAI API Key, or Deepseek API key, or no key if using Ollama
- Appropriate runtime environment for the mcp server commands you want to use such as
  - Docker
  - Node / npx
  - Uv / uvx
  - Python3 

> Note:
> Anthropic and Docker have published servers that are easy to install.
> PulseMCP website has the most comprehensive collection of MCP servers.

### .env 
Rename the env example file 
`mv .env.example .env`

Use your favourite editor (mine is **micro** - like nano but with mouse support and sensible keybindings) and update .env 

### mcp_config.json default setup
The `mcp_config.json` file contains the default configurations for MCP servers. Each server configuration includes the command to execute, arguments, and optional environment variables. This file is essential for the MCP agent to know how to interact with different servers.
An enable option has been added to be more flexible once running.

Example default setup:
```json
{
    "memory": {
        "command": "docker",
        "args": ["run", "-i", "--rm", "mcp/memory"],
        "env": {},
        "enable" : true
    },
    "compute": {
        "command": "docker",
        "args": ["run", "-i", "--rm", "mcp/compute"],
        "env": {},
        "enable" : false
    }
}
```

## Usage

### Run the agent

This launches the agent ready to be called by oTTomator chatbot

```bash
uvicorn thirdbrain-mcp-openai-agent:app --host 0.0.0.0 --port 8001
```
You can check if the agent is running by navigating with your browser to
```http://localhost:8001/api/thirdbrain-hello```

### Run standalone MCP Client from CLI

With the standalone mcp_client file, you can test the workings of the MCP client without oTTomator. 
They each have their own specific startup code, but the core library is the same. mcp_client.py functions are called from the agent.

```bash
python mcp_client.py
```

### Slash commands within the chatbot (agent and cli)

The management of the mcp_config.json file is done through slash commands. We want this to be really deterministic when managing the configuration.


![List mcp servers and request functions supported by mcp server](docs/ThirdBrain-mcp-openai-agent-Command-list-functions.png)

#### /list
Lists all enabled and disabled mcp servers known on the system

#### /functions <server-name>
Lists all the functions with their signature for the particular mcp server

#### /addMcpServer {servername: { command: X , args: [], env: {} }}
This command allows you to add a new MCP server configuration. The configuration should be in valid JSON format and will be saved in the `mcp_config.json` file. The `command` specifies the executable to run, `args` is an array of arguments for the command, and `env` is an optional object for environment variables.

Example:
```/addMcpServer {"memory": {"command": "docker", "args": ["run", "-i", "--rm", "mcp/memory"]}}```

> Note beginning and ending curly braces. Typically 2 unless you also have a "env": {} section

#### /disable [server-name, ...]
This command disables one or more specified MCP servers. Provide the server names as a comma-separated list.

Example:
```/disable server1, server2```

#### /enable [server-name, ...]
This command enables one or more specified MCP servers. Provide the server names as a comma-separated list.

Example:
```/enable server1, server2```

#### /dropMcpServer [server-name, ...]
This command removes an existing MCP server configuration. Provide the server name you wish to remove.

Example:
```/dropMcpServer server1```


## FAQ
1. **Why SELECTED in .env ?**: This gives more flexibility to test and support additional OpenAI compatible models without needing code changes
2. **Will you use pydantic-ai in the future ?**: Looking at the discussions for the Agent rewrite, depending on how strict the response validations are, this might be feasible or not. A lot of functionality is hidden under the Pydantic-AI hood.
3. **Why no setup for openrouter and others ?**: spent too much time trying to figure out why Pydantic AI wouldn't work with OpenAI that I did not test with extra intermediaries. Only when the OpenAI worked well, i started testing with deepseek-chat for real cheap solution. If other models are truely openai compatible than it should/would work

Author originally created a section outlining some bugs with Pydantic AI to explain why it isn't used here. That has been removed to keep this README from going stale quick.

## Contributing

This agent is part of the oTTomator agents collection. For contributions or issues, please refer to the main repository guidelines.
</file>

<file path="requirements.txt">
fastapi
uvicorn
pydantic
supabase
python-dotenv
asyncpg
colorama
httpx==0.27.2;
openai==1.55.3;
mcp
mcp-server-time
pydantic-ai
python-multipart
nest_asyncio==1.6.0;
</file>

<file path="thirdbrain_mcp_openai_agent.py">
from __future__ import annotations as _annotations

import httpx
from typing import Optional, Any, Dict
from fastapi import FastAPI, HTTPException, Security, Depends
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from fastapi.middleware.cors import CORSMiddleware
from supabase import create_client, Client
from pydantic import BaseModel
from dotenv import load_dotenv
import os
from exceptions import ConfigurationError, ToolError, DatabaseConnectionError

import logging

logger = logging.getLogger(__name__)

# mcp client for pydantic ai
from mcp_client import MCPClient, Deps, logging, agent_loop

def validate_env_vars(required_vars: list[str]) -> None:
    """Validate that all required environment variables are set."""
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        logging.error(f"Missing environment variables: {', '.join(missing_vars)}")
        raise ConfigurationError(f"Missing environment variables: {', '.join(missing_vars)}")

# Load environment variables
load_dotenv()

# Supabase setup
supabase: Client = None

# Define a context manager for startup and shutdown
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup logic
    logger.info("Starting up the FastAPI application.")

    # Validate environment variables
    try:
        validate_env_vars(["SUPABASE_URL", "SUPABASE_SERVICE_KEY", "API_BEARER_TOKEN"])
    except ConfigurationError as e:
        logger.error(f"Configuration error during startup: {e}")
        raise

    try:
        # Initialize Supabase client
        global supabase
        supabase = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_KEY")
        )
        if not supabase:
            logging.error("Supabase client is not initialized. Please check your environment variables.")
            raise DatabaseConnectionError("Supabase client initialization failed.")

        # Initialize MCPClient and connect to server
        global mcp_client
        mcp_client = MCPClient()
        await mcp_client.connect_to_server()
        logging.info("Startup tasks completed successfully.")
    except Exception as e:
        logger.error(f"Error during startup: {e}")
        raise

    # Yield control back to FastAPI
    yield

    # Shutdown logic
    logger.info("Shutting down the FastAPI application.")
    await mcp_client.cleanup()  
    logging.info("Shutdown tasks completed successfully.")

# Initialize FastAPI app with lifespan
app = FastAPI(lifespan=lifespan)
security = HTTPBearer()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response Models
class AgentRequest(BaseModel):
    query: str
    user_id: str
    request_id: str
    session_id: str

class AgentResponse(BaseModel):
    success: bool

def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)) -> bool:
    """Verify the bearer token against environment variable."""
    expected_token = os.getenv("API_BEARER_TOKEN")
    if not expected_token:
        raise HTTPException(
            status_code=500,
            detail="API_BEARER_TOKEN environment variable not set"
        )
    if credentials.credentials != expected_token:
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication token"
        )
    return True

async def get_conversation_history(session_id: str, limit: int = 10) -> list[dict[str, Any]]:
    """Fetch the most recent conversation history for a session."""
    try:
        response = supabase.table("messages") \
            .select("*") \
            .eq("session_id", session_id) \
            .order("created_at", desc=True) \
            .limit(limit) \
            .execute()
        
        return response.data[::-1]  # Reverse to get chronological order
    except Exception as e:
        raise DatabaseConnectionError(f"Failed to fetch conversation history: {str(e)}")

async def save_message(session_id: str, message_type: str, content: str, data: Optional[Dict] = None):
    """Store a message in the Supabase messages table."""
    message_obj = {"type": message_type, "content": content, **({"data": data} if data else {})}

    try:
        supabase.table("messages").insert({
            "session_id": session_id,
            "message": message_obj
        }).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to store message: {str(e)}")

@app.get("/api/thirdbrain-hello")
async def thirdbrain_hello():
    return {"message": "Server is running"}

@app.post("/api/thirdbrain-mcp-openai-agent", response_model=AgentResponse)
async def thirdbrain_mcp_openai_agent(
    request: AgentRequest,
    authenticated: bool = Depends(verify_token)
):
    try:
        # Fetch conversation history from the DB
        conversation_history = await get_conversation_history(request.session_id)
        
        # Convert conversation history to format expected by agent
        messages = []
        for msg in conversation_history:
            #logger.debug("Processing message: %s", msg)
            msg_data = msg["message"]
            msg_type = msg_data["type"]
            msg_content = msg_data["content"]

            # Convert to appropriate message type for the agent
            if msg_type == "human":
                #messages.append(UserPromptPart(content=msg_content))
                messages.append({"role": "user", "content": msg_content})
            elif msg_type == "ai":
                #messages.append(TextPart(content=msg_content))
                messages.append({"role": "assistant", "content": msg_content})
            else:
                logging.debug("this was most likely an error message stored in the messages table")

        # Store user's query if it doesn't start with a slash
        await save_message(
            session_id=request.session_id,
            message_type="human",
            content=request.query
        )

    except ToolError as e:
        logger.error(f"Tool error: {e}")
        raise HTTPException(status_code=500, detail=f"Tool error: {str(e)}")
    except Exception as e:
        logger.exception("Unexpected error during request processing")
        raise HTTPException(status_code=500, detail="Internal server error")
    

    # Get available tools and prepare them for the LLM
    tools = await mcp_client.get_available_tools()
    
    # Initialize agent dependencies
    async with httpx.AsyncClient() as client: 
        try:
            deps = Deps(
                client=client,
                supabase=supabase,
                session_id=request.session_id,
            )
            if request.query.startswith("/"):
                result = await mcp_client.handle_slash_commands(request.query)
            else:     
                result, messages = await agent_loop(request.query, tools, messages, deps)
            if request.query.startswith("/"):
                # Prepend the result with the slash command and server name
                command_info = f"Executed command: {request.query.split()[0]} {request.query.split()[1] if len(request.query.split()) > 1 else ''}".strip()
                result = f"{command_info}\n{result}"
            logging.info(f"Result: {result}")
        except KeyboardInterrupt:
            logger.info("Keyboard interrupt detected. Exiting...")
            return
        except Exception as e:
            logging.error(f"Error in agent loop: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")

        try:
            # Store agent's response
            await save_message(
                session_id=request.session_id,
                message_type="ai",
                content=result,
                data={"request_id": request.request_id}
            )

            return AgentResponse(success=True)
        except Exception as e:
            # Store error message in conversation
            await save_message(
                session_id=request.session_id,
                message_type="ai",
                content="I apologize, but I encountered an error processing your request.",
                data={"error": str(e), "request_id": request.request_id}
            )
            return AgentResponse(success=False)

if __name__ == "__main__":
    import uvicorn

    # Run the server
    uvicorn.run(app, host="0.0.0.0", port=8001)
</file>

</files>
