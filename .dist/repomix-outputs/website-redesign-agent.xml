This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
integrations/
  __init__.py
  wordpress.py
tests/
  __init__.py
  test_website_analyzer.py
tools/
  __init__.py
  website_analyzer.py
.env.example
config.json
Dockerfile
README.md
requirements.txt
run_local.py
run_tests.py
solnai_integration.py
Website_Redesign_Agent.json
website_redesign_agent.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="integrations/__init__.py">
"""
Integrations for the Website Redesign Agent.

This package contains integrations with various CMS platforms,
analytics services, and other tools.
"""

from .wordpress import WordPressIntegration

__all__ = ["WordPressIntegration"]
</file>

<file path="integrations/wordpress.py">
"""
WordPress Integration

This module provides tools for integrating with WordPress websites,
including content extraction, modification, and deployment.
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional

import requests
from requests.auth import HTTPBasicAuth

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WordPressIntegration:
    """
    Class for integrating with WordPress websites.
    """
    
    def __init__(self, site_url: str, username: Optional[str] = None, password: Optional[str] = None):
        """
        Initialize the WordPress integration.
        
        Args:
            site_url: The WordPress site URL
            username: WordPress API username (optional)
            password: WordPress API password (optional)
        """
        self.site_url = site_url.rstrip("/")
        self.api_url = f"{self.site_url}/wp-json/wp/v2"
        self.username = username or os.getenv("WORDPRESS_API_USERNAME")
        self.password = password or os.getenv("WORDPRESS_API_PASSWORD")
        
        # Check if credentials are available
        self.has_auth = bool(self.username and self.password)
    
    def get_site_info(self) -> Dict[str, Any]:
        """
        Get basic information about the WordPress site.
        
        Returns:
            Dict with site information
        """
        try:
            response = requests.get(f"{self.site_url}/wp-json")
            response.raise_for_status()
            data = response.json()
            
            return {
                "name": data.get("name", "Unknown"),
                "description": data.get("description", ""),
                "url": data.get("url", self.site_url),
                "home": data.get("home", self.site_url),
                "gmt_offset": data.get("gmt_offset", 0),
                "timezone_string": data.get("timezone_string", "UTC"),
                "namespaces": data.get("namespaces", [])
            }
        except Exception as e:
            logger.error(f"Error getting site info: {str(e)}")
            return {"error": str(e)}
    
    def get_pages(self, per_page: int = 10, page: int = 1) -> List[Dict[str, Any]]:
        """
        Get pages from the WordPress site.
        
        Args:
            per_page: Number of pages per request
            page: Page number
            
        Returns:
            List of pages
        """
        try:
            params = {
                "per_page": per_page,
                "page": page
            }
            
            response = requests.get(f"{self.api_url}/pages", params=params)
            response.raise_for_status()
            
            return response.json()
        except Exception as e:
            logger.error(f"Error getting pages: {str(e)}")
            return []
    
    def get_posts(self, per_page: int = 10, page: int = 1) -> List[Dict[str, Any]]:
        """
        Get posts from the WordPress site.
        
        Args:
            per_page: Number of posts per request
            page: Page number
            
        Returns:
            List of posts
        """
        try:
            params = {
                "per_page": per_page,
                "page": page
            }
            
            response = requests.get(f"{self.api_url}/posts", params=params)
            response.raise_for_status()
            
            return response.json()
        except Exception as e:
            logger.error(f"Error getting posts: {str(e)}")
            return []
    
    def get_menus(self) -> List[Dict[str, Any]]:
        """
        Get menus from the WordPress site.
        
        Returns:
            List of menus
        """
        try:
            # This endpoint requires the WP API Menus plugin
            response = requests.get(f"{self.site_url}/wp-json/wp-api-menus/v2/menus")
            
            if response.status_code == 200:
                return response.json()
            else:
                logger.warning("WP API Menus plugin may not be installed")
                return []
        except Exception as e:
            logger.error(f"Error getting menus: {str(e)}")
            return []
    
    def get_media(self, per_page: int = 10, page: int = 1) -> List[Dict[str, Any]]:
        """
        Get media from the WordPress site.
        
        Args:
            per_page: Number of media items per request
            page: Page number
            
        Returns:
            List of media items
        """
        try:
            params = {
                "per_page": per_page,
                "page": page
            }
            
            response = requests.get(f"{self.api_url}/media", params=params)
            response.raise_for_status()
            
            return response.json()
        except Exception as e:
            logger.error(f"Error getting media: {str(e)}")
            return []
    
    def update_page(self, page_id: int, title: str, content: str) -> Dict[str, Any]:
        """
        Update a page on the WordPress site.
        
        Args:
            page_id: The page ID
            title: The page title
            content: The page content
            
        Returns:
            Dict with update result
        """
        if not self.has_auth:
            return {"error": "Authentication credentials not provided"}
        
        try:
            auth = HTTPBasicAuth(self.username, self.password)
            data = {
                "title": title,
                "content": content
            }
            
            response = requests.post(
                f"{self.api_url}/pages/{page_id}",
                auth=auth,
                json=data
            )
            response.raise_for_status()
            
            return response.json()
        except Exception as e:
            logger.error(f"Error updating page: {str(e)}")
            return {"error": str(e)}
    
    def create_page(self, title: str, content: str, status: str = "draft") -> Dict[str, Any]:
        """
        Create a new page on the WordPress site.
        
        Args:
            title: The page title
            content: The page content
            status: The page status (draft, publish, etc.)
            
        Returns:
            Dict with creation result
        """
        if not self.has_auth:
            return {"error": "Authentication credentials not provided"}
        
        try:
            auth = HTTPBasicAuth(self.username, self.password)
            data = {
                "title": title,
                "content": content,
                "status": status
            }
            
            response = requests.post(
                f"{self.api_url}/pages",
                auth=auth,
                json=data
            )
            response.raise_for_status()
            
            return response.json()
        except Exception as e:
            logger.error(f"Error creating page: {str(e)}")
            return {"error": str(e)}
    
    def get_theme_info(self) -> Dict[str, Any]:
        """
        Get information about the active theme.
        
        Returns:
            Dict with theme information
        """
        try:
            response = requests.get(f"{self.site_url}/wp-json/wp/v2/themes")
            
            if response.status_code == 200:
                themes = response.json()
                active_theme = next((t for t in themes if t.get("active")), None)
                return active_theme or {}
            else:
                logger.warning("Could not get theme information")
                return {}
        except Exception as e:
            logger.error(f"Error getting theme info: {str(e)}")
            return {"error": str(e)}
    
    def export_content(self, output_dir: str) -> Dict[str, Any]:
        """
        Export content from the WordPress site to local files.
        
        Args:
            output_dir: Directory to save exported content
            
        Returns:
            Dict with export results
        """
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # Export pages
            pages = []
            page_num = 1
            while True:
                batch = self.get_pages(per_page=100, page=page_num)
                if not batch:
                    break
                pages.extend(batch)
                page_num += 1
            
            with open(os.path.join(output_dir, "pages.json"), "w") as f:
                json.dump(pages, f, indent=2)
            
            # Export posts
            posts = []
            post_num = 1
            while True:
                batch = self.get_posts(per_page=100, page=post_num)
                if not batch:
                    break
                posts.extend(batch)
                post_num += 1
            
            with open(os.path.join(output_dir, "posts.json"), "w") as f:
                json.dump(posts, f, indent=2)
            
            # Export menus
            menus = self.get_menus()
            with open(os.path.join(output_dir, "menus.json"), "w") as f:
                json.dump(menus, f, indent=2)
            
            # Export site info
            site_info = self.get_site_info()
            with open(os.path.join(output_dir, "site_info.json"), "w") as f:
                json.dump(site_info, f, indent=2)
            
            return {
                "success": True,
                "exported_items": {
                    "pages": len(pages),
                    "posts": len(posts),
                    "menus": len(menus)
                },
                "output_dir": output_dir
            }
        except Exception as e:
            logger.error(f"Error exporting content: {str(e)}")
            return {"error": str(e)}

if __name__ == "__main__":
    # Example usage
    wp = WordPressIntegration("https://example.com")
    site_info = wp.get_site_info()
    print(json.dumps(site_info, indent=2))
</file>

<file path="tests/__init__.py">
"""
Tests for the Website Redesign Agent.
"""
</file>

<file path="tests/test_website_analyzer.py">
"""
Tests for the Website Analyzer tool.
"""

import os
import sys
import unittest
from unittest.mock import patch, MagicMock

# Add parent directory to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from tools.website_analyzer import WebsiteAnalyzer

class TestWebsiteAnalyzer(unittest.TestCase):
    """Test cases for the WebsiteAnalyzer class."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.analyzer = WebsiteAnalyzer()
    
    def test_is_valid_url(self):
        """Test URL validation."""
        self.assertTrue(self.analyzer._is_valid_url("https://example.com"))
        self.assertTrue(self.analyzer._is_valid_url("http://example.com/page"))
        self.assertFalse(self.analyzer._is_valid_url("example.com"))
        self.assertFalse(self.analyzer._is_valid_url("not a url"))
    
    @patch('tools.website_analyzer.requests.get')
    def test_check_robots_txt(self, mock_get):
        """Test robots.txt checking."""
        # Mock response with no disallow
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.text = "User-agent: *\nAllow: /"
        mock_get.return_value = mock_response
        
        self.assertTrue(self.analyzer._check_robots_txt("https://example.com"))
        
        # Mock response with disallow all
        mock_response.text = "User-agent: *\nDisallow: /"
        self.assertFalse(self.analyzer._check_robots_txt("https://example.com"))
    
    @patch('tools.website_analyzer.requests.get')
    def test_analyze_website_basic(self, mock_get):
        """Test basic website analysis."""
        # Mock response
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.elapsed.total_seconds.return_value = 0.5
        mock_response.headers = {"Content-Type": "text/html"}
        mock_response.content = b"<html><body>Test</body></html>"
        mock_response.text = "<html><head><title>Test Page</title></head><body>Test</body></html>"
        mock_get.return_value = mock_response
        
        # Create a simple HTML document
        html = """
        <html>
            <head>
                <title>Test Page</title>
                <meta name="description" content="Test description">
                <meta name="viewport" content="width=device-width, initial-scale=1">
            </head>
            <body>
                <h1>Test Heading</h1>
                <p>Test paragraph</p>
                <img src="test.jpg">
                <a href="https://example.com/page1">Link 1</a>
                <a href="https://example.com/page2">Link 2</a>
                <a href="https://external.com">External Link</a>
            </body>
        </html>
        """
        mock_response.text = html
        mock_response.content = html.encode('utf-8')
        
        # Test with follow_links=False to avoid recursive calls
        results = self.analyzer.analyze_website("https://example.com", follow_links=False)
        
        # Check basic results
        self.assertEqual(results["url"], "https://example.com")
        self.assertEqual(results["pages_analyzed"], 1)
        self.assertEqual(results["structure"]["pages"]["/"]["title"], "Test Page")
        self.assertEqual(results["seo"]["titles"]["https://example.com"], "Test Page")
        self.assertEqual(results["seo"]["meta_descriptions"]["https://example.com"], "Test description")
        self.assertEqual(results["seo"]["headings"]["https://example.com"]["h1"], ["Test Heading"])
        self.assertEqual(results["seo"]["images_without_alt"], 1)
        self.assertEqual(results["seo"]["internal_links"], 0)  # 0 because follow_links=False
        self.assertEqual(results["seo"]["external_links"], 0)  # 0 because follow_links=False
        self.assertTrue(results["mobile_friendly"])

if __name__ == '__main__':
    unittest.main()
</file>

<file path="tools/__init__.py">
"""
Tools for the Website Redesign Agent.

This package contains tools for analyzing websites, generating designs,
optimizing SEO, and analyzing user behavior.
"""

from .website_analyzer import get_website_analyzer_tool

__all__ = ["get_website_analyzer_tool"]
</file>

<file path="tools/website_analyzer.py">
"""
Website Analyzer Tool

This module provides tools for analyzing websites, including structure,
content, SEO, performance, and user experience.
"""

import os
import time
import logging
import json
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup
from crewai.tools import Tool

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WebsiteAnalyzer:
    """
    Class for analyzing websites and extracting structured data.
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize the WebsiteAnalyzer.
        
        Args:
            config_path: Path to configuration file (optional)
        """
        self.config = self._load_config(config_path)
        self.rate_limit = self.config.get("settings", {}).get("rate_limit", 5)
        self.max_pages = self.config.get("settings", {}).get("max_pages_to_analyze", 100)
        self.user_agent = self.config.get("settings", {}).get(
            "user_agent", "SolnAI Website Redesign Agent/1.0"
        )
        self.follow_robots = self.config.get("settings", {}).get("follow_robots_txt", True)
        self.timeout = self.config.get("settings", {}).get("timeout", 30)
        self.max_retries = self.config.get("settings", {}).get("max_retries", 3)
    
    def _load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """
        Load configuration from file.
        
        Args:
            config_path: Path to configuration file
            
        Returns:
            Dict with configuration
        """
        if not config_path:
            # Look for config.json in the same directory as this file
            config_path = os.path.join(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                "config.json"
            )
        
        try:
            with open(config_path, "r") as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load config from {config_path}: {str(e)}")
            return {}
    
    def analyze_website(self, url: str, max_depth: int = 2, follow_links: bool = True) -> Dict[str, Any]:
        """
        Analyze a website and return structured data.
        
        Args:
            url: The URL to analyze
            max_depth: Maximum depth to crawl
            follow_links: Whether to follow links
            
        Returns:
            Dict with analysis results
        """
        logger.info(f"Analyzing website: {url} (max_depth={max_depth}, follow_links={follow_links})")
        
        # Validate URL
        if not self._is_valid_url(url):
            return {"error": f"Invalid URL: {url}"}
        
        # Check robots.txt if enabled
        if self.follow_robots and not self._check_robots_txt(url):
            return {"error": f"URL not allowed by robots.txt: {url}"}
        
        # Initialize results
        results = {
            "url": url,
            "pages_analyzed": 0,
            "structure": {
                "pages": {},
                "navigation": {},
                "depth": 0
            },
            "content": {
                "text_content": {},
                "media_content": {},
                "forms": {}
            },
            "seo": {
                "titles": {},
                "meta_descriptions": {},
                "headings": {},
                "images_without_alt": 0,
                "internal_links": 0,
                "external_links": 0
            },
            "performance": {
                "page_sizes": {},
                "response_times": {}
            },
            "accessibility": {
                "issues": []
            },
            "mobile_friendly": None,
            "technologies": [],
            "summary": {}
        }
        
        # Set maximum depth
        max_depth = min(max_depth, 3)  # Limit to 3 for demonstration
        
        # Crawl the URL and its linked pages
        visited = set()
        to_visit = [(url, 0)]  # (url, depth)
        
        while to_visit and results["pages_analyzed"] < self.max_pages:
            current_url, depth = to_visit.pop(0)
            
            # Skip if already visited or exceeds max depth
            if current_url in visited or depth > max_depth:
                continue
            
            # Mark as visited
            visited.add(current_url)
            
            try:
                # Respect rate limit
                time.sleep(1 / self.rate_limit)
                
                # Fetch page content
                headers = {"User-Agent": self.user_agent}
                response = requests.get(current_url, headers=headers, timeout=self.timeout)
                response.raise_for_status()
                
                # Record response time
                response_time = response.elapsed.total_seconds()
                results["performance"]["response_times"][current_url] = response_time
                
                # Record page size
                page_size = len(response.content) / 1024  # KB
                results["performance"]["page_sizes"][current_url] = page_size
                
                # Process HTML content
                if "text/html" in response.headers.get("Content-Type", "").lower():
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    # Extract page title
                    title = soup.title.string if soup.title else current_url
                    results["seo"]["titles"][current_url] = title
                    
                    # Extract meta description
                    meta_desc = soup.find("meta", attrs={"name": "description"})
                    if meta_desc and "content" in meta_desc.attrs:
                        results["seo"]["meta_descriptions"][current_url] = meta_desc["content"]
                    
                    # Extract headings
                    headings = {
                        "h1": [h.get_text(strip=True) for h in soup.find_all("h1")],
                        "h2": [h.get_text(strip=True) for h in soup.find_all("h2")],
                        "h3": [h.get_text(strip=True) for h in soup.find_all("h3")]
                    }
                    results["seo"]["headings"][current_url] = headings
                    
                    # Count images without alt text
                    images = soup.find_all("img")
                    images_without_alt = sum(1 for img in images if not img.get("alt"))
                    results["seo"]["images_without_alt"] += images_without_alt
                    
                    # Extract links
                    links = soup.find_all("a", href=True)
                    internal_links = 0
                    external_links = 0
                    
                    # Store page in structure
                    page_path = urlparse(current_url).path or "/"
                    results["structure"]["pages"][page_path] = {
                        "url": current_url,
                        "title": title,
                        "depth": depth
                    }
                    
                    # Update maximum depth
                    results["structure"]["depth"] = max(results["structure"]["depth"], depth)
                    
                    # If following links and not at max depth, add links to visit
                    if follow_links and depth < max_depth:
                        base_url = f"{urlparse(current_url).scheme}://{urlparse(current_url).netloc}"
                        
                        for link in links:
                            href = link["href"]
                            
                            # Skip empty, javascript, and anchor links
                            if not href or href.startswith(("javascript:", "#", "mailto:", "tel:")):
                                continue
                            
                            # Resolve relative URLs
                            if not href.startswith(("http://", "https://")):
                                href = urljoin(current_url, href)
                            
                            # Check if internal or external
                            if href.startswith(base_url):
                                internal_links += 1
                                
                                # Only follow internal links
                                if href not in visited:
                                    to_visit.append((href, depth + 1))
                            else:
                                external_links += 1
                    
                    # Update link counts
                    results["seo"]["internal_links"] += internal_links
                    results["seo"]["external_links"] += external_links
                    
                    # Extract forms
                    forms = soup.find_all("form")
                    if forms:
                        results["content"]["forms"][current_url] = len(forms)
                    
                    # Check for mobile viewport
                    viewport = soup.find("meta", attrs={"name": "viewport"})
                    if viewport and "content" in viewport.attrs:
                        if results["mobile_friendly"] is None:
                            results["mobile_friendly"] = True
                    else:
                        results["mobile_friendly"] = False
                    
                    # Detect technologies (simplified)
                    if soup.find("script", src=lambda s: s and "wp-content" in s):
                        if "WordPress" not in results["technologies"]:
                            results["technologies"].append("WordPress")
                    
                    if soup.find("link", href=lambda s: s and "bootstrap" in s):
                        if "Bootstrap" not in results["technologies"]:
                            results["technologies"].append("Bootstrap")
                    
                    if soup.find("script", src=lambda s: s and "jquery" in s):
                        if "jQuery" not in results["technologies"]:
                            results["technologies"].append("jQuery")
                
                # Increment pages analyzed
                results["pages_analyzed"] += 1
                
            except Exception as e:
                logger.error(f"Error analyzing {current_url}: {str(e)}")
        
        # Generate summary
        self._generate_summary(results)
        
        return results
    
    def _is_valid_url(self, url: str) -> bool:
        """
        Check if a URL is valid.
        
        Args:
            url: The URL to check
            
        Returns:
            bool: True if valid, False otherwise
        """
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
    
    def _check_robots_txt(self, url: str) -> bool:
        """
        Check if a URL is allowed by robots.txt.
        
        Args:
            url: The URL to check
            
        Returns:
            bool: True if allowed, False otherwise
        """
        try:
            # Simplified implementation - in a real scenario, use a proper robots.txt parser
            parsed_url = urlparse(url)
            robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
            
            response = requests.get(robots_url, timeout=self.timeout)
            if response.status_code != 200:
                # If robots.txt doesn't exist or can't be fetched, assume allowed
                return True
            
            # Very simplified check - in reality, use a proper parser
            if "Disallow: /" in response.text:
                return False
            
            return True
        except:
            # If there's an error checking robots.txt, assume allowed
            return True
    
    def _generate_summary(self, results: Dict[str, Any]) -> None:
        """
        Generate a summary of the analysis results.
        
        Args:
            results: The analysis results to summarize
        """
        summary = {}
        
        # Calculate SEO score (simplified)
        seo_score = 100
        
        # Deduct for missing meta descriptions
        meta_desc_count = len(results["seo"]["meta_descriptions"])
        pages_count = results["pages_analyzed"]
        if pages_count > 0:
            meta_desc_percentage = meta_desc_count / pages_count
            if meta_desc_percentage < 0.8:
                seo_score -= 10
        
        # Deduct for missing alt text
        if results["seo"]["images_without_alt"] > 0:
            seo_score -= 5
        
        # Deduct for missing H1 headings
        h1_count = sum(1 for headings in results["seo"]["headings"].values() 
                       if headings.get("h1"))
        if pages_count > 0:
            h1_percentage = h1_count / pages_count
            if h1_percentage < 0.8:
                seo_score -= 10
        
        # Calculate performance score (simplified)
        perf_score = 100
        
        # Deduct for slow pages
        slow_pages = sum(1 for time in results["performance"]["response_times"].values() 
                         if time > 1.0)
        if pages_count > 0:
            slow_percentage = slow_pages / pages_count
            perf_score -= int(slow_percentage * 50)
        
        # Deduct for large pages
        large_pages = sum(1 for size in results["performance"]["page_sizes"].values() 
                          if size > 1000)  # > 1MB
        if pages_count > 0:
            large_percentage = large_pages / pages_count
            perf_score -= int(large_percentage * 30)
        
        # Mobile friendliness
        mobile_score = 100 if results["mobile_friendly"] else 0
        
        # Overall score
        overall_score = (seo_score + perf_score + mobile_score) / 3
        
        # Set summary
        summary["seo_score"] = max(0, min(100, seo_score))
        summary["performance_score"] = max(0, min(100, perf_score))
        summary["mobile_score"] = mobile_score
        summary["overall_score"] = max(0, min(100, overall_score))
        
        # Add recommendations
        recommendations = []
        
        if seo_score < 80:
            if meta_desc_percentage < 0.8:
                recommendations.append("Add meta descriptions to all pages")
            if results["seo"]["images_without_alt"] > 0:
                recommendations.append("Add alt text to all images")
            if h1_percentage < 0.8:
                recommendations.append("Ensure all pages have a single H1 heading")
        
        if perf_score < 80:
            if slow_percentage > 0.2:
                recommendations.append("Improve page load times")
            if large_percentage > 0.2:
                recommendations.append("Optimize page sizes")
        
        if not results["mobile_friendly"]:
            recommendations.append("Make the website mobile-friendly")
        
        summary["recommendations"] = recommendations
        
        # Update results
        results["summary"] = summary

# Create Tool instance
def get_website_analyzer_tool() -> Tool:
    """
    Get the website analyzer tool.
    
    Returns:
        Tool: The website analyzer tool
    """
    analyzer = WebsiteAnalyzer()
    
    return Tool(
        name="Website Analyzer",
        description="Analyzes a website's structure, content, SEO, and performance",
        func=analyzer.analyze_website
    )

if __name__ == "__main__":
    # Example usage
    analyzer = WebsiteAnalyzer()
    results = analyzer.analyze_website("https://example.com")
    print(json.dumps(results, indent=2))
</file>

<file path=".env.example">
# API Authentication
API_BEARER_TOKEN=your_api_token_here

# LLM API Keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# Web Crawling Settings
CRAWL_RATE_LIMIT=5
CRAWL_MAX_DEPTH=3
CRAWL_USER_AGENT="SolnAI Website Redesign Agent/1.0"

# CMS Integration
WORDPRESS_API_KEY=your_wordpress_api_key_here
WORDPRESS_API_SECRET=your_wordpress_api_secret_here
WEBFLOW_API_KEY=your_webflow_api_key_here

# Analytics Integration
GOOGLE_ANALYTICS_KEY=your_ga_key_here
HOTJAR_SITE_ID=your_hotjar_site_id_here

# SEO Tools
SEMRUSH_API_KEY=your_semrush_api_key_here
AHREFS_API_KEY=your_ahrefs_api_key_here

# Database Connection (if needed)
DATABASE_URL=postgresql://username:password@localhost:5432/database

# Logging
LOG_LEVEL=INFO
</file>

<file path="config.json">
{
  "name": "Website Redesign Agent",
  "version": "1.0.0",
  "description": "AI-powered assistant for automating website redesign tasks",
  "author": "SolnAI",
  "license": "MIT",
  "settings": {
    "max_pages_to_analyze": 100,
    "analysis_depth": "deep",
    "rate_limit": 5,
    "user_agent": "SolnAI Website Redesign Agent/1.0",
    "follow_robots_txt": true,
    "timeout": 30,
    "max_retries": 3
  },
  "agents": {
    "analyzer": {
      "enabled": true,
      "tools": ["website_analyzer", "user_behavior_analyzer"],
      "llm_model": "claude-3.7-sonnet",
      "temperature": 0.2
    },
    "designer": {
      "enabled": true,
      "tools": ["design_generator"],
      "llm_model": "claude-3.7-sonnet",
      "temperature": 0.7
    },
    "seo": {
      "enabled": true,
      "tools": ["seo_optimizer"],
      "llm_model": "claude-3.7-sonnet",
      "temperature": 0.3
    },
    "content": {
      "enabled": true,
      "tools": [],
      "llm_model": "claude-3.7-sonnet",
      "temperature": 0.5
    },
    "testing": {
      "enabled": true,
      "tools": [],
      "llm_model": "claude-3.7-sonnet",
      "temperature": 0.2
    }
  },
  "integrations": {
    "cms": {
      "wordpress": {
        "enabled": true,
        "api_endpoint": "/wp-json/wp/v2"
      },
      "webflow": {
        "enabled": true,
        "api_endpoint": "https://api.webflow.com"
      },
      "custom": {
        "enabled": false,
        "api_endpoint": ""
      }
    },
    "analytics": {
      "google_analytics": {
        "enabled": true
      },
      "hotjar": {
        "enabled": true
      },
      "custom": {
        "enabled": false,
        "api_endpoint": ""
      }
    },
    "seo_tools": {
      "semrush": {
        "enabled": false
      },
      "ahrefs": {
        "enabled": false
      },
      "custom": {
        "enabled": false,
        "api_endpoint": ""
      }
    }
  },
  "output_formats": {
    "analysis_report": ["markdown", "json"],
    "design_mockups": ["png", "pdf"],
    "seo_recommendations": ["markdown", "csv"],
    "content_plan": ["markdown"],
    "testing_plan": ["markdown", "csv"]
  },
  "default_redesign_goals": [
    "Improve user experience",
    "Enhance mobile responsiveness",
    "Optimize for search engines",
    "Modernize design",
    "Improve content structure",
    "Increase conversion rate"
  ]
}
</file>

<file path="Dockerfile">
FROM python:3.11-slim

WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application
COPY . .

# Create a non-root user and switch to it
RUN useradd -m appuser
USER appuser

# Expose the port
EXPOSE 8002

# Run the application
CMD ["uvicorn", "website_redesign_agent:app", "--host", "0.0.0.0", "--port", "8002"]
</file>

<file path="README.md">
# Website Redesign Agent

An AI-powered assistant for automating website redesign tasks using CrewAI and AutoGen.

## Overview

This agent helps automate various aspects of website redesign, including:

- Site analysis and planning
- Content migration and updates
- SEO optimization
- Design generation and UX improvements
- User behavior analysis
- Testing and quality assurance

## Features

- **Multi-agent workflow**: Specialized agents work together to handle different aspects of website redesign
- **Comprehensive analysis**: Analyzes existing websites for structure, content, SEO, and user experience
- **Design generation**: Creates design recommendations based on modern trends and best practices
- **Content optimization**: Updates and improves website content for better engagement and SEO
- **User behavior insights**: Analyzes user interactions to suggest UX improvements
- **Testing automation**: Performs automated testing for cross-browser compatibility and accessibility

## Usage

### Prerequisites

- SolnAI platform
- Python 3.9+
- Required API keys (optional, for external services)

### Configuration

1. Configure the agent settings in `config.json`
2. Set up API keys in `.env` file if using external services
3. Run the agent through SolnAI interface

## Implementation

The agent uses a CrewAI workflow with specialized agents:

1. **Analyzer Agent**: Examines the existing website and creates a redesign plan
2. **Designer Agent**: Generates design recommendations and mockups
3. **Content Agent**: Optimizes and migrates content
4. **SEO Agent**: Improves search engine optimization
5. **Testing Agent**: Performs quality assurance and testing

## Integration

This agent can integrate with various CMS platforms and tools:

- WordPress
- Webflow
- Custom websites
- Analytics platforms
- SEO tools
- Design systems

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="requirements.txt">
fastapi>=0.95.0
uvicorn>=0.22.0
pydantic>=2.0.0
python-dotenv>=1.0.0
crewai>=0.28.0
requests>=2.28.0
beautifulsoup4>=4.12.0
httpx>=0.24.0
pytest>=7.3.1
pytest-asyncio>=0.21.0
anthropic>=0.5.0
langchain>=0.0.267
langchain-community>=0.0.9
langchain-anthropic>=0.0.5
openai>=1.1.1
</file>

<file path="run_local.py">
#!/usr/bin/env python3
"""
Run the Website Redesign Agent locally.

This script demonstrates how to use the Website Redesign Agent
directly without going through the API.
"""

import os
import json
import argparse
from dotenv import load_dotenv
from website_redesign_agent import run_website_redesign

# Load environment variables
load_dotenv()

def main():
    """Run the Website Redesign Agent locally"""
    parser = argparse.ArgumentParser(description="Run Website Redesign Agent locally")
    parser.add_argument("--url", required=True, help="URL of the website to redesign")
    parser.add_argument("--output", default="redesign_report.json", help="Output file path")
    args = parser.parse_args()
    
    print(f"Starting website redesign process for URL: {args.url}")
    print("This may take some time depending on the website size and complexity...")
    
    # Run the redesign process
    result = run_website_redesign(args.url)
    
    # Save the result to a file
    with open(args.output, "w") as f:
        json.dump(result, f, indent=2)
    
    print(f"Website redesign completed. Results saved to {args.output}")

if __name__ == "__main__":
    main()
</file>

<file path="run_tests.py">
#!/usr/bin/env python3
"""
Run tests for the Website Redesign Agent.
"""

import os
import sys
import unittest

def run_tests():
    """Run all tests in the tests directory."""
    # Get the directory containing this script
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Add the script directory to the Python path
    sys.path.insert(0, script_dir)
    
    # Discover and run tests
    test_loader = unittest.TestLoader()
    test_suite = test_loader.discover('tests', pattern='test_*.py')
    
    test_runner = unittest.TextTestRunner(verbosity=2)
    result = test_runner.run(test_suite)
    
    return result.wasSuccessful()

if __name__ == "__main__":
    success = run_tests()
    sys.exit(0 if success else 1)
</file>

<file path="solnai_integration.py">
#!/usr/bin/env python3
"""
SolnAI Integration for Website Redesign Agent.

This script provides integration with the SolnAI platform,
allowing the Website Redesign Agent to be used as a SolnAI agent.
"""

import os
import sys
import json
import logging
from typing import Dict, Any

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import the website redesign agent
from website_redesign_agent import run_website_redesign

def handle_solnai_request(request_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Handle a request from SolnAI.
    
    Args:
        request_data: The request data from SolnAI
        
    Returns:
        Dict with response data
    """
    try:
        logger.info(f"Received SolnAI request: {json.dumps(request_data)}")
        
        # Extract parameters from request
        website_url = request_data.get("website_url")
        if not website_url:
            return {
                "success": False,
                "error": "Missing required parameter: website_url"
            }
        
        # Run the website redesign process
        result = run_website_redesign(website_url)
        
        # Return the result
        return {
            "success": True,
            "result": result
        }
    except Exception as e:
        logger.error(f"Error handling SolnAI request: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

def main():
    """Main entry point for SolnAI integration."""
    # Check if input file is provided
    if len(sys.argv) < 2:
        logger.error("Usage: python solnai_integration.py <input_file> [output_file]")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2] if len(sys.argv) > 2 else "response.json"
    
    try:
        # Read input file
        with open(input_file, "r") as f:
            request_data = json.load(f)
        
        # Process request
        response_data = handle_solnai_request(request_data)
        
        # Write response to output file
        with open(output_file, "w") as f:
            json.dump(response_data, f, indent=2)
        
        logger.info(f"Response written to {output_file}")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Error in SolnAI integration: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
</file>

<file path="Website_Redesign_Agent.json">
{
  "name": "Website Redesign Agent",
  "version": "1.0.0",
  "description": "AI-powered assistant for automating website redesign tasks",
  "author": "SolnAI",
  "license": "MIT",
  "type": "CrewAI",
  "category": "Web Development",
  "tags": ["website", "redesign", "SEO", "content", "design", "UX"],
  "icon": "ðŸŒ",
  "entry_point": "solnai_integration.py",
  "parameters": [
    {
      "name": "website_url",
      "type": "string",
      "description": "URL of the website to redesign",
      "required": true
    },
    {
      "name": "redesign_goals",
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Goals for the website redesign",
      "required": false,
      "default": [
        "Improve user experience",
        "Enhance mobile responsiveness",
        "Optimize for search engines",
        "Modernize design",
        "Improve content structure",
        "Increase conversion rate"
      ]
    },
    {
      "name": "include_seo",
      "type": "boolean",
      "description": "Include SEO optimization in the redesign",
      "required": false,
      "default": true
    },
    {
      "name": "include_content_migration",
      "type": "boolean",
      "description": "Include content migration in the redesign",
      "required": false,
      "default": true
    },
    {
      "name": "include_design_generation",
      "type": "boolean",
      "description": "Include design generation in the redesign",
      "required": false,
      "default": true
    },
    {
      "name": "include_user_analysis",
      "type": "boolean",
      "description": "Include user behavior analysis in the redesign",
      "required": false,
      "default": true
    },
    {
      "name": "target_cms",
      "type": "string",
      "description": "Target CMS for the redesigned website",
      "required": false,
      "enum": ["WordPress", "Webflow", "Custom"]
    }
  ],
  "output": {
    "type": "object",
    "properties": {
      "success": {
        "type": "boolean",
        "description": "Whether the redesign was successful"
      },
      "result": {
        "type": "object",
        "description": "The redesign result"
      },
      "error": {
        "type": "string",
        "description": "Error message if the redesign failed"
      }
    }
  },
  "examples": [
    {
      "name": "Basic Website Redesign",
      "description": "Redesign a simple website with default settings",
      "input": {
        "website_url": "https://example.com"
      }
    },
    {
      "name": "WordPress Website Redesign",
      "description": "Redesign a WordPress website with SEO focus",
      "input": {
        "website_url": "https://example-wordpress.com",
        "redesign_goals": [
          "Improve SEO rankings",
          "Modernize design",
          "Improve page load speed"
        ],
        "include_seo": true,
        "include_content_migration": true,
        "include_design_generation": true,
        "include_user_analysis": false,
        "target_cms": "WordPress"
      }
    }
  ],
  "dependencies": {
    "python": ">=3.9",
    "packages": [
      "fastapi>=0.95.0",
      "uvicorn>=0.22.0",
      "pydantic>=2.0.0",
      "python-dotenv>=1.0.0",
      "crewai>=0.28.0",
      "requests>=2.28.0",
      "beautifulsoup4>=4.12.0",
      "httpx>=0.24.0"
    ]
  },
  "resources": {
    "cpu": "2",
    "memory": "4Gi",
    "gpu": "0"
  },
  "documentation": {
    "usage": "This agent helps automate various aspects of website redesign, including site analysis, content migration, SEO optimization, design generation, and user behavior analysis.",
    "examples": [
      {
        "title": "Basic Website Redesign",
        "description": "Analyze and redesign a simple website",
        "code": "python run_local.py --url https://example.com --output redesign_report.json"
      },
      {
        "title": "WordPress Integration",
        "description": "Analyze and redesign a WordPress website",
        "code": "python run_local.py --url https://example-wordpress.com --output wordpress_redesign.json"
      }
    ],
    "links": [
      {
        "title": "GitHub Repository",
        "url": "https://github.com/solnai/website-redesign-agent"
      },
      {
        "title": "CrewAI Documentation",
        "url": "https://docs.crewai.com/"
      }
    ]
  }
}
</file>

<file path="website_redesign_agent.py">
#!/usr/bin/env python3
"""
Website Redesign Agent

A CrewAI-based agent system for automating website redesign tasks.
This agent orchestrates multiple specialized agents to handle different aspects
of website redesign, from analysis to implementation.
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Depends, Security
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# CrewAI imports
from crewai import Agent, Task, Crew, Process
from crewai.tools import Tool

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI()
security = HTTPBearer()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response Models
class WebsiteRedesignRequest(BaseModel):
    """Request model for website redesign job"""
    website_url: str
    redesign_goals: List[str]
    user_id: str
    request_id: str
    session_id: str
    include_seo: bool = True
    include_content_migration: bool = True
    include_design_generation: bool = True
    include_user_analysis: bool = True
    target_cms: Optional[str] = None
    custom_requirements: Optional[Dict[str, Any]] = None

class WebsiteRedesignResponse(BaseModel):
    """Response model for website redesign job"""
    success: bool
    job_id: Optional[str] = None
    message: Optional[str] = None

def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)) -> bool:
    """Verify the bearer token against environment variable."""
    expected_token = os.getenv("API_BEARER_TOKEN")
    if not expected_token:
        raise HTTPException(
            status_code=500,
            detail="API_BEARER_TOKEN environment variable not set"
        )
    if credentials.credentials != expected_token:
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication token"
        )
    return True

# Tool definitions
def analyze_website(url: str) -> Dict[str, Any]:
    """
    Analyze a website and return structured data about its content,
    structure, SEO, and performance.
    
    Args:
        url: The website URL to analyze
        
    Returns:
        Dict with analysis results
    """
    logger.info(f"Analyzing website: {url}")
    # In a real implementation, this would use web scraping libraries
    # and analytics APIs to gather data about the website
    
    # Placeholder implementation
    return {
        "url": url,
        "pages_count": 10,
        "structure": {
            "homepage": True,
            "about": True,
            "contact": True,
            "blog": True,
            "products": False
        },
        "seo_score": 65,
        "performance_score": 72,
        "content_quality_score": 68,
        "mobile_friendly": True
    }

def generate_design_mockup(requirements: Dict[str, Any]) -> Dict[str, Any]:
    """
    Generate design mockups based on requirements.
    
    Args:
        requirements: Design requirements
        
    Returns:
        Dict with design mockup information
    """
    logger.info(f"Generating design mockup with requirements: {requirements}")
    # In a real implementation, this would use design generation APIs
    # or AI image generation to create mockups
    
    # Placeholder implementation
    return {
        "mockup_id": "mock-12345",
        "design_style": requirements.get("style", "modern"),
        "color_scheme": requirements.get("colors", ["#336699", "#FFFFFF", "#333333"]),
        "layout": requirements.get("layout", "responsive"),
        "mockup_url": "https://example.com/mockups/12345.png"
    }

def optimize_seo(url: str, keywords: List[str]) -> Dict[str, Any]:
    """
    Generate SEO optimization recommendations.
    
    Args:
        url: The website URL to optimize
        keywords: Target keywords
        
    Returns:
        Dict with SEO recommendations
    """
    logger.info(f"Optimizing SEO for {url} with keywords: {keywords}")
    # In a real implementation, this would use SEO analysis tools
    # and AI to generate optimization recommendations
    
    # Placeholder implementation
    return {
        "url": url,
        "recommendations": {
            "title_tags": [
                {"page": "home", "current": "Home", "suggested": "Home | Brand | Keywords"},
                {"page": "about", "current": "About Us", "suggested": "About Our Company | Keywords"}
            ],
            "meta_descriptions": [
                {"page": "home", "current": "Welcome to our site", "suggested": "Discover our products and services. We offer the best solutions for your needs."}
            ],
            "content_gaps": ["keyword1", "keyword2"],
            "backlink_opportunities": ["site1.com", "site2.com"]
        }
    }

def analyze_user_behavior(url: str) -> Dict[str, Any]:
    """
    Analyze user behavior on a website.
    
    Args:
        url: The website URL to analyze
        
    Returns:
        Dict with user behavior analysis
    """
    logger.info(f"Analyzing user behavior for {url}")
    # In a real implementation, this would use analytics APIs
    # and heatmap tools to analyze user behavior
    
    # Placeholder implementation
    return {
        "url": url,
        "page_views": {
            "home": 1200,
            "about": 450,
            "contact": 320,
            "blog": 780
        },
        "average_time_on_site": "2m 45s",
        "bounce_rate": "65%",
        "conversion_rate": "3.2%",
        "popular_content": ["blog/post-1", "products/item-3"],
        "drop_off_points": ["checkout/payment", "signup/form"]
    }

# Create tools from functions
website_analyzer_tool = Tool(
    name="Website Analyzer",
    description="Analyzes a website's structure, content, SEO, and performance",
    func=analyze_website
)

design_generator_tool = Tool(
    name="Design Generator",
    description="Generates website design mockups based on requirements",
    func=generate_design_mockup
)

seo_optimizer_tool = Tool(
    name="SEO Optimizer",
    description="Provides SEO optimization recommendations",
    func=optimize_seo
)

user_behavior_analyzer_tool = Tool(
    name="User Behavior Analyzer",
    description="Analyzes user behavior on a website",
    func=analyze_user_behavior
)

# Agent definitions
def create_analyzer_agent() -> Agent:
    """Create the website analyzer agent"""
    return Agent(
        role="Website Analyzer",
        goal="Thoroughly analyze websites and identify improvement opportunities",
        backstory="""You are an expert website analyst with years of experience in 
        evaluating websites for usability, performance, SEO, and content quality. 
        You have a keen eye for detail and can quickly identify issues and opportunities 
        for improvement.""",
        verbose=True,
        allow_delegation=False,
        tools=[website_analyzer_tool, user_behavior_analyzer_tool]
    )

def create_designer_agent() -> Agent:
    """Create the website designer agent"""
    return Agent(
        role="Website Designer",
        goal="Create modern, user-friendly website designs that meet client requirements",
        backstory="""You are a talented website designer with expertise in UI/UX design, 
        responsive layouts, and modern design trends. You create designs that are both 
        visually appealing and functional, with a focus on user experience.""",
        verbose=True,
        allow_delegation=False,
        tools=[design_generator_tool]
    )

def create_seo_agent() -> Agent:
    """Create the SEO specialist agent"""
    return Agent(
        role="SEO Specialist",
        goal="Optimize websites for search engines to improve visibility and traffic",
        backstory="""You are an experienced SEO specialist who knows all the best practices 
        for improving website visibility in search engines. You understand both technical SEO 
        and content optimization strategies.""",
        verbose=True,
        allow_delegation=False,
        tools=[seo_optimizer_tool]
    )

def create_content_agent() -> Agent:
    """Create the content specialist agent"""
    return Agent(
        role="Content Specialist",
        goal="Create and optimize website content for engagement and conversion",
        backstory="""You are a skilled content creator and strategist who knows how to craft 
        compelling website content that engages users and drives conversions. You understand 
        content hierarchy, readability, and how to write for the web.""",
        verbose=True,
        allow_delegation=False
    )

def create_testing_agent() -> Agent:
    """Create the testing specialist agent"""
    return Agent(
        role="Testing Specialist",
        goal="Ensure websites function correctly across devices and browsers",
        backstory="""You are a meticulous testing specialist who ensures websites work 
        flawlessly across different devices, browsers, and connection speeds. You have 
        a systematic approach to identifying and documenting issues.""",
        verbose=True,
        allow_delegation=False
    )

# Task definitions
def create_analysis_task(agent: Agent, website_url: str) -> Task:
    """Create a website analysis task"""
    return Task(
        description=f"""Analyze the website at {website_url} and create a comprehensive report on its 
        current state, including structure, content, SEO, performance, and user experience. 
        Identify key issues and opportunities for improvement.""",
        expected_output="""A detailed analysis report with sections for site structure, 
        content quality, SEO status, performance metrics, and user experience evaluation. 
        Include specific recommendations for improvement in each area.""",
        agent=agent
    )

def create_design_task(agent: Agent, analysis_task: Task) -> Task:
    """Create a design creation task"""
    return Task(
        description="""Based on the analysis report, create design mockups for the 
        redesigned website. Consider modern design trends, user experience best practices, 
        and the client's brand identity. Create mockups for at least the homepage and 
        one interior page.""",
        expected_output="""Design mockups for the homepage and at least one interior page, 
        with explanations of design choices, color schemes, typography, and layout. 
        Include recommendations for responsive design considerations.""",
        agent=agent,
        context=[analysis_task]
    )

def create_seo_task(agent: Agent, analysis_task: Task) -> Task:
    """Create an SEO optimization task"""
    return Task(
        description="""Based on the analysis report, create a comprehensive SEO optimization 
        plan for the website. Include recommendations for on-page SEO, technical SEO, 
        content optimization, and keyword strategy.""",
        expected_output="""A detailed SEO optimization plan with specific recommendations 
        for title tags, meta descriptions, content improvements, URL structure, internal linking, 
        and technical SEO fixes. Include a prioritized list of actions.""",
        agent=agent,
        context=[analysis_task]
    )

def create_content_task(agent: Agent, analysis_task: Task, seo_task: Task) -> Task:
    """Create a content optimization task"""
    return Task(
        description="""Based on the analysis report and SEO optimization plan, create a 
        content strategy for the redesigned website. Include recommendations for content 
        structure, messaging, tone, and specific content improvements.""",
        expected_output="""A content strategy document with recommendations for site messaging, 
        content structure, specific page improvements, and new content opportunities. 
        Include sample content for key sections.""",
        agent=agent,
        context=[analysis_task, seo_task]
    )

def create_testing_task(agent: Agent, design_task: Task) -> Task:
    """Create a testing plan task"""
    return Task(
        description="""Create a comprehensive testing plan for the redesigned website. 
        Include test cases for functionality, usability, performance, compatibility, 
        and accessibility.""",
        expected_output="""A detailed testing plan with specific test cases, testing methodologies, 
        and tools to use. Include a checklist for pre-launch testing and post-launch monitoring.""",
        agent=agent,
        context=[design_task]
    )

def create_final_report_task(agent: Agent, all_tasks: List[Task]) -> Task:
    """Create a final report compilation task"""
    return Task(
        description="""Compile all the findings and recommendations from the previous tasks 
        into a comprehensive website redesign plan. Organize the information in a clear, 
        actionable format with prioritized recommendations.""",
        expected_output="""A comprehensive website redesign plan document with executive summary, 
        detailed findings, specific recommendations, implementation roadmap, and expected outcomes. 
        The document should be well-structured and ready to present to stakeholders.""",
        agent=agent,
        context=all_tasks
    )

# Crew creation
def create_website_redesign_crew(website_url: str) -> Crew:
    """Create a CrewAI crew for website redesign"""
    # Create agents
    analyzer_agent = create_analyzer_agent()
    designer_agent = create_designer_agent()
    seo_agent = create_seo_agent()
    content_agent = create_content_agent()
    testing_agent = create_testing_agent()
    
    # Create tasks
    analysis_task = create_analysis_task(analyzer_agent, website_url)
    design_task = create_design_task(designer_agent, analysis_task)
    seo_task = create_seo_task(seo_agent, analysis_task)
    content_task = create_content_task(content_agent, analysis_task, seo_task)
    testing_task = create_testing_task(testing_agent, design_task)
    final_report_task = create_final_report_task(
        analyzer_agent, 
        [analysis_task, design_task, seo_task, content_task, testing_task]
    )
    
    # Create crew
    crew = Crew(
        agents=[analyzer_agent, designer_agent, seo_agent, content_agent, testing_agent],
        tasks=[analysis_task, design_task, seo_task, content_task, testing_task, final_report_task],
        verbose=True,
        process=Process.sequential
    )
    
    return crew

# API endpoint
@app.post("/api/website-redesign-agent", response_model=WebsiteRedesignResponse)
async def website_redesign_agent(
    request: WebsiteRedesignRequest,
    authenticated: bool = Depends(verify_token)
):
    """API endpoint for the website redesign agent"""
    try:
        logger.info(f"Received website redesign request for URL: {request.website_url}")
        
        # In a real implementation, this would create a background job
        # and return a job ID for status tracking
        
        # For demonstration purposes, we'll just return a success response
        return WebsiteRedesignResponse(
            success=True,
            job_id="job-12345",
            message="Website redesign job created successfully"
        )
    except Exception as e:
        logger.error(f"Error processing website redesign request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Example usage
def run_website_redesign(website_url: str) -> Dict[str, Any]:
    """Run the website redesign process directly"""
    crew = create_website_redesign_crew(website_url)
    result = crew.kickoff()
    return {"result": result}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)
</file>

</files>
